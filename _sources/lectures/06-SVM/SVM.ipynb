{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Motivation: Why Support Vector Machines?\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful class of supervised learning algorithms used for classification and regression tasks. While models like logistic regression are probabilistic, SVMs are deterministic and based on geometrical properties of the data. The core idea behind SVMs is to find an optimal hyperplane that best separates different classes of data points. This 'optimality' is achieved by maximizing the margin, which is the distance between the hyperplane and the closest data points from each class. These closest points are called **support vectors**, and they are the critical elements of the training data that define the decision boundary.\n",
    "\n",
    "- Maximizes **margin** → better generalization.\n",
    "- Works in high-dimensional spaces (e.g., gene expression, spectroscopy)\n",
    "- Flexible with kernels for non-linear separation.\n",
    "- Robust to outliers with soft margins.\n",
    "- The number of features > number of samples (common in biochemistry, genomics).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate linear data\n",
    "X_lin, y_lin = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, class_sep=2.0, random_state=42)\n",
    "\n",
    "# Generate non-linear data\n",
    "X_nonlin, y_nonlin = make_circles(n_samples=100, factor=0.5, noise=0.1, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, ax, title):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Fit models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for data, ax_row, kernel in zip([(X_lin, y_lin), (X_nonlin, y_nonlin)], axes, ['linear', 'rbf']):\n",
    "    X, y = data\n",
    "    log_reg = LogisticRegression().fit(X, y)\n",
    "    svm = SVC(kernel=kernel).fit(X, y)\n",
    "    plot_decision_boundary(log_reg, X, y, ax_row[0], 'Logistic Regression')\n",
    "    plot_decision_boundary(svm, X, y, ax_row[1], f'SVM ({kernel} kernel)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Some applications to basic sciences\n",
    "\n",
    "### **Biology & Bioinformatics**\n",
    "\n",
    "1.  Ahmad, M., & Hayat, M. (2022). A comparative study of machine learning approaches for the prediction of anticancer peptides. *Briefings in Bioinformatics, 23*(5), bbac327. [https://doi.org/10.1093/bib/bbac327](https://doi.org/10.1093/bib/bbac327)\n",
    "\n",
    "2.  Deo, R. C. (2023). Machine learning in medicine. *Circulation, 148*(20), 1637–1655. [https://doi.org/10.1161/CIRCULATIONAHA.123.064953](https://doi.org/10.1161/CIRCULATIONAHA.123.064953)\n",
    "\n",
    "3.  El-Saadawy, A. A., & El-Bakry, H. M. (2023). A proposed model for breast cancer diagnosis using a hybrid feature selection and a support vector machine. *Scientific Reports, 13*(1), 17799. [https://doi.org/10.1038/s41598-023-45091-8](https://doi.org/10.1038/s41598-023-45091-8)\n",
    "\n",
    "4.  Su, R., Liu, Y., Zhang, R., Liu, T., & Wang, X. (2024). Deep-Resp-Forest: A deep learning model for predicting respiratory system-related adverse drug reactions. *Computers in Biology and Medicine, 171*, 108130. [https://doi.org/10.1016/j.compbiomed.2024.108130](https://doi.org/10.1016/j.compbiomed.2024.108130)\n",
    "\n",
    "### **Chemistry & Materials Science**\n",
    "\n",
    "5.  Al-Janabi, A. H. (2024). Classification of crude oil using machine learning algorithms based on their physical properties. *Egyptian Journal of Chemistry, 67*(11), 385-391. [https://doi.org/10.21608/ejchem.2024.267815.9221](https://doi.org/10.21608/ejchem.2024.267815.9221)\n",
    "\n",
    "6.  Gao, C., Wang, Z., Li, Y., Wang, C., Li, D., & Lu, S. (2024). Development and validation of machine learning models for predicting the oral bioavailability of drugs. *AAPS PharmSciTech, 25*(2), 52. [https://doi.org/10.1208/s12249-024-02758-1](https://doi.org/10.1208/s12249-024-02758-1)\n",
    "\n",
    "7.  He, Y., Zhao, Y., & Ai, Q. (2024). A data-driven machine learning framework for performance prediction and parameter optimization of vanadium flow batteries. *Journal of Energy Storage, 81*, 110433. [https://doi.org/10.1016/j.est.2024.110433](https://doi.org/10.1016/j.est.2024.110433)\n",
    "\n",
    "8.  Zhu, X., Li, S., Yuan, S., & Li, G. (2024). Machine learning prediction of single-atom catalysts for CO2 reduction reaction. *Journal of Materials Chemistry A, 12*(2), 652-661. [https://doi.org/10.1039/D3TA05770K](https://doi.org/10.1039/D3TA05770K)\n",
    "\n",
    "### **Physics**\n",
    "\n",
    "9.  Lei, B., Wang, Y., Jiang, G., Li, C., & Ding, Z. (2024). Prediction of rockburst intensity grade based on the K-means and support vector machine. *International Journal for Numerical and Analytical Methods in Geomechanics, 48*(8), 1999-2019. [https://doi.org/10.1002/nag.3729](https://doi.org/10.1002/nag.3729)\n",
    "\n",
    "10. Singh, V. K., & Foufoula-Georgiou, E. (2024). A hierarchical machine learning model for predicting sub-grid scale atmospheric turbulence. *Journal of Advances in Modeling Earth Systems, 16*(1), e2023MS003889. [https://doi.org/10.1029/2023MS003889](https://doi.org/10.1029/2023MS003889)\n",
    "\n",
    "11. Wrembel, M., & Bąk, K. (2023). A new combined model for predicting the top-of-atmosphere daily incoming solar radiation. *Astronomy and Computing, 44*, 100742. [https://doi.org/10.1016/j.ascom.2023.100742](https://doi.org/10.1016/j.ascom.2023.100742)\n",
    "\n",
    "12. Xie, Y., & Yang, B. (2021). Identification of the physical parameters of the nonlinear pendulum by the support vector machine method. *Physical Review E, 104*(4), 044201. [https://doi.org/10.1103/PhysRevE.104.044201](https://doi.org/10.1103/PhysRevE.104.044201)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## SVM Fundamentals\n",
    "SVM finds the hyperplane that maximizes the margin (distance to nearest data points of each class). These nearest points are called support vectors.\n",
    "\n",
    "Given a dataset $(x_i, y_i)$ with $y_i \\in \\{-1, 1\\}$, SVM solves:\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2} ||w||^2 \\quad \\text{s.t.} \\quad y_i (w \\cdot x_i + b) \\geq 1,\n",
    "$$\n",
    "- $ \\mathbf{w} $: normal vector to hyperplane\n",
    "- $ b $: bias\n",
    "- $ y_i \\in \\{-1, +1\\} $\n",
    "- Margin = $ \\frac{2}{\\|\\mathbf{w}\\|} $\n",
    "  \n",
    "Soft-margin SVM introduces slack variables $\\xi_i$ and penalty $C$.\n",
    "\n",
    ":::{exercise}\n",
    "Derive the dual form of the SVM optimization problem and explain the role of Lagrange multipliers.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Visualizing the Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create linearly separable data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# Fit the SVM model\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# Plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('Linearly Separable Data with SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{tip} The hyperplane is equidistant from the closest points (support vectors). The margin is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Exploring the Margin\n",
    "1.  Modify the `C` parameter in the `svm.SVC` function in the code above. What do you observe about the margin and the number of support vectors when you use a very small `C` (e.g., 0.01) versus a very large `C` (e.g., 10000)?\n",
    "2.  What does the `C` parameter control in the context of the SVM classifier? *Hint: Think about the trade-off between a smooth decision boundary and classifying training points correctly.*\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}Identify Support Vectors\n",
    "Use `svm.support_vectors_` to extract and plot the support vectors on the above figure.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Comparison with Other Methods\n",
    "\n",
    "### SVM vs. Logistic Regression\n",
    "\n",
    "| Feature | Support Vector Machine (SVM) | Logistic Regression |\n",
    "|---|---|---|\n",
    "| **Underlying Principle** | Geometric: finds the optimal hyperplane that maximizes the margin between classes. | Statistical: models the probability of a certain class or event existing. |\n",
    "| **Decision Boundary** | Can be linear or non-linear using the kernel trick. | Fundamentally linear, though can be extended to non-linear with feature engineering. |\n",
    "| **Sensitivity to Outliers**| Less sensitive to outliers due to the margin-based optimization.  | Can be more sensitive to outliers as it tries to classify all points correctly. |\n",
    "| **Use Cases** | Effective in high-dimensional spaces and for both linear and non-linear problems. Works well with unstructured data like text and images. | Performs well on linearly separable data and when a probabilistic interpretation is needed. Often a good first model to try.  |\n",
    "| **Robust to overfitting** | Strong (especially in high-dim) | Moderate |\n",
    "\n",
    "In general, if the number of features is much larger than the number of training examples, logistic regression or a linear SVM is recommended. If the number of examples is intermediate and the data is not linearly separable, an SVM with a non-linear kernel is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Kernel Trick: Handling Non-Linear Data\n",
    "\n",
    "What if the data is not linearly separable? This is where the **kernel trick** comes in. The kernel trick is a powerful technique that allows SVMs to classify non-linear data. [5] It works by mapping the input data into a higher-dimensional space where a linear separator can be found. [4, 5] This is done implicitly, without ever having to compute the coordinates of the data in this higher-dimensional space, which is computationally efficient. [5]\n",
    "\n",
    "Commonly used kernels include:\n",
    "*   **Linear:** For linearly separable data.\n",
    "*   **Polynomial:** Creates a polynomial decision boundary. $ K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d $\n",
    "*   **Radial Basis Function (RBF):** A popular choice for its flexibility in handling complex relationships. $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $\n",
    "*   **Sigmoid:** Can be useful in certain neural network-like scenarios.\n",
    "\n",
    "```{note}\n",
    "**RBF is default**: Handles complex, non-linear patterns (e.g., in metabolomics, protein folding)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=100, factor=.1, noise=.1, random_state=42)\n",
    "\n",
    "# Fit the SVM model with an RBF kernel\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma='auto')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# Plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('Non-Linearly Separable Data with RBF Kernel SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}Experimenting with Kernels\n",
    "\n",
    "1.  In the code above, change the `kernel` parameter to `'linear'` and `'poly'`. How does the decision boundary change? Which kernel performs best for this dataset?\n",
    "2.  For the RBF kernel, experiment with different values for the `gamma` parameter. What is the effect of a very small `gamma` versus a very large `gamma` on the decision boundary? What might this imply about the model's complexity and potential for overfitting?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A more complex boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate non-linear data\n",
    "X_moon, y_moon = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
    "X_moon_scaled = scaler.fit_transform(X_moon)\n",
    "\n",
    "# Fit SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf.fit(X_moon_scaled, y_moon)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_moon_scaled[y_moon == 0, 0], X_moon_scaled[y_moon == 0, 1], c='red', label='Class 0', alpha=0.7)\n",
    "plt.scatter(X_moon_scaled[y_moon == 1, 0], X_moon_scaled[y_moon == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "\n",
    "# Decision boundary (grid)\n",
    "xx, yy = np.meshgrid(np.linspace(X_moon_scaled[:, 0].min(), X_moon_scaled[:, 0].max(), 100),\n",
    "                     np.linspace(X_moon_scaled[:, 1].min(), X_moon_scaled[:, 1].max(), 100))\n",
    "Z = svm_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='-', alpha=0.7)\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdBu')\n",
    "\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.title('SVM with RBF Kernel: Non-Linear Separation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Compare SVM vs Logistic Regression\n",
    "Fit a logistic regression model on the same moons dataset. Compare accuracy and decision boundary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58e30c1a402857c1d6bccdfec3070f02",
     "grade": false,
     "grade_id": "cell-4691c03f3aff962f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive role of hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data\n",
    "X, y = make_classification(n_samples=150, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create interactive controls\n",
    "@widgets.interact(\n",
    "    C=widgets.FloatSlider(value=1.0, min=0.1, max=10.0, step=0.1, description='C'),\n",
    "    gamma=widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01, description='Gamma'),\n",
    "    kernel=widgets.Dropdown(options=['rbf', 'linear', 'poly'], value='rbf', description='Kernel')\n",
    ")\n",
    "def plot_svm(C=1.0, gamma=0.1, kernel='rbf'):\n",
    "    # Fit SVM\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma, random_state=42)\n",
    "    svm.fit(X_scaled, y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', label='Class 0', alpha=0.7)\n",
    "    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "    \n",
    "    # Decision boundary (grid)\n",
    "    xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100),\n",
    "                         np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100))\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='-', alpha=0.8)\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdBu')\n",
    "    \n",
    "    # Support vectors\n",
    "    sv = svm.support_vectors_\n",
    "    plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='green', linewidth=2, label='Support Vectors')\n",
    "    \n",
    "    plt.xlabel('Feature 1 (scaled)')\n",
    "    plt.ylabel('Feature 2 (scaled)')\n",
    "    plt.title(f'SVM Decision Boundary (C={C}, gamma={gamma:.2f}, kernel={kernel})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Applications in Basic Sciences\n",
    "\n",
    "SVMs have found numerous applications in various scientific domains:\n",
    "\n",
    "*   **Bioinformatics:** SVMs are widely used for tasks like protein classification, gene expression analysis, and cancer classification. Their ability to handle high-dimensional data makes them suitable for analyzing genomic and proteomic datasets.\n",
    "*   **Image Classification:** In fields like medical imaging and satellite imagery analysis, SVMs can be used to classify images, for instance, to identify tumors in medical scans or to classify different types of land cover from satellite data. \n",
    "*   **Chemistry:** SVMs can be used in cheminformatics to predict the properties of molecules, such as their bioactivity or toxicity, based on their chemical structure.\n",
    "\n",
    "### Biochemistry: Enzyme Substrate Classification\n",
    "Features: 3D molecular descriptors (e.g., hydrophobicity, size, charge).\n",
    "Task: Classify if a molecule is a substrate (yes/no) for an enzyme.\n",
    "Why SVM? High-dim, small samples, non-linear relationships\n",
    "\n",
    "### Spectroscopy: Material Phase Classification\n",
    "Features: Raman or IR intensity at 1000+ wavenumbers.\n",
    "Task: Classify solid vs. liquid phase.\n",
    "Why SVM? RBF kernel captures subtle spectral shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise}\n",
    "Dataset: breast_cancer from sklearn (classic biomedical dataset).\n",
    "Goal: Classify tumors as malignant/benign using 30 morphological features.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f962b0cd3e849857e75cff216e93e541",
     "grade": false,
     "grade_id": "cell-4551882fe3fbfc1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercises: Applying SVM to a Biological Dataset\n",
    "\n",
    "For this final set of exercises, we will use the **Breast Cancer Wisconsin (Diagnostic) Dataset**, which is available through scikit-learn. The task is to predict whether a tumor is malignant or benign based on several features of the cell nuclei.\n",
    "\n",
    "**Dataset Information:** The dataset contains 30 numeric, predictive attributes and the class (malignant or benign). [23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --- Your Code Here --- #\n",
    "# 1. Train a linear SVM classifier\n",
    "\n",
    "\n",
    "# 2. Evaluate the linear SVM model\n",
    "\n",
    "\n",
    "# 3. Train an SVM classifier with an RBF kernel\n",
    "\n",
    "\n",
    "# 4. Evaluate the RBF SVM model\n",
    "\n",
    "\n",
    "# 5. Compare the performance of the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Instructions:\n",
    "\n",
    "1.  **Train a Linear SVM:** In the provided code cell, create and train an `svm.SVC` model with a linear kernel on the training data.\n",
    "2.  **Evaluate the Linear SVM:** Use the trained linear model to make predictions on the test set. Then, print the confusion matrix and the classification report to evaluate its performance.\n",
    "3.  **Train an RBF SVM:** Now, create and train another `svm.SVC` model, but this time use an RBF kernel.\n",
    "4.  **Evaluate the RBF SVM:**  Similarly, evaluate the performance of the RBF kernel SVM on the test set by printing its confusion matrix and classification report.\n",
    "5.  **Compare Models:** Which model performed better on this dataset? Why do you think that is the case? Consider the nature of the data and the strengths of each kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## EXTRA: scikit pipeline\n",
    " A full scikit pipeline allows you to estimate the best hyper parameters for a given problem. It uses <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Build full pipeline\n",
    "def create_svm_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=10)),  # Select top 10 features\n",
    "        ('svm', SVC(kernel='rbf', random_state=42))\n",
    "    ])\n",
    "\n",
    "# Use on breast cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = create_svm_pipeline()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'feature_selection__k': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_.round(3))\n",
    "\n",
    "# Final prediction\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Comparing svm and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compare_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'SVM (RBF)': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=10)),\n",
    "            ('clf', SVC(kernel='rbf', C=10, gamma=0.01, probability=True))\n",
    "        ]),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=10)),\n",
    "            ('clf', LogisticRegression(max_iter=1000))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        results[name] = {'test_score': score, 'cv_mean': cv_scores.mean(), 'cv_std': cv_scores.std()}\n",
    "        print(f\"{name}: Test Accuracy = {score:.3f}, CV Accuracy = {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare on breast cancer\n",
    "compare_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Build Your Own Pipeline\n",
    "Use make_classification to generate a dataset with 100 features, 500 samples, and non-linear structure.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fefbfff147bbb37f45ffebb305aec9a6",
     "grade": false,
     "grade_id": "cell-d972df28fb08ab5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## EXTRA: Support Vector Machines (SVM) with Model Explainability (SHAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Scientific\" data\n",
    "# Simulate metabolomics data (n=100 samples, p=50 features)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "X_metab = np.random.normal(0, 1, (n_samples, n_features))\n",
    "\n",
    "# Introduce signal: 10 features differ between classes\n",
    "signal_indices = np.random.choice(n_features, 10, replace=False)\n",
    "X_metab[:50, signal_indices] += 1.0  # Healthy (class 0)\n",
    "X_metab[50:, signal_indices] += -0.8  # Diseased (class 1)\n",
    "\n",
    "y_metab = np.hstack([np.zeros(50), np.ones(50)])\n",
    "\n",
    "# Split\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_metab, y_metab, test_size=0.3, random_state=42, stratify=y_metab)\n",
    "\n",
    "print(\"Metabolomics Dataset: Shape =\", X_train_m.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SVM Pipeline with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "def create_svm_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(f_classif, k=10)),\n",
    "        ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ])\n",
    "\n",
    "# Tune\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'selector__k': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(create_svm_pipeline(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid.fit(X_train_m, y_train_m)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"CV Accuracy:\", grid.best_score_.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability with SHAP\n",
    "Why SHAP for SVM?\n",
    "SVM is not inherently interpretable, but SHAP provides global and local explanations by attributing prediction changes to input features, based on game theory.\n",
    "\n",
    "Create SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Use a subset for SHAP (faster)\n",
    "X_train_shap = X_train_m[:100]  # Use 100 samples for explainability\n",
    "\n",
    "# Create SHAP explainer (use TreeExplainer for speed; fallback to KernelExplainer for SVM)\n",
    "explainer = shap.KernelExplainer(\n",
    "    lambda x: best_model.predict_proba(x)[:, 1],  # Prob of class 1\n",
    "    X_train_shap\n",
    ")\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_test_m[:10], nsamples=100)\n",
    "\n",
    "# Show shape\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize SHAP Results\n",
    "#### Summary Plot (Global Feature Importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first 10 test samples for summary\n",
    "shap_values = explainer.shap_values(X_test_m[:10], nsamples=100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test_m[:10], feature_names=[f'Feature_{i}' for i in range(50)], \n",
    "                 plot_type=\"bar\", color='blue', show=False)\n",
    "plt.title(\"SVM: Feature Importance (SHAP Bar Plot)\", fontsize=14)\n",
    "plt.ylabel(\"Mean |SHAP| Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Force Plot (Local Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction for first test sample\n",
    "idx = 0\n",
    "pred_prob = best_model.predict_proba(X_test_m[idx:idx+1])[0, 1]\n",
    "print(f\"Predicted probability (class 1): {pred_prob:.3f}\")\n",
    "\n",
    "# Force plot\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[idx], X_test_m[idx:idx+1], \n",
    "               feature_names=[f'Feature_{i}' for i in range(50)], show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependence Plot (Feature Effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how Feature_23 affects prediction\n",
    "shap.dependence_plot(\n",
    "    ind=23,  # Index of top feature\n",
    "    shap_values=shap_values,\n",
    "    features=X_test_m[:10],\n",
    "    feature_names=[f'Feature_{i}' for i in range(50)],\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"Feature Effect: Feature_23 on Prediction\", fontsize=14)\n",
    "plt.xlabel(\"Feature_23 Value\")\n",
    "plt.ylabel(\"SHAP Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:Explain a Prediction from Real Scientific Data\n",
    "Use the load_breast_cancer dataset and explain the prediction for a malignant tumor using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Re-train best model\n",
    "grid = GridSearchCV(create_svm_pipeline(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "mal = 2\n",
    "# Explain a malignant sample\n",
    "malignant_idx = np.where(y_test == 1)[0][0]  # First malignant test sample\n",
    "X_mal = X_test[malignant_idx:mal+1]\n",
    "\n",
    "# SHAP explainer\n",
    "explainer = shap.KernelExplainer(\n",
    "    lambda x: grid.best_estimator_.predict_proba(x)[:, 1],\n",
    "    X_train[:100]  # Use 100 for speed\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(X_mal, nsamples=100)\n",
    "\n",
    "# Plot\n",
    "shap.plots.waterfall(\n",
    "    shap_values=shap_values[0], \n",
    "    #feature_names=data.feature_names, \n",
    "    max_display=10\n",
    ")\n",
    "plt.title(\"SHAP Waterfall Plot: Malignant Tumor Prediction\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(shap.plots.waterfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
