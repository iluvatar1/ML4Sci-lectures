{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## A Brief History of the Support Vector Machine (SVM)\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm has a different lineage than statistically-rooted methods like logistic regression. Its development is a story of deep theoretical work on learning theory and optimization, culminating in a practical and powerful algorithm that dominated machine learning for many years.\n",
    "\n",
    "Here is a timeline of its key developments:\n",
    "\n",
    "*   **1963: The Linear Classifier**\n",
    "    *   The foundational ideas were first introduced by **Vladimir Vapnik** and **Alexey Chervonenkis**.\n",
    "    *   They proposed the \"optimal hyperplane\" algorithm, which is the basis for the linear SVM. The core idea was to find a decision boundary (a line, or hyperplane in higher dimensions) that is maximally far from the data points of either class. This distance is called the **margin**, and the goal was to maximize it. This was initially a linear classifier for perfectly separable data.\n",
    "\n",
    "*   **1970s: The Theoretical Underpinnings (VC Theory)**\n",
    "    *   Throughout the 1970s, Vapnik and Chervonenkis developed what is now known as **Vapnik-Chervonenkis (VC) Theory**, or more broadly, Statistical Learning Theory.\n",
    "    *   This theory provided a rigorous mathematical framework for understanding how a model generalizes from training data to unseen data. It introduced the concept of the **VC Dimension**, a measure of a model's capacity or complexity. This work established the principle of **Structural Risk Minimization (SRM)**, which aims to balance model complexity with its error on the training data, providing strong theoretical justification for the maximum-margin approach.\n",
    "\n",
    "*   **1992: The Kernel Trick for Non-Linearity**\n",
    "    *   The algorithm's true power was unlocked in a groundbreaking paper by **Bernhard Boser, Isabelle Guyon, and Vladimir Vapnik**.\n",
    "    *   They introduced the **\"kernel trick.\"** The problem was that the optimal hyperplane was a linear separator, but most real-world data isn't linearly separable.\n",
    "    *   The kernel trick is a mathematically elegant solution: it allows the algorithm to operate in a very high-dimensional feature space *without ever explicitly calculating the coordinates of the data in that space*. It does this by using a kernel function (like a Polynomial or Radial Basis Function) to calculate the dot products between images of data points in the feature space, which is all the algorithm needs. This made it possible to find complex, non-linear decision boundaries efficiently.\n",
    "\n",
    "*   **1995: The Soft Margin Classifier**\n",
    "    *   Another major practical limitation was that the original algorithm required the data to be perfectly separable (even in the higher-dimensional space).\n",
    "    *   **Corinna Cortes** and **Vapnik** introduced the \"soft margin\" formulation. This modified the optimization problem to allow for some data points to be misclassified or fall within the margin.\n",
    "    *   It introduced a \"cost\" hyperparameter, often denoted as `C`, that controls the trade-off between maximizing the margin and minimizing the classification error. A small `C` allows for a wider margin at the cost of more margin violations, while a large `C` penalizes violations heavily, leading to a narrower margin. This made SVMs robust to noisy data and applicable to virtually any dataset.\n",
    "\n",
    "*   **Late 1990s - 2000s: The Golden Age**\n",
    "    *   With the combination of the kernel trick and soft margins, SVMs became the state-of-the-art for many classification tasks.\n",
    "    *   They achieved top performance in areas like handwriting recognition (outperforming early neural networks), text categorization, and bioinformatics. Their strong theoretical backing and excellent empirical performance made them a dominant force in the machine learning community.\n",
    "\n",
    "*   **Present Day: A Powerful and Relevant Tool**\n",
    "    *   While Deep Learning and large neural networks have since become the state-of-the-art for unstructured data problems (like image, audio, and natural language processing), SVMs remain a powerful and relevant tool.\n",
    "    *   They are particularly effective for classification tasks with high-dimensional, structured data, and often perform very well on small-to-medium-sized datasets where deep learning models might overfit. They are still a go-to algorithm and an important benchmark in any machine learning practitioner's toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Motivation: Why Support Vector Machines?\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful class of supervised learning algorithms used for classification and regression tasks. While models like logistic regression are probabilistic, SVMs are deterministic and based on geometrical properties of the data. The core idea behind SVMs is to find an optimal hyperplane that best separates different classes of data points. This 'optimality' is achieved by maximizing the margin, which is the distance between the hyperplane and the closest data points from each class. These closest points are called **support vectors**, and they are the critical elements of the training data that define the decision boundary.\n",
    "\n",
    "- Maximizes **margin** â†’ better generalization.\n",
    "- Works in high-dimensional spaces (e.g., gene expression, spectroscopy)\n",
    "- Flexible with kernels for non-linear separation.\n",
    "- Robust to outliers with soft margins.\n",
    "- The number of features > number of samples (common in biochemistry, genomics).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" width=60%>\n",
    "<figcaption> https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's see a comparison between SVM and logistic regression for two different data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate linear data\n",
    "X_lin, y_lin = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, class_sep=2.0, random_state=42)\n",
    "\n",
    "# Generate non-linear data\n",
    "X_nonlin, y_nonlin = make_circles(n_samples=100, factor=0.5, noise=0.1, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, ax, title):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Fit models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for data, ax_row, kernel in zip([(X_lin, y_lin), (X_nonlin, y_nonlin)], axes, ['linear', 'rbf']):\n",
    "    X, y = data\n",
    "    log_reg = LogisticRegression().fit(X, y)\n",
    "    svm = SVC(kernel=kernel).fit(X, y)\n",
    "    plot_decision_boundary(log_reg, X, y, ax_row[0], 'Logistic Regression')\n",
    "    plot_decision_boundary(svm, X, y, ax_row[1], f'SVM ({kernel} kernel)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## SVM Fundamentals\n",
    "SVM finds the hyperplane that maximizes the margin (distance to nearest data points of each class). These nearest points are called support vectors.\n",
    "\n",
    "Given a dataset $(x_i, y_i)$ with $y_i \\in \\{-1, 1\\}$, SVM solves:\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2} ||w||^2 \\quad \\text{s.t.} \\quad y_i (w \\cdot x_i + b) \\geq 1,\n",
    "$$\n",
    "- $ \\mathbf{w} $: normal vector to hyperplane\n",
    "- $ b $: bias\n",
    "- $ y_i \\in \\{-1, +1\\} $\n",
    "- Margin = $ \\frac{2}{\\|\\mathbf{w}\\|} $\n",
    "\n",
    "This is the so-called hard-margin classifier. The actual hyperplane might not always exists, like in the case of the \"circular boundary\". \n",
    "\n",
    "**Soft-margin** \n",
    "\n",
    "When there is not linear (hyperplane boundary), a hinge-loss function is introduced,\n",
    "\\begin{equation}\n",
    "\\max(0, 1 - y_i(w\\cdot x_i - b)),\n",
    "\\end{equation}\n",
    "This function is 0 if the constrain is sattisfied, otherwise its value is proportional to the distance to the margin. This also allows for some misclassification, but this also makes the method less sentitive to outliers. The goal now is to minimize \n",
    "\\begin{equation}\n",
    "\\|\\mathbf{w}\\|^{2} \n",
    "+ C \\left[ \\frac{1}{n} \\sum_{i=1}^{n} \n",
    "\\max \\bigl(0,\\, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i - b)\\bigr) \\right],\n",
    "\\end{equation}\n",
    "where $C \\lt 0$ represents the trade-off between increasing the margin size and ansuring that $x_i$ ies on the correct size of the boundary.\n",
    "\n",
    "In this case, soft-margin SVM introduces slack variables $\\zeta_i$ and penalty $C$, and the problem becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},\\, b,\\, \\boldsymbol{\\zeta}} \\quad \n",
    "& \\|\\mathbf{w}\\|_{2}^{2} + C \\sum_{i=1}^{n} \\zeta_i \\\\[6pt]\n",
    "\\text{subject to} \\quad \n",
    "& y_i \\bigl(\\mathbf{w}^\\top \\mathbf{x}_i - b\\bigr) \\geq 1 - \\zeta_i, \n",
    "\\quad \\zeta_i \\geq 0, \n",
    "\\quad \\forall i \\in \\{1, \\ldots, n\\}.\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    ":::{exercise}\n",
    "Derive the dual form of the SVM optimization problem and explain the role of Lagrange multipliers.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Visualizing the Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create linearly separable data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# Fit the SVM model\n",
    "clf = svm.SVC(kernel='linear', C=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# Plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('Linearly Separable Data with SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Exploring the Margin\n",
    "1.  Modify the `C` parameter in the `svm.SVC` function in the code above. What do you observe about the margin and the number of support vectors when you use a very small `C` (e.g., 0.01) versus a very large `C` (e.g., 10000)?\n",
    "2.  What does the `C` parameter control in the context of the SVM classifier? *Hint: Think about the trade-off between a smooth decision boundary and classifying training points correctly.*\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}Identify Support Vectors\n",
    "Use `svm.support_vectors_` to extract and plot the support vectors on the above figure.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Comparison with Other Methods\n",
    "\n",
    "### SVM vs. Logistic Regression\n",
    "\n",
    "| Feature | Support Vector Machine (SVM) | Logistic Regression |\n",
    "|---|---|---|\n",
    "| **Underlying Principle** | Geometric: finds the optimal hyperplane that maximizes the margin between classes. | Statistical: models the probability of a certain class or event existing. |\n",
    "| **Decision Boundary** | Can be linear or non-linear using the kernel trick. | Fundamentally linear, though can be extended to non-linear with feature engineering. |\n",
    "| **Sensitivity to Outliers**| Less sensitive to outliers due to the margin-based optimization.  | Can be more sensitive to outliers as it tries to classify all points correctly. |\n",
    "| **Use Cases** | Effective in high-dimensional spaces and for both linear and non-linear problems. Works well with unstructured data like text and images. | Performs well on linearly separable data and when a probabilistic interpretation is needed. Often a good first model to try.  |\n",
    "| **Robust to overfitting** | Strong (especially in high-dim) | Moderate |\n",
    "\n",
    "In general, if the number of features is much larger than the number of training examples, logistic regression or a linear SVM is recommended. If the number of examples is intermediate and the data is not linearly separable, an SVM with a non-linear kernel is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Kernel Trick: Handling Non-Linear Data\n",
    "\n",
    "What if the data is not linearly separable? This is where the **kernel trick** comes in. The kernel trick is a powerful technique that allows SVMs to classify non-linear data.  It works by mapping the input data into a higher-dimensional space where a linear separator can be found. This is done implicitly, without ever having to compute the coordinates of the data in this higher-dimensional space, which is computationally efficient.\n",
    "\n",
    "Commonly used kernels include:\n",
    "*   **Linear:** For linearly separable data.\n",
    "*   **Polynomial:** Creates a polynomial decision boundary. $ K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d $\n",
    "*   **Radial Basis Function (RBF):** A popular choice for its flexibility in handling complex relationships. $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $\n",
    "*   **Sigmoid:** Can be useful in certain neural network-like scenarios.\n",
    "\n",
    "```{note}\n",
    "**RBF is default**: Handles complex, non-linear patterns (e.g., in metabolomics, protein folding)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=100, factor=.1, noise=.1, random_state=42)\n",
    "\n",
    "# Fit the SVM model with an RBF kernel\n",
    "clf = svm.SVC(kernel='rbf', C=10, gamma='auto', verbose=False)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# Plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('Non-Linearly Separable Data with RBF Kernel SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}Experimenting with Kernels\n",
    "\n",
    "1.  In the code above, change the `kernel` parameter to `'linear'` and `'poly'`. How does the decision boundary change? Which kernel performs best for this dataset?\n",
    "2.  For the RBF kernel, experiment with different values for the `gamma` parameter. What is the effect of a very small `gamma` versus a very large `gamma` on the decision boundary? What might this imply about the model's complexity and potential for overfitting?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A more complex boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Generate non-linear data\n",
    "scaler = StandardScaler()\n",
    "X_moon, y_moon = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
    "X_moon_scaled = scaler.fit_transform(X_moon)\n",
    "\n",
    "# Fit SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf.fit(X_moon_scaled, y_moon)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_moon_scaled[y_moon == 0, 0], X_moon_scaled[y_moon == 0, 1], c='red', label='Class 0', alpha=0.7)\n",
    "plt.scatter(X_moon_scaled[y_moon == 1, 0], X_moon_scaled[y_moon == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "\n",
    "# Decision boundary (grid)\n",
    "xx, yy = np.meshgrid(np.linspace(X_moon_scaled[:, 0].min(), X_moon_scaled[:, 0].max(), 100),\n",
    "                     np.linspace(X_moon_scaled[:, 1].min(), X_moon_scaled[:, 1].max(), 100))\n",
    "Z = svm_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='-', alpha=0.7)\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdBu')\n",
    "\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.title('SVM with RBF Kernel: Non-Linear Separation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Compare SVM vs Logistic Regression\n",
    "Fit a logistic regression model on the same moons dataset. Compare accuracy and decision boundary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d5f9026d575cd22994edba522fd5530",
     "grade": false,
     "grade_id": "cell-4691c03f3aff962f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_moon, y_pred_lr):.3f}\")\n",
    "print(f\"SVM (RBF) Accuracy: {accuracy_score(y_moon, svm_rbf.predict(X_moon_scaled)):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive role of hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data\n",
    "X, y = make_classification(n_samples=150, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create interactive controls\n",
    "@widgets.interact(\n",
    "    C=widgets.FloatSlider(value=1.0, min=0.1, max=10.0, step=0.1, description='C'),\n",
    "    gamma=widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01, description='Gamma'),\n",
    "    kernel=widgets.Dropdown(options=['rbf', 'linear', 'poly'], value='rbf', description='Kernel')\n",
    ")\n",
    "def plot_svm(C=1.0, gamma=0.1, kernel='rbf'):\n",
    "    # Fit SVM\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma, random_state=42)\n",
    "    svm.fit(X_scaled, y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', label='Class 0', alpha=0.7)\n",
    "    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "    \n",
    "    # Decision boundary (grid)\n",
    "    xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100),\n",
    "                         np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100))\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='-', alpha=0.8)\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdBu')\n",
    "    \n",
    "    # Support vectors\n",
    "    sv = svm.support_vectors_\n",
    "    plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='green', linewidth=2, label='Support Vectors')\n",
    "    \n",
    "    plt.xlabel('Feature 1 (scaled)')\n",
    "    plt.ylabel('Feature 2 (scaled)')\n",
    "    plt.title(f'SVM Decision Boundary (C={C}, gamma={gamma:.2f}, kernel={kernel})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Applications in Basic Sciences\n",
    "\n",
    "SVMs have found numerous applications in various scientific domains:\n",
    "\n",
    "*   **Bioinformatics:** SVMs are widely used for tasks like protein classification, gene expression analysis, and cancer classification. Their ability to handle high-dimensional data makes them suitable for analyzing genomic and proteomic datasets.\n",
    "*   **Image Classification:** In fields like medical imaging and satellite imagery analysis, SVMs can be used to classify images, for instance, to identify tumors in medical scans or to classify different types of land cover from satellite data. \n",
    "*   **Chemistry:** SVMs can be used in cheminformatics to predict the properties of molecules, such as their bioactivity or toxicity, based on their chemical structure.\n",
    "\n",
    "### Biochemistry: Enzyme Substrate Classification\n",
    "Features: 3D molecular descriptors (e.g., hydrophobicity, size, charge).\n",
    "Task: Classify if a molecule is a substrate (yes/no) for an enzyme.\n",
    "Why SVM? High-dim, small samples, non-linear relationships\n",
    "\n",
    "### Spectroscopy: Material Phase Classification\n",
    "Features: Raman or IR intensity at 1000+ wavenumbers.\n",
    "Task: Classify solid vs. liquid phase.\n",
    "Why SVM? RBF kernel captures subtle spectral shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise}\n",
    "Dataset: breast_cancer from sklearn (classic biomedical dataset).\n",
    "Goal: Classify tumors as malignant/benign using 30 morphological features.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f962b0cd3e849857e75cff216e93e541",
     "grade": false,
     "grade_id": "cell-4551882fe3fbfc1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## EXTRA: scikit pipeline\n",
    " A full scikit pipeline allows you to estimate the best hyper parameters for a given problem. It uses <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Build full pipeline\n",
    "def create_svm_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=10)),  # Select top 10 features\n",
    "        ('svm', SVC(kernel='rbf', random_state=42))\n",
    "    ])\n",
    "\n",
    "# Use on breast cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = create_svm_pipeline()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'feature_selection__k': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_.round(3))\n",
    "\n",
    "# Final prediction\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Comparing svm and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compare_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'SVM (RBF)': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=10)),\n",
    "            ('clf', SVC(kernel='rbf', C=10, gamma=0.01, probability=True))\n",
    "        ]),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=10)),\n",
    "            ('clf', LogisticRegression(max_iter=1000))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        results[name] = {'test_score': score, 'cv_mean': cv_scores.mean(), 'cv_std': cv_scores.std()}\n",
    "        print(f\"{name}: Test Accuracy = {score:.3f}, CV Accuracy = {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare on breast cancer\n",
    "compare_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Build Your Own Pipeline\n",
    "Use make_classification to generate a dataset with 100 features, 500 samples, and non-linear structure.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fefbfff147bbb37f45ffebb305aec9a6",
     "grade": false,
     "grade_id": "cell-d972df28fb08ab5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## EXTRA: Support Vector Machines (SVM) with Model Explainability (SHAP)\n",
    "Please look at <https://shap.readthedocs.io/en/latest/index.html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
