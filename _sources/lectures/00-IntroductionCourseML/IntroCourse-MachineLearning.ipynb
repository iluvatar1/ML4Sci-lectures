{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the course and to Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this course\n",
    "Este es un curso introductorio al aprendizaje automático (machine learning - ML) pensado para estudiantes de ciencias naturales (por ejemplo astronomia, biología, ecología, estadística, farmacia, física, geología, matemáticas, medicina, meteorología, química) y enfocado al uso de estos nuevos métodos en problemas científicos. El curso introduce conceptos, técnicas y aplicaciones del aprendizaje automático desde una perspectiva de la modelación y la predicción en problemas científicos, cubriendo desde temas básicos como clasificación y regresión lineal hasta enfoques más actuales como redes neuronales y acercamientos al aprendizaje profundo, con aplicaciones a imágenes, series temporales, catálogos, y en general bases de datos de relevancia científica. El curso ofrecerá al estudiante además de las ideas e intuición básica detrás del aprendizaje automático, un entendimiento formal del que le permitirá responder cuándo, cómo y por qué utilizar estos métodos.\n",
    "\n",
    "### Metodología\n",
    "La asignatura tiene una orientación teórico-práctica. Se desarrolla preferencialmente usando casos de estudio que permitan dar un contexto aplicado a la introducción de los conceptos fundamentales de ML, orientado a problemas científicos. Estos casos de estudio pueden tocar uno o más temas del contenido de la asignatura. Se usarán intensivamente herramientas computacionales, como notebooks de python en diversas plataformas, además de librerías propias del área, y en casos específicos, hardware gpu. Se espera que el estudiante desarrolle trabajo durante la clase y autónomamente fuera de la misma. La evaluación se enfocará en talleres prácticos y proyectos aplicados. Se invitarán expertos para compartir experiencias y crear y fortalecer lazos de investigación y mentoría. \n",
    "\n",
    "### Conceptos previos\n",
    "Para desarrollar exitosamente la asignatura, se espera que el estudiante tenga nociones básicas de: \n",
    "- Programación - Saber representar un problema de forma algorítimica y estar familiarizado con las sintaxis básicas y propias de un lenguaje como python. \n",
    "- Cálculo diferencial - Comprender el uso y aplicación de la derivada y su relación con procedimirntos de optimización, además de una generalización básica a varias dimensiones, sin necesitar un curso de cálculo vectorial.\n",
    "- Álgebra lineal - Comprender los conceptos de multiplicación de matrices, espacios vectoriales, transformaciones lineales.\n",
    "\n",
    "Adicionalmente, se sugiere, pero no es obligatorio, tener conocimientos básicos de probabilidad y estadística, tales como eventos independientes/dependientes, muestras aleatorias, etc\n",
    "\n",
    "### Contents\n",
    "<img src=\"./figs/Course-Contents.png\" alt=\"under-over-truncation\" class=\"centerimg80\">\n",
    "\n",
    "### Evaluación\n",
    "\n",
    "- Módulo I: 40%\n",
    "- Módulo II: 60%\n",
    "\n",
    "La metodología de evaluación es mixta, usando diferentes recursos que evalúan diferentes aspectos del aprendizaje:\n",
    "\n",
    "a) Tareas normales que se califican sobre 5.0. Estas puede ser tareas sencillas de un día para otro, o workshops en donde ustedes entregan y además revisan el trabajo de sus compañeros, y la nota final depende de la calidad de lo que entregó y de lo bien que evaluó (se castigan calificaciones demasiado buenas, demasiado malas, etc)\n",
    "\n",
    "b) Minitareas, que normalmente valen entre 1.0 a 2.0, y se van acumulando hasta convertirse en una tarea completa que puede sumar más de 5.0 (ahí está la ayuda en bonos). Estas muchas veces se hacen en clase y al sumar más de 5.0 permiten que usted no presente alguna y aún así aspire a sacar 5.0. Adicionalmente, pueden consistir de videos de preparación de clase y otras actividades. \n",
    "\n",
    "c) Proyectos se busca la solución de problemas más complejos , en grupo. Allí se deben fortalecer las habilidades blandas y usar herramientas apropiadas para coordinar el trabajo en grupo. Adicionalmente, la nota final de estos trabajos en grupo depende también de una evaluación anónima que hagan sus propios compañeros sobre su contribución, de manera que si usted no trabaja, no obtendrá la misma nota que los demás aunque estén en el mismo grupo. \n",
    "\n",
    "### Atención de dudas\n",
    "1. **Foro en moodle: micampus.unal.edu.co**.\n",
    "2. **Email con la pregunta claramente detallada**.\n",
    "3. **Reservas usando el calendario de google**.\n",
    "4. Email proponiendo al menos dos horarios de reunión e indicando la pregunta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning\n",
    "\n",
    "> Machine learning is the field of study that gives computers the ability\n",
    "to learn without being explicitly programmed. Arthur Samuel, 1959\n",
    "\n",
    "Learning:\n",
    "> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. Tom Mitchell’s, 1997\n",
    "\n",
    "Machine learning is a branch of artificial intelligence that allows computers to learn from given information and perform new but similar tasks. \n",
    "\n",
    "Artificial intelligence: The compuyter performs actions defined as requiring intelligence -> Moving target\n",
    "- Search Based Heuristic Optimization\n",
    "- Evolutionary computation\n",
    "- Logic Programming (inductive logic programming, fuzzy logic)\n",
    "- robabilistic Reasoning Under Uncertainty (bayesian networks)\n",
    "- Computer Vision\n",
    "- Natural Language Processing\n",
    "- Robotics\n",
    "- Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Historical development\n",
    "- https://letsdatascience.com/learn/history/history-of-machine-learning/\n",
    "- https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/2-history-of-ML/README.md\n",
    "- https://www.inveniam.fr/a-brief-history-of-machine-learning\n",
    "- https://ahistoryofai.com/\n",
    "\n",
    "### Brief history\n",
    "|Period||\n",
    "|:---:|:---:|\n",
    "|Antiguity: Greek myths referencing intelligent robots and artifical begins.| <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b1/Lawrence_Alma-Tadema_10.jpeg\" alt=\"Drawing\" style=\"width: 250px;\"/> |\n",
    "| 800, talmudic period, medieval times: search for artifical life and devices to answer questions. | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Prague-golem-reproduction.jpg/440px-Prague-golem-reproduction.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> |\n",
    "|1200: Lullian circle to perform logical operations|  <img src=\"http://www.computer-timeline.com/wp-content/uploads/2019/12/Llull2.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1495: Leonardo davinci's Mechanical knight|  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Leonardo-Robot3.jpg/440px-Leonardo-Robot3.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|...||\n",
    "|1940: Edward Uhler created the Nimatron| <img src=\"https://upload.wikimedia.org/wikipedia/en/1/1b/Nimatron_Photograph_1940.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1943: McCulloch and Pitts develop the first artifical neuron|<img src=\"https://historyofinformation.com/images/Screen_Shot_2020-09-09_at_6.46.46_AM_big.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1950: Shannon publishes \"Programming a computer to play chess\". Also, Turing test is formulated. |<img src=\"https://time.graphics/uploadedFiles/500/36/3e/363ea30a75e282e5c52614a95746814d.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "| 1950: Alan Turing explores the possibility of machines to mimic human behaviour.| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Alan_Turing_%281912-1954%29_in_1936_at_Princeton_University.jpg/440px-Alan_Turing_%281912-1954%29_in_1936_at_Princeton_University.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1951: Minsky first neural network machine| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Marvin_Minsky_at_OLPCb.jpg/440px-Marvin_Minsky_at_OLPCb.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1955: Logist Theorist, by Allen Newell, J.C. Shaw, and Hertbert Simon at Carnegie Institute of Technology| <img src=\"http://www.computer-timeline.com/wp-content/uploads/2023/02/Simon_and_Newell.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "| 1956: The [Logic Theorist](https://ahistoryofai.com/logic-theorist/) become the first artificial intelligence program ever created. ||\n",
    "|1956: John McCarthy coined the term “artificial intelligence”. Golden years of AI| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/John_McCarthy_Stanford.jpg/440px-John_McCarthy_Stanford.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1957: Frank Rosenblatt invented the Perceptron (it is non boolean and accepts weights| <img src=\"https://maelfabien.github.io/assets/images/neuron_4.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1959: Herbert Simon, programmer J. C. Shaw, and computer scientist Allen Newell create the General Problem Solver based on logic machine architecture| |\n",
    "|1963: John McCarthy, MIT Artificial Intelligence Lab; McCarthy and Marvin Minskey launched SAIL: Stanford Artificial Intelligence Laboratory ||\n",
    "|1963: Foundation of kernel methods. Aizerman, Braverman, and Rozonoer|  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Kernels.svg/1000px-Kernels.svg.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1965: Joseph Weizenbaum created the computer program Eliza|<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/ELIZA_conversation.png/660px-ELIZA_conversation.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1966: Ross Qullian showed semantic networks to represent human knowledge|<img src=\"https://ahistoryofai.files.wordpress.com/2018/12/semanticnetworkgray.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1967: The Nearest Neighbor Rule, Cover and Hart introduced the k-nearest neighbor algorithm| <img src=\"https://upload.wikimedia.org/wikipedia/en/8/87/ThomasMCover.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1968: Alexey Ivakhnenko, a Ukrainian scientist, introduced the Group Method of Data Handling (GMDH)| <img src=\"https://upload.wikimedia.org/wikipedia/en/9/98/Alexey_Ivakhnenko%2C_Kyiv_1967.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1969: Marvin Minsky and Seymour Papert presented limitations of perceptrons and the Rosenblatt's perceptron learning theorem. ||\n",
    "|1970: Predictions on near human like artifical intelligence were overconfident. First AI Winter until 1980||\n",
    "|1973-74-75: SOme algorithms are developed, like IAD, CART and ID3||\n",
    "|1980: Golden era for AI starts. First internationl conference. Edward Feigenbaum developed a computer that makes decisions as a human can| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/27._Dr._Edward_A._Feigenbaum_1994-1997.jpg/440px-27._Dr._Edward_A._Feigenbaum_1994-1997.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1980: Kunihiko Fukushima published his work on the neocognitron, a deep convolutional neural network| <img src=\"https://fi.edu/sites/default/files/2021-08/Kunihiko_Fukushima.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> |\n",
    "|1985: NetTalk, pronounced words as a baby| <img src=\"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcTXVxcNDvd7g-IgJaDsvBu0gT62zFjqcp9ufGhQxMq-ZemloKCP\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1986: Back propagation algorithm,David Rumelhart, Geoffrey Hinton, and Ronald Williams| <img src=\"https://img.youtube.com/vi/Ilg3gGewQ5U/maxresdefault.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1986: Carnegie Mellon University engineers built Navlab, the first autonomous car. | <img src=\"https://media.techeblog.com/images/elon-musk-tesla-chevrolet-cmu-navlab-self-driving-car.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1986: Linear predictive programming, speech recocnition| <img src=\"https://fi.edu/sites/default/files/2015-01/Laureates_BenajminFranklinAward_2003_OldSite_BishnuSAtal_Atal2.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1987: Computer scientist Natarajan Shankar used the Nqthm theorem-prover to prove Gödel’s first incompleteness theorem.| |\n",
    "|1989: Richard S. Sutton published a paper that helped to better formalize the ideas of reinforcement learning.| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/500px-Reinforcement_learning_diagram.svg.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1995: Support Vector Machines (SVM), Vladimir Vapnik and Cortes developed the Support Vector Machine (SVM) method for linear classification| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/600px-SVM_margin.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|1966: Ensemble methods, Bagging. Aggrgation of multiple decision trees results| <img src=\"https://upload.wikimedia.org/wikipedia/en/4/4d/Leo_Breiman.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|...||\n",
    "|2001: Bagging and Random Forests||\n",
    "|2005: Classical ML era, statistical learning: linear regression, clustering, etc. Recommendation era. Netflix price||\n",
    "|2009: “Eureka machine” by Cornell scientists generated Newton’s three laws of motion in a few hours||\n",
    "|2009: Image net database| <img src=\"https://production-media.paperswithcode.com/datasets/ImageNet-0000000008-f2e87edd_Y0fT5zg.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2010: Microsoft kinect, computer vision in the market| <img src=\"https://media.wired.com/photos/59333c7626780e6c04d2eca9/master/w_2240,c_limit/Kinect-Windows.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2010: Deep learning era. Neural networks, convolutional models, and GPU use| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/269px-Artificial_neural_network.svg.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2011: IBM’s Watson defeated Jeopardy! champions Brad Rutter and Ken Jennings| <img src=\"https://avatars.githubusercontent.com/u/9221727?s=200&v=4\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2014: Generative Adversarial Networks (GANs)| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Ian_Goodfellow.jpg/440px-Ian_Goodfellow.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2015: Large scale era.||\n",
    "|2015: Google DeepMind’s AlphaGo beat 3 time European Go champion 2 Fan Hui.| <img src=\"https://lh3.googleusercontent.com/mk1-ulqF037tTLFwTE1opVUKTlvkNVgcKfSRpsbfs5ZV6xW8HtqakWtNvX0OYeZawFQcchkUJIMWiy38UmtvC0F4wdKo5wkN7zb2D9aJl2iqbNy_Ug=w2140-rw\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2017: Attentions is all you need, google. Transformers as origin of LLM| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/580px-The-Transformer-model-architecture.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n",
    "|\n",
    "|2018: The AI Now Report 2018 by the AI Now Institute revealed unsafe and poor practices by IBM Watson, the U.S. Immigration and Customs Enforcement, the Xinjiang Autonomous Region, and Amazon’s Recongnition tool||\n",
    "|2019: oogle’s AlphaStar defeated pro StarCraft II players. Visual processing| <img src=\"https://lh3.googleusercontent.com/mxneffVMqhQ1zgfuBeJjSSYsjZKoPUStbjvgNjtUnURToE1TTj4J0FekTeT7rCtDAJY6daqpe-8cw6DSZEg26TUPvo5HiBHQ6EVZAPeRKiWxjLmXb50=w1232-rw\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2022: ChatGPT and the boom of LLM. | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/ChatGPT_logo.svg/360px-ChatGPT_logo.svg.png\" alt=\"Drawing\" style=\"width: 250px;\"/>|\n",
    "|2023...: Generative AI, [Sora](https://openai.com/index/sora/) (video creation) , [stable difussion](https://stability.ai/) (image creation) , midjourney,  ...| <img src=\"https://imageio.forbes.com/specials-images/imageserve/650945e2810848cde5016621/What-Is-Generative-AI--A-super-Simple-Explanation-Anyone-Can-Understand/960x0.jpg?format=jpg&width=1440\" alt=\"Drawing\" style=\"width: 250px;\"/>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
