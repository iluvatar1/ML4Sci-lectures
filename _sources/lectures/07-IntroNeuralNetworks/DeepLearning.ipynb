{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# An Introduction to Deep Learning\n",
    "\n",
    "**Why Deep Learning?**\n",
    "- Automatic feature extraction from raw data.\n",
    "- Scales to large datasets.\n",
    "- Can approximate any continuous function (Universal Approximation Theorem).\n",
    "\n",
    "**Applications in Basic Sciences:**\n",
    "- Physics: Predicting particle trajectories.\n",
    "- Chemistry: Predicting molecular properties.\n",
    "- Biology: Classifying cells in microscopy images.\n",
    "\n",
    "**Quick Links**\n",
    "- [TensorFlow Playground](https://playground.tensorflow.org/)\n",
    "- [3Blue1Brown — What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [MIT Lecture: Deep Learning Basics](https://youtu.be/n1ViNeWhC24)\n",
    "- [Distill.pub Momentum Visualization](https://distill.pub/2017/momentum/)\n",
    "- [Why deep learning works unreasonable well](https://www.youtube.com/watch?v=qx7hirqgfuU)\n",
    "- [The moment we stopped understanding AI](https://www.youtube.com/watch?v=UZDiGooFs54)\n",
    "- [These Numbers Can Make AI Dangerous (Subliminal learning)](https://www.youtube.com/watch?v=NUAb6zHXqdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Historical Overview: The Rise of Neural Networks\n",
    "\n",
    "### 1940s–1950s: Birth of the Perceptron\n",
    "- **1943**: McCulloch & Pitts propose a computational model of neurons.\n",
    "- **1958**: Frank Rosenblatt introduces the **Perceptron** — a single-layer neural network.\n",
    "  - Could classify linearly separable data.\n",
    "  - Limited by the XOR problem (Minsky & Papert, 1969).\n",
    "\n",
    "> *“The Perceptron is the first machine learning model capable of learning from data.”*\n",
    "\n",
    "### 1980s–1990s: Backpropagation and the First Wave\n",
    "- **1986**: Rumelhart, Hinton & Williams publish **backpropagation algorithm** — enabling training of multi-layer networks.\n",
    "- **1989**: LeCun et al. apply CNNs to handwritten digit recognition (MNIST precursor).\n",
    "- **Limitations**: Lack of data, weak compute power → Neural networks fall out of favor.\n",
    "\n",
    "### 2000s–2010s: The Deep Learning Revolution\n",
    "- **2006**: Hinton et al. introduce **deep belief networks** and the term “Deep Learning”.\n",
    "- **2012**: AlexNet (Krizhevsky, Sutskever, Hinton) wins ImageNet with CNN + ReLU + Dropout → **Deep Learning boom**.\n",
    "- **2014**: GANs (Goodfellow), Seq2Seq, attention mechanisms.\n",
    "- **2017**: Transformer architecture (Vaswani et al.) — basis for modern LLMs.\n",
    "- **2020s**: Scaling laws, vision transformers (ViT), multimodal models (CLIP, DALL·E), [LLMs](https://jalammar.github.io/illustrated-transformer/) (GPT, Gemini).\n",
    "\n",
    "> **Key Enablers**: GPU acceleration, Big Data (ImageNet), open-source frameworks (TensorFlow, PyTorch)\n",
    "\n",
    "### Timeline Summary\n",
    "\n",
    "```{image} fig/deeplearning-history.png\n",
    "   :alt: \n",
    "   :class: bg-primary\n",
    "   :width: 70%\n",
    "   :align: center\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Neural Networks: Structure and Nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Shallow neural network:\n",
    "\n",
    "```{image} fig/nn-shallow.png\n",
    "   :alt: shallow\n",
    "   :class: bg-primary\n",
    "   :width: 80 %\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### What is a (Deep) Neural Network?\n",
    "A function composed of **layers** of interconnected neurons:\n",
    "\n",
    "```{image} fig/nn.svg\n",
    "   :alt: asdas\n",
    "   :width: 100%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aircAruvnKk?si=FIIlvaq4-qrDh1u2&amp;start=218\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Each neuron computes:  \n",
    "$$z = \\sum w_i x_i + b \\to a = \\sigma(z)$$\n",
    "\n",
    "Where:\n",
    "- $w$: weights\n",
    "- $x$: inputs\n",
    "- $b$: bias\n",
    "- $\\sigma$: activation function (e.g., ReLU, Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A pytorch implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data generation\n",
    "import numpy as np\n",
    "# 1. Generate X data (input/features)\n",
    "# Create 1000 evenly spaced points from 0 to 2*pi\n",
    "X = np.linspace(0, 2 * np.pi, 1000).astype(np.float32)\n",
    "\n",
    "# Reshape X to be a column vector (1000 samples, 1 feature)\n",
    "# PyTorch/NN models typically expect a 2D input array (samples, features)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# 2. Generate y data (output/labels)\n",
    "# Base sine function\n",
    "y_base = np.sin(X) - 0.1*X\n",
    "\n",
    "# Add noise (e.g., normal distribution noise)\n",
    "noise = np.random.normal(loc=0.0, scale=0.1, size=X.shape).astype(np.float32)\n",
    "\n",
    "# Final noisy y data\n",
    "y = y_base + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!uv pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Toy dataset\n",
    "X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_torch = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 20), # linear combination from one input to 20 neurons\n",
    "    nn.Tanh(), # Activation function per neuron\n",
    "    nn.Linear(20, 1) # \n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) # Is lr useful?\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_torch)\n",
    "    loss = loss_fn(y_pred, y_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Move data to CPU for plotting\n",
    "X_cpu = X_torch.detach().cpu().numpy()\n",
    "y_cpu = y_torch.detach().cpu().numpy()\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_torch).cpu().numpy()\n",
    "\n",
    "# Sort for a smooth plot\n",
    "sorted_idx = np.argsort(X_cpu.flatten())\n",
    "X_sorted = X_cpu.flatten()[sorted_idx]\n",
    "y_sorted = y_pred.flatten()[sorted_idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_cpu, y_cpu, label=\"Data\", color=\"blue\")\n",
    "plt.plot(X_sorted, y_sorted, label=\"Model Prediction\", color=\"red\", linewidth=2)\n",
    "plt.title(\"Model Fit to Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}\n",
    "Play with hyperparameters\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Core of Deep Learning: Deep Neural Networks\n",
    "\n",
    "The process of passing input data through the network to get an output is called **forward propagation**. For each neuron, we calculate a weighted sum of the outputs from the previous layer, add a bias, and then pass this result through a non-linear **activation function**.\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "For a single neuron *j* in layer *l*, its output *a<sub>j</sub><sup>(l)</sup>* is:\n",
    "\n",
    "$z_j^{(l)} = \\sum_k (w_{jk}^{(l)} \\cdot a_k^{(l-1)}) + b_j^{(l)}$\n",
    "\n",
    "$a_j^{(l)} = g(z_j^{(l)})$\n",
    "\n",
    "Where:\n",
    "- $a_k^{(l-1)}$ is the activation of the *k*-th neuron in the previous layer.\n",
    "- $w_{jk}^{(l)}$ is the weight of the connection from neuron *k* to neuron *j*.\n",
    "- $b_j^{(l)}$ is the bias of neuron *j*.\n",
    "- $g$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Common Activation Functions\n",
    "\n",
    "The most common activation functions are Sigmoid, Tanh, and **ReLU (Rectified Linear Unit)**. ReLU is the most popular choice for hidden layers in deep learning today because it helps mitigate a problem called the \"vanishing gradient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's visualize the common activation functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z, relu(z))\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z, tanh(z))\n",
    "plt.title('Tanh Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Relu activation\n",
    "If a neuron in a hidden layer uses a ReLU activation function and its input *z* is -5, what will be its output? What if the input *z* is 5?\n",
    ":::\n",
    "\n",
    "Relu activation is normally used since it is easy to compute and also helps to control the vanishing gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Compute requirements:\n",
    "\n",
    "- For dense (fully connected) layer: cost is O(nin×nout) multiplies per sample.\n",
    "\n",
    "- For convolutional layer: cost depends on kernel size, input channels, output channels and spatial size — often more efficient than dense for images due to weight sharing.\n",
    "\n",
    "- Memory: store activations for backward pass (unless using checkpointing).\n",
    "\n",
    "- Training scales with dataset size, model size and number of epochs; GPUs / TPUs accelerate matrix ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://archive.ourworldindata.org/20251007-111536/grapher/exponential-growth-of-datapoints-used-to-train-notable-ai-systems.html?tab=chart\" style=\"width: 100%; height: 600px; border: 0px none;\" allow=\"web-share; clipboard-write\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://ourworldindata.org/grapher/hardware-and-energy-cost-to-train-notable-ai-systems?tab=chart\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\" allow=\"web-share; clipboard-write\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://ourworldindata.org/grapher/exponential-growth-of-computation-in-the-training-of-notable-ai-systems?tab=chart\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\" allow=\"web-share; clipboard-write\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{image} https://iee.psu.edu/sites/default/files/styles/large_1200_2400_/public/image/total-server-electricity-consumption.png?itok=yNSLVfzC\n",
    "   :alt: energy\n",
    "   :class: bg-primary\n",
    "   :width: 80%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- <https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/>\n",
    "- <https://www.nature.com/articles/d41586-025-00616-z>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Learning from Mistakes: Backpropagation\n",
    "\n",
    "How does the network learn the correct values for its weights and biases?\n",
    "\n",
    "1.  It first makes a prediction using **forward propagation**.\n",
    "2.  It measures how wrong that prediction is using a **loss function** (e.g., Mean Squared Error or Cross-Entropy).\n",
    "3.  It calculates the gradient of the loss with respect to every weight and bias in the network. This is done efficiently via an algorithm called **[backpropagation](https://en.wikipedia.org/wiki/Backpropagation?useskin=vector)**, which is essentially an application of the chain rule from calculus.\n",
    "4.  It uses an **optimizer** (like Gradient Descent, **Adam, Ada Delta, Adagrad, RMSProp**) to update the weights and biases in the direction that minimizes the loss.\n",
    "\n",
    "This cycle is repeated many times with the training data. The core of backpropagation is figuring out how much each parameter contributed to the error, and propagating this error information \"backward\" from the output layer to the input layer.\n",
    "\n",
    "The error $\\delta$ in a hidden layer *l* is calculated based on the errors in the next layer *l+1*:\n",
    "\n",
    "$\\delta^{(l)} = ((W^{(l+1)})^T \\delta^{(l+1)}) \\odot g'(z^{(l)})$\n",
    "\n",
    "Where $\\odot$ is element-wise multiplication and $g'(z^{(l)})$ is the derivative of the activation function.\n",
    "\n",
    "```{figure} fig/SGD.png\n",
    "   :figwidth: 90 %\n",
    "   <https://introtodeeplearning.com/>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Visualization  \n",
    "- <https://xnought.github.io/backprop-explainer/>\n",
    "- <https://www.geogebra.org/m/dyq2rcup>\n",
    "- <https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>\n",
    "- <https://www.youtube.com/watch?v=Ilg3gGewQ5U>\n",
    "- <https://www.youtube.com/watch?v=VkHfRKewkWw&t=0s>\n",
    "- <https://visionbook.mit.edu/backpropagation.html>\n",
    "- <https://www.youtube.com/watch?v=SmZmBKc7Lrs>\n",
    ":::{exercise} Relu and linear activation\n",
    "Why is the derivative of the activation function ($g'$) important in the backpropagation equation above? What would happen if we used a linear activation function (where $g'(z)$ is just a constant) in all hidden layers?\n",
    ":::\n",
    "\n",
    "### How complex is the loss landscape?\n",
    "```{figure} fig/loss-01.png\n",
    ":figwidth: 70%\n",
    "   Visualizing the loss landscape of Neural nets, 2017, hao Li et. al. <https://arxiv.org/abs/1712.09913> , <https://github.com/tomgoldstein/loss-landscape>\n",
    "```\n",
    "\n",
    "\n",
    "Some visualizations:\n",
    "- <https://www.telesens.co/loss-landscape-viz/viewer.html>, from <https://www.cs.umd.edu/~tomg/projects/landscapes/>\n",
    "- <https://losslandscape.com/explorer>\n",
    "\n",
    "```{figure} fig/loss-02.png\n",
    ":figwidth: 70%\n",
    "   <https://www.youtube.com/watch?v=NrO20Jb-hy0> \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NrO20Jb-hy0?si=i_7UuC9dcFfm781K\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Muon Optimizer\n",
    "https://www.youtube.com/watch?v=bO5nvE289ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Double descent\n",
    "https://www.youtube.com/watch?v=z64a7USuGX0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Deep learning : some arquitectures\n",
    "\n",
    "See: <https://mriquestions.com/deep-network-types.html>\n",
    "\n",
    "---\n",
    "- **Convolutional Neural Networks (CNNs)**\n",
    "```{image} https://mriquestions.com/uploads/3/4/5/7/34572113/screenshot-2024-09-04-at-3-35-24-pm_orig.png\n",
    "   :alt: ccn\n",
    "   :class: bg-primary\n",
    "   :width: 90%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n",
    "Check it step-by-step at <https://www.youtube.com/watch?v=jDe5BAsT2-Y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UZDiGooFs54?si=qGN1zTpa4F8RvrMS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a nice cnn visualization for the mist digits:\n",
    "<https://adamharley.com/nn_vis/cnn/3d.html>\n",
    "\n",
    "```{image} fig/cnn-3d.png\n",
    "   :alt: cnn-3d\n",
    "   :class: bg-primary\n",
    "   :width: 100%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- **Encoder-Decoder Networks**\n",
    "```{image} https://mriquestions.com/uploads/3/4/5/7/34572113/screenshot-2024-09-04-at-3-39-34-pm_orig.png\n",
    "   :alt: encoder decoder\n",
    "   :class: bg-primary\n",
    "   :width: 90%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n",
    "---\n",
    "- **Generative Adversarial Networks (GANs)**\n",
    "```{image} https://mriquestions.com/uploads/3/4/5/7/34572113/editor/gan-diagram.png?1643144211\n",
    "   :alt: GAN\n",
    "   :class: bg-primary\n",
    "   :width: 90%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "---\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "```{image} https://mriquestions.com/uploads/3/4/5/7/34572113/published/recurrent-network.png?1643575415\n",
    "   :alt: rnn\n",
    "   :class: bg-primary\n",
    "   :width: 70%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n",
    "---\n",
    "- **Transformer Neural Networks (TNNs)**\n",
    "Used in NLP, Vision, Multimodal models. https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)?useskin=vector\n",
    "```{image} https://mriquestions.com/uploads/3/4/5/7/34572113/transformer_orig.png\n",
    "   :alt: transformer\n",
    "   :class: bg-primary\n",
    "   :width: 50%\n",
    "   :align: center\n",
    "   # CHECK https://myst-parser.readthedocs.io/en/latest/syntax/images_and_figures.html#block-level-images\n",
    "```\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hands-On: Classifying MNIST Digits\n",
    "\n",
    "We’ll classify handwritten digits (28x28 grayscale) using 4 frameworks.\n",
    "\n",
    "Dataset: MNIST (70,000 images, 10 classes)\n",
    "\n",
    "| Feature                  | Scikit-learn        | PyTorch             | Keras               | TensorFlow          |\n",
    "|--------------------------|---------------------|---------------------|---------------------|---------------------|\n",
    "| **Ease of Use**          | Easy                | Moderate            | Very Easy           | Moderate            |\n",
    "| **Flexibility**          | Low                 | Very High           | High                | High                |\n",
    "| **GPU Support**          | No                  | Yes                 | Yes                 | Yes                 |\n",
    "| **Debugging**            | Easy                | Very Easy           | Moderate            | Difficult           |\n",
    "| **Production Ready**     | Yes (small models)  | Yes                 | Yes                 | Excellent           |\n",
    "| **Best For**             | Quick ML baseline   | Research, custom models | Prototyping     | Industry-scale apps |\n",
    "| **Learning Curve**       | Low                 | Steep               | Moderate            | Steep               |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Scikit-learn (MLPClassifier)\n",
    "Simple, fast, but no GPU use — limited to shallow nets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=10, random_state=42, verbose=True)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(f\"Scikit-learn Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "###  PyTorch (Custom NN)\n",
    "Full control, GPU-ready, great for learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transform and load data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "# Define model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"PyTorch Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Keras (High-Level API)\n",
    "“Write less, do more.” — Ideal for prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Keras Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### TensorFlow (Low-Level)\n",
    "Full control over gradients — used in research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load data (same as above)\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, X_test = tf.cast(X_train / 255.0, tf.float32), tf.cast(X_test / 255.0, tf.float32)\n",
    "y_train, y_test = tf.cast(y_train, tf.int64), tf.cast(y_test, tf.int64)\n",
    "\n",
    "# Define model using tf.keras.layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64):\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1000):\n",
    "    logits = model(x_batch)\n",
    "    test_acc.update_state(y_batch, logits)\n",
    "print(f\"TensorFlow Accuracy: {test_acc.result():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercises\n",
    "### Fitting a planetary orbit\n",
    "Complete the following code to fit a planetary orbit.\n",
    "\n",
    "How many training epochs do you need to get at least a \"visually\" close trajectory? use two hidden layers with 64 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robust PyTorch example: learn orbital motion (positions) from initial state + time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Physics simulator (two-body central force, simple explicit integrator)\n",
    "G = 1.0\n",
    "M = 1.0\n",
    "\n",
    "def acceleration(x, y, eps=1e-8):\n",
    "    r2 = x**2 + y**2\n",
    "    r = np.sqrt(r2) + eps   # add eps to avoid div by zero\n",
    "    r3 = r2 * r\n",
    "    ax = -G * M * x / r3\n",
    "    ay = -G * M * y / r3\n",
    "    return ax, ay\n",
    "\n",
    "def simulate_orbit(x0, y0, vx0, vy0, dt=0.01, steps=400):\n",
    "    # simple symplectic-like Euler (semi-implicit) integrator for stability\n",
    "    x, y, vx, vy = x0, y0, vx0, vy0\n",
    "    traj = np.zeros((steps, 2), dtype=np.float32)\n",
    "    for i in range(steps):\n",
    "        ax, ay = acceleration(x, y)\n",
    "        vx = vx + ax * dt\n",
    "        vy = vy + ay * dt\n",
    "        x = x + vx * dt\n",
    "        y = y + vy * dt\n",
    "        traj[i, 0] = x\n",
    "        traj[i, 1] = y\n",
    "    return traj\n",
    "\n",
    "# --- Build dataset: many trajectories with varied initial conditions\n",
    "n_traj = 120         # number of different initial conditions\n",
    "steps = 400          # time steps per trajectory\n",
    "dt = 0.02\n",
    "total_samples = n_traj * steps\n",
    "\n",
    "Xs = np.zeros((total_samples, 5), dtype=np.float32)  # x0,y0,vx0,vy0,t\n",
    "Ys = np.zeros((total_samples, 2), dtype=np.float32)  # x_t, y_t\n",
    "\n",
    "ptr = 0\n",
    "for i in range(n_traj):\n",
    "    # sample random initial radius around 0.7..1.5\n",
    "    r = np.random.uniform(0.7, 1.5)\n",
    "    theta = np.random.uniform(0, 2*np.pi)\n",
    "    x0 = r * np.cos(theta)\n",
    "    y0 = r * np.sin(theta)\n",
    "    # circular velocity magnitude\n",
    "    v_circ = np.sqrt(G*M / r)\n",
    "    # scale velocity around circular (0.6..1.4)\n",
    "    v_factor = np.random.uniform(0.6, 1.4)\n",
    "    v = v_circ * v_factor\n",
    "    # perpendicular velocity to radius for near-orbit initial conditions\n",
    "    vx0 = -v * np.sin(theta)\n",
    "    vy0 =  v * np.cos(theta)\n",
    "    traj = simulate_orbit(x0, y0, vx0, vy0, dt=dt, steps=steps)\n",
    "    t = np.arange(1, steps+1) * dt  # time relative to start (avoid t=0 exactly if you prefer)\n",
    "    for j in range(steps):\n",
    "        Xs[ptr, :] = np.array([x0, y0, vx0, vy0, t[j]], dtype=np.float32)\n",
    "        Ys[ptr, :] = traj[j]\n",
    "        ptr += 1\n",
    "\n",
    "# quick sanity\n",
    "print(\"Dataset shape X, Y:\", Xs.shape, Ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Normalize inputs and outputs with epsilon guard for std\n",
    "eps = 1e-8\n",
    "X_mean = Xs.mean(axis=0)\n",
    "X_std  = Xs.std(axis=0)\n",
    "X_std[X_std < eps] = 1.0   # guard against zero std\n",
    "Y_mean = Ys.mean(axis=0)\n",
    "Y_std  = Ys.std(axis=0)\n",
    "Y_std[Y_std < eps] = 1.0\n",
    "\n",
    "Xn = (Xs - X_mean) / X_std\n",
    "Yn = (Ys - Y_mean) / Y_std\n",
    "\n",
    "# check for NaN/inf\n",
    "assert not np.isnan(Xn).any(), \"NaNs in normalized X\"\n",
    "assert not np.isnan(Yn).any(), \"NaNs in normalized Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbd96a4e74088bfe5500c304f034bc6d",
     "grade": false,
     "grade_id": "cell-595f117480e4c9e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Torch dataset and loaders\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Evaluate on a fresh test trajectory (unseen initial cond)\n",
    "theta = 0.3\n",
    "r = 1.1\n",
    "x0 = r * np.cos(theta); y0 = r * np.sin(theta)\n",
    "v_circ = np.sqrt(G*M / r)\n",
    "v = v_circ * 1.05\n",
    "vx0 = -v * np.sin(theta); vy0 = v * np.cos(theta)\n",
    "traj_true = simulate_orbit(x0, y0, vx0, vy0, dt=dt, steps=steps)\n",
    "\n",
    "t = np.arange(1, steps+1) * dt\n",
    "X_test = np.stack([np.full_like(t, x0), np.full_like(t, y0),\n",
    "                   np.full_like(t, vx0), np.full_like(t, vy0),\n",
    "                   t], axis=1).astype(np.float32)\n",
    "\n",
    "X_test_n = (X_test - X_mean) / X_std\n",
    "with torch.no_grad():\n",
    "    pred_n = model(torch.from_numpy(X_test_n).to(device).float()).cpu().numpy()\n",
    "pred = pred_n * Y_std + Y_mean\n",
    "\n",
    "# Plot true vs predicted trajectory\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(traj_true[:,0], traj_true[:,1], label='True', linewidth=2)\n",
    "plt.plot(pred[:,0], pred[:,1], '--', label='NN predicted', linewidth=2)\n",
    "plt.scatter([x0], [y0], color='k', marker='x', label='start')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title('Orbit: true vs NN prediction')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Quantitative error\n",
    "mse = np.mean((pred - traj_true)**2)\n",
    "print(\"Test MSE (position):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Galaxy Morphology Classification\n",
    "Astronomers classify galaxies (spiral, elliptical, irregular) based on their shape. This classification provides clues about a galaxy's formation, age, and evolutionary history.\n",
    "\n",
    "The Task: Build a Convolutional Neural Network (CNN) to classify images of galaxies into different morphological types.\n",
    "\n",
    "Dataset: A great starting point is the Galaxy Zoo 2 dataset, which has hundreds of thousands of galaxy images classified by citizen scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Predicting Protein Subcellular Localization\n",
    "Knowing where a protein resides within a cell (e.g., nucleus, cytoplasm, mitochondria) is crucial for understanding its function. This is called subcellular localization.\n",
    "\n",
    "The Task: Predict a protein's location based on its amino acid sequence. This is a sequence classification problem.\n",
    "\n",
    "Dataset: You can find datasets on platforms like the UCI Machine Learning Repository or by searching for \"protein subcellular localization dataset.\" The data consists of protein sequences (strings of letters like 'ARND...') and their corresponding location labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Predicting Quantum Mechanical Properties of Molecules \n",
    "Quantum chemistry calculations can predict molecular properties (like electronic energy) but are computationally very expensive. Machine learning can approximate these calculations much faster.\n",
    "\n",
    "The Task: Build a model to predict a molecule's properties based on its 3D structure. Molecules are naturally represented as graphs, where atoms are nodes and bonds are edges.\n",
    "\n",
    "Dataset: The QM9 dataset is a standard benchmark. It contains about 134,000 small organic molecules with 13 different quantum properties calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
