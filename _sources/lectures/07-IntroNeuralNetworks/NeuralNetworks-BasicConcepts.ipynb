{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Neural Networks Introduction\n",
    "An artificial neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. \n",
    "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. Neural networks can adapt to changing input; so the network generates the best possible \n",
    "result without needing to redesign the output criteria. In general, neural networks are useful to approximate non-linear, multi-dimensional functions. Or basically functions where we don't have any info about their functional form just inputs and outputs.\n",
    "Early models resembled the biological neural networks (see the preceptron)\n",
    "\n",
    "[Neural networks](https://en.wikipedia.org/wiki/Neural_network?useskin=vector) are usually arranged into layers, where information passing from layer to the next one firm the first one (input layer), to maybe some intermediate layers (the hidden layers), \n",
    "to the last one (the output layer). For each neuron, the input is a linear combination of the outputs of the previous layer connecting to it, and the neuron output is gien by the so-called \n",
    "activation function. The strenghst or weights of the network connections change dynamically so to approach optimally (given a minimization procedure) the data used during training usually following an algorithm like [back-propagation](https://en.wikipedia.org/wiki/Backpropagation?useskin=vector).  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\" alt=\"Image Description\" width=\"600\">\n",
    "    <figcaption>From: \"https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\"</figcaption>\n",
    "</div>\n",
    "\n",
    "![Neural network with several layeres (Credit http://alexlenail.me/NN-SVG/index.html)](fig/nn.svg)\n",
    "\n",
    "The input for each network is computed as \n",
    "$$\n",
    "z = b + \\sum_i w_i X_i,\n",
    "$$\n",
    "where $b$ is the bias, $w_i$ are the weights, and $X_i$ are the outputs of the neurons in the previous layer and connected t =o this one. Then , the output for this neuron is computed aas\n",
    "$$\n",
    "X = f(z),\n",
    "$$ \n",
    "where $f$ is some [activation function](https://en.wikipedia.org/wiki/Activation_function?useskin=vector). Examples for [activations funtions](https://en.wikipedia.org/wiki/Activation_function?useskin=vector#Folding_activation_functions) are\n",
    "- ReLu\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- Linear\n",
    "- ...\n",
    "\n",
    "The evolution of the training process is measured in *epochs* (an iteration, basically). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "These are some recommended tools to get familiar with neural networks:\n",
    "- A visual introduction: https://www.youtube.com/watch?v=UOvPeC8WOt8\n",
    "- But what is a neural network? | Chapter 1, Deep learning: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "- Neural netoworks full playlist: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "- Visualization of a fully connected neural network, version 1: https://www.youtube.com/watch?v=Tsvxx-GGlTg\n",
    "- Watching Neural Networks Learn: https://www.youtube.com/watch?v=TkwXa7Cvfr8\n",
    "- Neural network Visualization: http://alexlenail.me/NN-SVG/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tensorflow playground\n",
    "Now let'sÂ play a bit with a neural network: https://playground.tensorflow.org\n",
    "\n",
    "![Neural network with several layeres (Credit http://alexlenail.me/NN-SVG/index.html)](fig/tensorflow-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Simple neural network: A perceptron\n",
    "Based on https://www.youtube.com/watch?v=kft1AJ9WVDk\n",
    "\n",
    "**TODO**\n",
    "- Add bias\n",
    "- Generalize to more hidden neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is what we want to train our neural network with:\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/inputs.png)\n",
    "\n",
    "And we want to predict the new output (try to guess the rule)\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/newoutput.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural network that we are going to use (you can also use http://alexlenail.me/NN-SVG/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nnv import NNV\n",
    "\n",
    "layersList = [\n",
    "    {\"title\":\"input\", \"units\": 3, \"color\": \"darkBlue\"},\n",
    "    {\"title\":\"hidden 1\\n(sigmoid)\", \"units\": 1, \"edges_color\":\"red\", \"edges_width\":2},\n",
    "    {\"title\":\"output\\n(sigmoid)\", \"units\": 1,\"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand better the training, let's show explicitly the weights\n",
    "![weightds](fig/neuralnetwork/weights.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\phi$ is called the activation function, and there are several proposals to it. We will use a sigmoid function\n",
    "$$\n",
    "f(x) = \\dfrac{1}{1+\\exp(-x)},\n",
    "$$\n",
    "where $x = \\sum x_i w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_context('poster')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "xdata = np.linspace(-6.0, 6.0, 100)\n",
    "plt.plot(xdata, sigmoid(xdata))\n",
    "# Highlight x=0 and y=0 axes\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=2.5)  # Horizontal line at y=0\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=2.5)  # Vertical line at x=0\n",
    "# Add labels to the x-axis and y-axis\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(rf\"sigmoid(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Basic Implementation\n",
    "For this very basic nn, we will:\n",
    "- set the input or start of the algorithm:\n",
    "    + Random weights $w_i$\n",
    "    + Set the training inputs and outputs\n",
    "- Create an iteration function to perform the training for `nsteps` (initially 1)\n",
    "\n",
    "The we just iterate once and check what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def get_training_inputs():\n",
    "    return np.array([[0, 0, 1],\n",
    "                     [1, 1, 1], \n",
    "                     [1, 0, 1],\n",
    "                     [0, 1, 1]])\n",
    "\n",
    "def get_training_outputs():\n",
    "    return np.array([0, 1, 1, 0]).reshape(4, 1)\n",
    "\n",
    "def get_init_weights():\n",
    "    \"\"\"\n",
    "    Initially, simply return random weights in [-1, 1)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(-1.0, 1.0, size=(3, 1))\n",
    "\n",
    "def training_one_step(training_inputs, training_outputs, initial_weights):\n",
    "    # iter only once\n",
    "    input_layer = training_inputs\n",
    "    outputs = sigmoid(np.dot(input_layer, initial_weights))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()\n",
    "print(inputs_t)\n",
    "print(outputs_t)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = training_one_step(inputs_t, outputs_t, weights)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after one step training:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Improving the training\n",
    "These results are not optimal, and depend a lot on the initial weights. Also, we are not yet comparing with the expecting output for the training data. We are now going to include it and add correction terms to the weights, so we will be using back-propagation. Our algorithm is now:\n",
    "- Take each input from the training data.\n",
    "- Compute the error, i.e. the difference between the output and the expected one, `output - expectedoutput`. \n",
    "- According to the error, adjust the weights\n",
    "- Repeat this many times, hopefully getting convergence , and also being able to apply our nn to new cases not used already.\n",
    "\n",
    "But how to adjust the weights? There are several techniques based on the actual error $\\Delta$. Here we will use error weighted derivative. Given the form of the sigmoid function, this increases the adjust if the derivative is larger, and viceversa. It can be expressed as \n",
    "\n",
    "$$\n",
    "\\Delta w = \\Delta \\times \\text{input} \\times \\phi'(output),\n",
    "$$\n",
    "where $\\phi'$ is the derivative of the activation function. In our one-dimensional case we can compute it easily, but with more complex problems it becomes a gradient and its efficient computation is very important (remember automatic differentiation?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def train_nn(training_inputs, training_outputs, initial_weights, niter, errors_data):\n",
    "    \"\"\"\n",
    "    training_inputs: asdasdasda\n",
    "    ...\n",
    "    errors_data: output - stores the errors per iteration\n",
    "    \"\"\"\n",
    "    w = initial_weights\n",
    "    for ii in range(niter):\n",
    "        # Forward propagation\n",
    "        input_layer = training_inputs\n",
    "        outputs = sigmoid(np.dot(input_layer, w))\n",
    "        # Backward propagation\n",
    "        errors = training_outputs - outputs\n",
    "        deltaw = errors*sigmoid_prime(outputs)\n",
    "        deltaw = np.dot(input_layer.T, deltaw)\n",
    "        w += deltaw\n",
    "        # Save errors for plotting later\n",
    "        errors_data[ii] = errors.reshape((4,))\n",
    "    return outputs, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITER = 50000\n",
    "errors = np.zeros((NITER, 4))\n",
    "outputs, weights = train_nn(inputs_t, outputs_t, weights, NITER, errors)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after training:\")\n",
    "print(outputs)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax[0].plot(range(NITER), errors)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Errors\")\n",
    "ax[1].loglog(range(NITER), np.abs(errors))\n",
    "ax[1].set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our network is very well trained, But how does it perform with a new input? let's check with `[1, 0, 0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weights)\n",
    "#print(weights.shape)\n",
    "input_new = np.array([1, 0, 0]).reshape(3, 1)\n",
    "#print(input_new)\n",
    "#print(input_new.shape)\n",
    "#print(np.sum(weights*input_new))\n",
    "print(sigmoid(np.sum(weights*input_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is basically one, as expected.\n",
    "There are more topics related to this that we have not used, like more layers, more neurons per hidden layer, bias on the activation function, and a lot of other details, but hopefully you now see how a neural network works on the core.\n",
    "\n",
    "Recommended lectures:\n",
    "- 3blue1brown Neural Networks: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "- Neural networks from scratch: https://www.youtube.com/watch?v=9RN2Wr8xvro\n",
    "- Backprop basic: https://www.youtube.com/watch?v=wqPt3qjB6uA\n",
    "- https://www.youtube.com/watch?v=khUVIZ3MON8&t=0s\n",
    "\n",
    "TODO:\n",
    "- Plot the weights as a function of the epoch.\n",
    "- Remove one data from training and check if the prediction is ok. Remove more.\n",
    "- Add a second layer and compare the convergence\n",
    "- Add an example using pythorch/tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
