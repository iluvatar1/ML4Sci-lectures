{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Backpropagation in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this interactive guide on **Backpropagation**! You already have a foundational understanding of neural networks, linear and logistic regression, and SVMs. This notebook will build upon that knowledge to provide a deep and intuitive understanding of the backpropagation algorithm, which is the cornerstone of training most neural networks. [1]\n",
    "\n",
    "**What is Backpropagation?**\n",
    "\n",
    "Backpropagation, short for \"backward propagation of errors,\" is an algorithm used to train artificial neural networks. [1] It is a supervised learning algorithm that works by calculating the gradient of the loss function with respect to the network's weights and biases. This gradient is then used to update the weights and biases in the direction that minimizes the loss. [1, 2]\n",
    "\n",
    "Think of it as a methodical way to assign blame for the network's overall error to each of its individual connections (weights). Once we know which weights are most responsible for the error, we can adjust them accordingly. This process is repeated iteratively until the network's performance on the training data is satisfactory.\n",
    "\n",
    "Some resources:\n",
    "- Backprop visualized: <https://xnought.github.io/backprop-explainer/>\n",
    "- 3Blue1brown: <https://www.youtube.com/watch?v=Ilg3gGewQ5U>\n",
    "- Foundations of computer vision: <https://visionbook.mit.edu/backpropagation.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Idea: Gradient Descent\n",
    "\n",
    "Before diving into backpropagation, let's quickly recap **Gradient Descent**. Imagine you are on a mountain in a thick fog and you want to get to the lowest point. What would you do? You would look at the ground beneath your feet and take a step in the steepest downhill direction. You would repeat this process until you could no longer step down.\n",
    "\n",
    "This is exactly what gradient descent does. In the context of machine learning, the \"mountain\" is the **loss function**, and the \"position\" is the set of weights and biases of our model. The goal is to find the weights and biases that result in the lowest possible loss (error).\n",
    "\n",
    "The **gradient** of the loss function is a vector that points in the direction of the steepest ascent. Therefore, to move downhill, we take a step in the **negative** direction of the gradient. The size of this step is determined by the **learning rate** (α).\n",
    "\n",
    "The update rule for a weight *w* is:\n",
    "$$ w_{new} = w_{old} - \\alpha \\frac{\\partial L}{\\partial w} $$\n",
    "\n",
    "where *L* is the loss function. Backpropagation is the algorithm that allows us to efficiently compute this gradient,  $\\frac{\\partial L}{\\partial w}$ , for all the weights and biases in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematics of Backpropagation\n",
    "\n",
    "Let's consider a simple neural network with one hidden layer to understand the mathematics. Our goal is to derive the gradients of the loss function with respect to the weights and biases.\n",
    "\n",
    "### The Forward Pass\n",
    "\n",
    "First, the input data is fed forward through the network to compute the output. For a single training example:\n",
    "\n",
    "1.  **Input Layer to Hidden Layer:**\n",
    "    $$ z^{[1]} = W^{[1]}x + b^{[1]} $$\n",
    "    $$ a^{[1]} = g(z^{[1]}) $$\n",
    "\n",
    "2.  **Hidden Layer to Output Layer:**\n",
    "    $$ z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} $$\n",
    "    $$ \\hat{y} = a^{[2]} = \\sigma(z^{[2]}) $$\n",
    "\n",
    "Where:\n",
    "-  *x* is the input vector.\n",
    "-  *W<sup>[1]</sup>* and *b<sup>[1]</sup>* are the weight matrix and bias vector for the hidden layer.\n",
    "-  *g* is the activation function for the hidden layer (e.g., ReLU or tanh).\n",
    "-  *W<sup>[2]</sup>* and *b<sup>[2]</sup>* are the weight matrix and bias vector for the output layer.\n",
    "-  *σ* is the activation function for the output layer (e.g., sigmoid for binary classification).\n",
    "-  *ŷ* is the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    "\n",
    "We need a way to measure how wrong our network's prediction is. For binary classification, we often use the **Binary Cross-Entropy Loss**:\n",
    "\n",
    "$$ L(y, \\hat{y}) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Backward Pass and the Chain Rule\n",
    "\n",
    "This is where the magic happens. We need to compute the derivatives of the loss with respect to all weights and biases. The key to this is the **Chain Rule** from calculus.\n",
    "\n",
    "**Chain Rule:** If *z* depends on *y*, and *y* depends on *x*, then the derivative of *z* with respect to *x* is:\n",
    "$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} $$\n",
    "\n",
    "We start from the end of the network and work our way backward.\n",
    "\n",
    "**Step 1: Output Layer Gradients**\n",
    "\n",
    "We first compute the derivative of the loss with respect to the output layer's weighted sum, *z<sup>[2]</sup>*:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} = (a^{[2]} - y) $$\n",
    "\n",
    "(This is a simplified result for the combination of sigmoid and binary cross-entropy).\n",
    "\n",
    "Now we can compute the gradients for *W<sup>[2]</sup>* and *b<sup>[2]</sup>*:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial W^{[2]}} = (a^{[2]} - y) \\cdot (a^{[1]})^T $$\n",
    "$$ \\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial b^{[2]}} = (a^{[2]} - y) $$\n",
    "\n",
    "**Step 2: Hidden Layer Gradients**\n",
    "\n",
    "Next, we propagate the error backward to the hidden layer:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} = (W^{[2]})^T (a^{[2]} - y) \\ast g'(z^{[1]}) $$\n",
    "\n",
    "where *g'* is the derivative of the hidden layer's activation function and *∗* denotes element-wise multiplication.\n",
    "\n",
    "Finally, we can compute the gradients for *W<sup>[1]</sup>* and *b<sup>[1]</sup>*:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} \\cdot x^T $$\n",
    "$$ \\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} $$\n",
    "\n",
    "Once we have these gradients, we can use them to update the weights and biases using the gradient descent rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Exercise 1:\n",
    "\n",
    "If the hidden layer activation function *g(z)* is the Rectified Linear Unit (ReLU), where *g(z) = max(0, z)*, what is its derivative *g'(z)*? How does this simplify the calculation for the hidden layer gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Backpropagation from Scratch\n",
    "\n",
    "Now, let's implement a simple neural network in Python using only NumPy to see backpropagation in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Gradients for the output layer\n",
    "        dZ2 = self.A2 - Y\n",
    "        self.dW2 = (1 / m) * np.dot(dZ2, self.A1.T)\n",
    "        self.db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Gradients for the hidden layer\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * sigmoid_derivative(self.Z1)\n",
    "        self.dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        self.db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            y_pred = self.forward_pass(X)\n",
    "            loss = compute_loss(Y, y_pred)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            self.backward_pass(X, Y)\n",
    "            self.update_parameters(learning_rate)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Training Process\n",
    "\n",
    "Let's use the `make_moons` dataset from scikit-learn, which is a simple dataset for binary classification that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and prepare data\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Transpose data for our network's expected input shape\n",
    "X_train_t = X_train.T\n",
    "y_train_t = y_train.reshape(1, -1)\n",
    "\n",
    "# Initialize and train the network\n",
    "net = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "losses = net.train(X_train_t, y_train_t, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Plot the loss over time\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Boundary\n",
    "\n",
    "A great way to see what the network has learned is to visualize its decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    Z = model.forward_pass(np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z > 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Exercise 2:\n",
    "\n",
    "In the `SimpleNeuralNetwork` class, change the hidden layer size from 4 to other values (e.g., 1, 2, 20, 50). How does this affect the decision boundary and the final loss? What do you observe about potential overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Considerations\n",
    "**Vanishing Gradient Problem**\n",
    "\n",
    "In deep networks, gradients can become exponentially small:\n",
    "Each term ∂aᵢ/∂aᵢ₋₁ involves the derivative of the activation function. For sigmoid: σ'(x) ≤ 0.25, so the product shrinks exponentially.\n",
    "\n",
    "Solutions:\n",
    "- Use ReLU activations (gradient is 1 or 0)\n",
    "- Apply gradient clipping\n",
    "- Use residual connections\n",
    "- Batch normalization\n",
    "\n",
    "**Exploding Gradient Problem**\n",
    "\n",
    "Conversely, gradients can grow exponentially large.\n",
    "\n",
    "Solutions:\n",
    "- Gradient clipping: if ||g|| > threshold: g = g × threshold/||g||\n",
    "- Careful weight initialization\n",
    "- Learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in Other ML Models\n",
    "\n",
    "The core idea of backpropagation—calculating a gradient to minimize a loss—is not unique to complex neural networks.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "In linear regression, we want to find the line of best fit by minimizing the Mean Squared Error (MSE) loss function:\n",
    "$$ L(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - (wx_i + b))^2 $$\n",
    "\n",
    "The gradients are:\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} -2x_i(y_i - (wx_i + b)) $$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} -2(y_i - (wx_i + b)) $$\n",
    "\n",
    "This is a very simple form of backpropagation, where the \"network\" is just a single node with a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic regression can be viewed as a single-neuron neural network with a sigmoid activation function. The loss function is the Binary Cross-Entropy loss, just like in our example network. The process of using gradient descent to find the optimal weights for logistic regression is exactly backpropagation on a network with no hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVMs)\n",
    "\n",
    "While standard SVMs are often solved using different optimization techniques (like quadratic programming), they can also be trained using gradient descent, especially for linear SVMs. The loss function for an SVM is the **Hinge Loss**:\n",
    "\n",
    "$$ L = \\max(0, 1 - y(w^T x - b)) $$\n",
    "\n",
    "By calculating the gradient of this loss function with respect to the weights *w*, one can use gradient descent to find the optimal hyperplane. This approach is particularly useful for large datasets where traditional methods are too slow.\n",
    "\n",
    "### Final comparison\n",
    " Method | How It Uses Gradients | Backprop Connection |\n",
    "|-------|------------------------|---------------------|\n",
    "| Linear Regression | ∂L/∂W = -2X(y - y_pred) | Same idea: gradient of loss w.r.t. weights |\n",
    "| Logistic Regression | ∂L/∂W = X^T(σ(WX+b) - y) | Uses chain rule on sigmoid |\n",
    "| SVM | Uses hinge loss + gradient | Gradient flow is similar, but loss is different |\n",
    "| **Neural Networks** | Chain rule through many layers | **Backprop is the generalization** of all three |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application in Basic Sciences: Classifying Phases of Matter\n",
    "\n",
    "Neural networks are powerful tools in the basic sciences. For example, they can be used to identify the phases of matter (e.g., ordered vs. disordered) from raw simulation data, a task that often requires deep physical insight.\n",
    "\n",
    "Let's simulate a simple dataset for a 2D Ising model, a model of magnetism. We'll have two phases: an ordered (ferromagnetic) phase at low temperatures and a disordered (paramagnetic) phase at high temperatures. The input to our network will be the spin configurations (grids of +1s and -1s).\n",
    "\n",
    "*This is a simplified example for demonstration purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ising_data(L, n_samples):\n",
    "    # Simple function to generate fake Ising model configurations\n",
    "    # L: grid size (L x L)\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_samples // 2):\n",
    "        # Ordered phase (low temp): mostly aligned spins\n",
    "        config = np.random.choice([-1, 1], size=(L, L), p=[0.1, 0.9])\n",
    "        X.append(config.flatten())\n",
    "        y.append(0) # Label for ordered\n",
    "        \n",
    "    for _ in range(n_samples // 2):\n",
    "        # Disordered phase (high temp): random spins\n",
    "        config = np.random.choice([-1, 1], size=(L, L), p=[0.5, 0.5])\n",
    "        X.append(config.flatten())\n",
    "        y.append(1) # Label for disordered\n",
    "        \n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "L = 4\n",
    "X_ising, y_ising = generate_ising_data(L, 1000)\n",
    "\n",
    "# Shuffle the data\n",
    "permutation = np.random.permutation(X_ising.shape[0])\n",
    "X_ising = X_ising[permutation, :]\n",
    "y_ising = y_ising[permutation, :]\n",
    "\n",
    "# Split and transpose\n",
    "X_train_ising, y_train_ising = X_ising.T[:, :800], y_ising.T[:, :800]\n",
    "X_test_ising, y_test_ising = X_ising.T[:, 800:], y_ising.T[:, 800:]\n",
    "\n",
    "print(f\"Input data shape: {X_train_ising.shape}\")\n",
    "print(f\"Labels shape: {y_train_ising.shape}\")\n",
    "\n",
    "# Train a network to classify the phases\n",
    "ising_net = SimpleNeuralNetwork(input_size=L*L, hidden_size=8, output_size=1)\n",
    "ising_losses = ising_net.train(X_train_ising, y_train_ising, epochs=5000, learning_rate=0.05)\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure()\n",
    "plt.plot(ising_losses)\n",
    "plt.title(\"Ising Model Phase Classification Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "y_pred_ising = ising_net.forward_pass(X_test_ising)\n",
    "predictions = (y_pred_ising > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test_ising) * 100\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercises\n",
    "\n",
    "Here are some exercises to solidify your understanding of backpropagation.\n",
    "\n",
    "**Exercise 1:**\n",
    "Modify the `SimpleNeuralNetwork` class to use the **tanh** activation function in the hidden layer instead of the sigmoid function. You will need to find the derivative of tanh and use it in the `backward_pass` method. The tanh function and its derivative are:\n",
    "$$ \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} $$\n",
    "$$ \\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z) $$\n",
    "Retrain the model on the `make_moons` dataset and visualize the new decision boundary. Does it perform better or worse?\n",
    "\n",
    "**Exercise 2:**\n",
    "Implement **L2 Regularization** to the training process. This involves adding a penalty term to the loss function that is proportional to the square of the weights. The new loss function will be:\n",
    "$$ L_{reg} = L + \\frac{\\lambda}{2m} (||W^{[1]}||^2_F + ||W^{[2]}||^2_F) $$\n",
    "This will add a term to the gradient updates for the weights:\n",
    "$$ \\frac{\\partial L_{reg}}{\\partial W} = \\frac{\\partial L}{\\partial W} + \\frac{\\lambda}{m} W $$\n",
    "Add a `reg_lambda` parameter to the `train` method and modify the `backward_pass` or `update_parameters` method to include this regularization term. Observe its effect on the decision boundary for a model with a large hidden layer (e.g., 50 units).\n",
    "\n",
    "**Exercise 3:**\n",
    "Our current implementation uses batch gradient descent (it processes all training examples at once). Modify the training loop to implement **Stochastic Gradient Descent (SGD)**, where the forward and backward passes are performed for one training example at a time. How does the loss curve change compared to batch gradient descent? (It should be much noisier).\n",
    "\n",
    "**Exercise 4:**\n",
    "Create a neural network to solve a simple regression problem. For example, try to fit the function *y = sin(x)*. You will need to:\n",
    "1. Generate training data (e.g., `X = np.linspace(-np.pi, np.pi, 100)`, `y = np.sin(X)`).\n",
    "2. Change the output layer's activation to be linear (i.e., no activation function).\n",
    "3. Change the loss function to Mean Squared Error (MSE).\n",
    "4. Modify the backpropagation equations to account for the new loss and activation.\n",
    "Plot the network's predictions against the true `sin(x)` curve.\n",
    "\n",
    "**Exercise 5 (Conceptual):**\n",
    "Explain in your own words how backpropagation would work in a neural network with **two hidden layers**. Write down the equations for the gradient of the loss with respect to the weights of the *first* hidden layer (*W<sup>[1]</sup>*). How does the error signal from the final layer propagate back to this first layer?\n",
    "\n",
    "**Exercise 1: Complete Backpropagation Calculation**\n",
    "Given a 2-layer neural network:\n",
    "\n",
    "Input: x = [1, 2]\n",
    "W₁ = [[0.5, 0.3], [0.2, 0.8]], b₁ = [0.1, 0.4]\n",
    "W₂ = [[0.9, 0.7]], b₂ = [0.2]\n",
    "Activation: sigmoid for all layers\n",
    "Loss: MSE\n",
    "Target: y = 0.8\n",
    "\n",
    "Calculate:\n",
    "a) Forward pass (all z and a values)\n",
    "b) Loss value\n",
    "c) All gradients (∂J/∂W₁, ∂J/∂b₁, ∂J/∂W₂, ∂J/∂b₂)\n",
    "d) Updated weights after one gradient descent step (α = 0.1)\n",
    "\n",
    "**Exercise 2: Vanishing Gradient Analysis**\n",
    "\n",
    "Consider a 5-layer network with sigmoid activations. If the initial gradient at the output layer has magnitude 1.0:\n",
    "a) Estimate the gradient magnitude at each layer working backwards\n",
    "b) What happens to the gradient magnitude at the input layer?\n",
    "c) Suggest three techniques to mitigate this problem\n",
    "d) Recalculate assuming ReLU activations (assume 50% of neurons are active)\n",
    "\n",
    "**Exercise 3: Scientific Application Design**\n",
    "Design a neural network to predict chemical reaction rates from molecular descriptors:\n",
    "a) Define input features (at least 5 molecular properties)\n",
    "b) Design the network architecture (number of layers, neurons, activations)\n",
    "c) Choose an appropriate loss function and justify your choice\n",
    "d) Describe how backpropagation would optimize this network\n",
    "e) Identify potential challenges and solutions\n",
    "\n",
    "\n",
    "**Exercise 4: Custom Activation Function**\n",
    "Create a new activation function: f(x) = x × tanh(x)\n",
    "a) Derive its derivative f'(x)\n",
    "b) Implement the backpropagation equations for a layer using this activation\n",
    "c) Analyze its properties: range, monotonicity, gradient behavior\n",
    "d) Compare advantages/disadvantages vs ReLU and sigmoid\n",
    "\n",
    "**Exercise 5: Multi-Output Regression**\n",
    "Design backpropagation for a network predicting multiple outputs:\n",
    "\n",
    "Input: Environmental measurements `[temperature, humidity, pressure, wind_speed]`\n",
    "Outputs: `[pm2.5_concentration, ozone_level, no2_level]`\n",
    "\n",
    "a) Write the forward pass equations\n",
    "b) Define an appropriate loss function for multiple outputs\n",
    "c) Derive the backpropagation equations\n",
    "d) Discuss how to handle outputs with different scales/importance\n",
    "e) Implement gradient updates for the multi-output case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Backpropagation is the fundamental algorithm enabling neural network training. Key takeaways:\n",
    "\n",
    "- Chain Rule Application: Systematically applies calculus chain rule to compute gradients\n",
    "- Efficiency: Computes all gradients in O(n) time through clever reuse of computations\n",
    "- Universal Algorithm: Works for any differentiable network architecture\n",
    "- Scientific Applications: Enables neural networks to learn complex patterns in physics, chemistry, biology\n",
    "- Optimization Foundation: Provides gradients for sophisticated optimization algorithms\n",
    "\n",
    "Understanding backpropagation deeply allows you to:\n",
    "\n",
    "- Debug training problems\n",
    "- Design custom architectures\n",
    "- Apply neural networks to novel scientific problems\n",
    "- Optimize training procedures\n",
    "\n",
    "The algorithm's elegance lies in its simplicity: compute errors backward, update weights to reduce errors. Yet this simple principle powers the most sophisticated AI systems solving complex scientific and engineering problems.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- \"Deep Learning\" by Goodfellow, Bengio, and Courville (Chapter 6)\n",
    "\"- Pattern Recognition and Machine Learning\" by Bishop (Chapter 5)\n",
    "- Original backpropagation papers by Rumelhart, Hinton, and Williams (1986)\n",
    "- Modern optimization techniques: Adam, RMSprop, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
