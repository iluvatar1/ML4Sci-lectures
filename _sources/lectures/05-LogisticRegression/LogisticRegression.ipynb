{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# An Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A Brief History of Logistic Regression\n",
    "\n",
    "Logistic Regression is often one of the first classification algorithms that people learn in machine learning, but its story begins over a century earlier in the field of statistics and demography. It wasn't invented by a computer scientist for a machine learning task, but rather evolved over time to solve problems related to understanding relationships in data.\n",
    "\n",
    "Here is a brief timeline of its key developments:\n",
    "\n",
    "*   **Early 19th Century (1830s-1840s): The Logistic Function**\n",
    "    *   The mathematical foundation, the **logistic function** (also known as the sigmoid function), was first described by the Belgian mathematician **Pierre François Verhulst**.\n",
    "    *   He used it not for classification, but to model population growth. The S-shaped curve was a perfect fit for describing how a population grows rapidly at first, then slows down as it approaches a carrying capacity or resource limit. This work laid the mathematical groundwork for what would come later.\n",
    "\n",
    "*   **Mid-20th Century (1944): The \"Logit\" is Born**\n",
    "    *   The term **\"logit\"** was coined by the statistician **Joseph Berkson**. The logit function is the core of logistic regression; it's the natural logarithm of the odds $$ \\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) $$\n",
    "    *   Berkson proposed that this transformation could be used to linearize the relationship between predictor variables and a binary (yes/no) outcome, making it suitable for analysis with methods similar to linear regression. His work was primarily in the field of biostatistics.\n",
    "\n",
    "*   **Mid-20th Century (1958): Formalization as a Regression Model**\n",
    "    *   The British statistician **David Cox** is widely credited with popularizing and formalizing logistic regression as we know it today.\n",
    "    *   His influential 1958 paper, \"The regression analysis of binary data,\" detailed how to use the logit model to analyze the relationship between a set of explanatory variables and a binary dependent variable. This cemented its place as a fundamental tool in statistical analysis, especially in epidemiology and medical research.\n",
    "\n",
    "*   **Late 20th Century (1970s-1990s): Adoption into Machine Learning**\n",
    "    *   As the field of machine learning grew out of computer science and statistics, logistic regression was naturally adopted as a simple, efficient, and highly interpretable classification algorithm.\n",
    "    *   Its inclusion within the framework of **Generalized Linear Models (GLMs)** in the 1970s provided a strong theoretical foundation.\n",
    "    *   It became (and remains) a crucial **baseline model**—a simple model to which more complex models (like SVMs, Random Forests, or Neural Networks) are compared.\n",
    "\n",
    "*   **Present Day: A Foundational Concept**\n",
    "    *   Today, logistic regression is still one of the most widely used algorithms in both statistics and machine learning. Its importance also extends to being a building block for more complex models. A single neuron in a neural network using a sigmoid activation function is, in essence, performing logistic regression. This makes understanding it fundamental to understanding deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Motivation: Why Not Linear Regression for Classification?\n",
    "\n",
    "Linear regression is excellent for predicting continuous values, like the price of a house or the temperature tomorrow. But what if we want to predict a categorical outcome? For example:\n",
    "\n",
    "- Will a patient test positive or negative for a disease?\n",
    "- Is an email spam or not spam?\n",
    "- Is a tumor malignant or benign?\n",
    "\n",
    "These are **classification problems** with binary outcomes (Yes/No, 1/0, True/False).\n",
    "\n",
    "Let's see what happens if we try to fit a linear regression model to a binary outcome. Imagine we have data on tumor size and whether it's malignant (1) or benign (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "tumor_size = np.random.normal(10, 5, 100)\n",
    "is_malignant = (tumor_size + np.random.normal(0, 3, 100) > 12).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression line\n",
    "m, b = np.polyfit(tumor_size, is_malignant, 1)\n",
    "\n",
    "# Plot the data and the line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(tumor_size, is_malignant, label='Data (0=Benign, 1=Malignant)', alpha=0.7)\n",
    "plt.plot(tumor_size, m*tumor_size + b, color='red', label='Linear Regression Fit')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=1, color='gray', linestyle='--')\n",
    "plt.xlabel('Tumor Size')\n",
    "plt.ylabel('Malignant (1) or Benign (0)')\n",
    "plt.title('Why Linear Regression Fails for Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As you can see, the linear regression line extends beyond 0 and 1. How would we interpret a prediction of 1.5? Or -0.5? It doesn't make sense as a probability. \n",
    "\n",
    "We need a model that outputs a value between 0 and 1, which can be interpreted as the **probability** of the outcome being 'Yes' (or 1).\n",
    "\n",
    "This is where **Logistic Regression** comes in. It uses a special S-shaped function, the **Sigmoid function**, to squash the output of a linear equation into the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Core Component: The Sigmoid Function\n",
    "\n",
    "The sigmoid function is a mathematical function that takes any real number and maps it to a value between 0 and 1. \n",
    "\n",
    "The formula is:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Where `z` is the output of our linear equation (e.g., `z = mx + b`).\n",
    "\n",
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "# prepare some data\n",
    "# create a new plot with a title and axis labels\n",
    "p = figure(title=\"The sigmoid function\", x_axis_label='z', y_axis_label=rf'$$\\sigma(z)$$')\n",
    "# add a line renderer with legend and line thickness to the plot\n",
    "p.line(z, sigma_z, line_width=2)\n",
    "p.line(z, 0.5, legend_label=\"Threshold at 0.5\", line_width=2, color='red')\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Key Observations:**\n",
    "- When `z` is large and positive, $e^{-z}$ approaches 0, so `σ(z)` approaches 1.\n",
    "- When `z` is large and negative, $e^{-z}$ becomes very large, so `σ(z)` approaches 0.\n",
    "- When `z = 0`, $e^0 = 1$, so $\\sigma(z) = 1 / (1 + 1) = 0.5$.\n",
    "\n",
    "This is perfect for modeling probability! The output of the sigmoid function, `σ(z)`, can be interpreted as the probability of the positive class (e.g., the probability that a tumor is malignant).\n",
    "\n",
    "$$ P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n)}} $$\n",
    "\n",
    "We typically set a **decision boundary** or **threshold** at 0.5. \n",
    "- If `P(Y=1) > 0.5`, we classify the outcome as 1 (Malignant).\n",
    "- If `P(Y=1) <= 0.5`, we classify the outcome as 0 (Benign)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate the 2D heat equation in LaTeX surrounded by `$$`. Do not include an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## What is the cost function?\n",
    "In logistic regression, we are comparing probabilities. Therefore, it is more important to compare ratios than differences. Furthermore, the cost function from MSE is now not a simple paraboloid, so comparing \"distances\" is not correct. Given that outcomes are not 0 or 1 (or binary), they follow a Bernoulli distribution, and, therefore, we are fitting a probability distribution, finding the likelihood of the parameters.\n",
    "\n",
    "### The **Likelihood Function**\n",
    "\n",
    "Suppose you have data and a statistical model with parameters. The **likelihood function** tells you how *likely* it is that your observed data came from the model, given specific parameter values.\n",
    "\n",
    "* If your model is ($P(y \\mid \\theta)$), the probability of observing outcome ($y$) given parameters ($\\theta$),\n",
    "* And you have a dataset ($y_1, y_2, \\dots, y_n$),\n",
    "* Then the **likelihood function** is\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n P(y_i \\mid \\theta).\n",
    "$$\n",
    "\n",
    "This product appears because we assume the data points are independent.\n",
    "\n",
    "Instead of maximizing this product directly (which gets very small), we usually maximize its **logarithm**:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log P(y_i \\mid \\theta).\n",
    "$$\n",
    "\n",
    "This is called the **log-likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Application to **Logistic Regression**\n",
    "\n",
    "In logistic regression, we model the probability that ($y_i = 1$) given predictors ($x_i$) as:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid x_i, \\beta) = \\sigma(x_i^\\top \\beta) = \\frac{1}{1 + e^{-x_i^\\top \\beta}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(y_i = 0 \\mid x_i, \\beta) = 1 - \\sigma(x_i^\\top \\beta).\n",
    "$$\n",
    "\n",
    "So the probability of observing (y_i) is:\n",
    "\n",
    "$$\n",
    "P(y_i \\mid x_i, \\beta) = \\big[\\sigma(x_i^\\top \\beta)\\big]^{y_i} \\cdot \\big[1 - \\sigma(x_i^\\top \\beta)\\big]^{1-y_i}.\n",
    "$$\n",
    "\n",
    "This formula works because if ($y_i = 1$), the first term stays; if ($y_i = 0$), the second term stays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Likelihood for the whole dataset\n",
    "\n",
    "Since data points are independent:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n \\big[\\sigma(x_i^\\top \\beta)\\big]^{y_i} \\cdot \\big[1 - \\sigma(x_i^\\top \\beta)\\big]^{1-y_i}.\n",
    "$$\n",
    "\n",
    "Taking logs:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n \\Big[ y_i \\log \\sigma(x_i^\\top \\beta) + (1-y_i)\\log (1 - \\sigma(x_i^\\top \\beta)) \\Big].\n",
    "$$\n",
    "This is called the logit.\n",
    "\n",
    "With this, it is possible to define the loss or cost function as \n",
    "\\begin{equation}\n",
    "Cost = -\\ell (\\beta)  = -\\sum[y\\log(\\hat y) - (1-y)\\log(1-\\hat y)],\n",
    "\\end{equation}\n",
    "to define it positive and compute the minimum. This cost function maximizes the probability for each label. For instance, if the label must be 1 but the predicted probability for it is 0.1, then we have a large penalty. Same for the inverse case. Also, it can be used nicely with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Why not least squares?\n",
    "\n",
    "* In linear regression, minimizing squared error works because the residuals are assumed Gaussian.\n",
    "* But in logistic regression, the outcomes are binary ((0/1)), not continuous.\n",
    "* Squared error isn’t statistically justified here. Instead, we use **maximum likelihood**, because it directly models the probability of binary outcomes using the Bernoulli distribution.\n",
    "\n",
    "**Summary**:\n",
    "\n",
    "* The **likelihood function** measures how probable the observed data is under your model.\n",
    "* **Maximum likelihood estimation (MLE)** finds the parameters that maximize this probability.\n",
    "* Logistic regression is naturally derived via MLE, since binary outcomes follow a **Bernoulli distribution**, and maximizing the likelihood leads to the standard logistic regression log-loss function used in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Gradient descent implementation\n",
    "For the gradient descent update (\"backward propagation\"), we have\n",
    "\\begin{equation}\n",
    "\\beta_i' = \\beta_i - \\alpha \\frac{\\partial Cost}{\\beta_i}. \n",
    "\\end{equation}\n",
    "Given that the cost function is a function of $\\hat y$, and $\\hat y = 1/(1 + \\exp(-z))$, with $z= \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_n X_n$, then, by using the chain rule, we have\n",
    "\\begin{align}\n",
    "\\frac{\\partial Cost}{\\partial \\beta_i} &= \\frac{\\partial Cost}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial z}\\frac{\\partial z}{\\partial \\beta_i}\\\\\n",
    "\\frac{\\partial Cost}{\\partial \\beta_i} &= (\\hat y - y) X_i,\n",
    "\\end{align}\n",
    "(for $\\beta_0$ it is only $\\hat y - y$), and this shows how to simply update the parameters. \n",
    "\n",
    "This is an example (here $b = \\beta_0$ and $\\vec w = (\\beta_1, \\beta_2, \\ldots, \\beta_n)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}\n",
    "Deduce analytically the previous expressions for the parameters estimation\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is an example of a manual implementation for the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model\"\"\"\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Compute cost (for monitoring)\n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters using gradient descent\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "            self.bias = self.bias - self.learning_rate * db\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost:.4f}\")\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Compute cross-entropy cost\"\"\"\n",
    "        m = len(y_true)\n",
    "        # Avoid log(0) by clipping predictions\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        cost = -(1/m) * np.sum(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = LogisticRegression(learning_rate=0.1, max_iterations=1000)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    probabilities = model.predict_proba(X)\n",
    "    \n",
    "    print(f\"Final weights: {model.weights}\")\n",
    "    print(f\"Final bias: {model.bias}\")\n",
    "    print(f\"Accuracy: {np.mean(predictions == y):.4f}\")\n",
    "    \n",
    "    # Plot cost function\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(model.costs)\n",
    "    plt.title('Cost Function Over Training')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} SDGClassifier\n",
    "Implement the same but using scikit-learn SDGClassifier (use partial fit to contorl the numbers of iterations). Plot the loss as function of the iterations.  \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baef73b117d4881ca477b062774e6023",
     "grade": false,
     "grade_id": "cell-bc450f2e7aa6ca7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.model = None\n",
    "        self.costs = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "    model_sgd = LogisticRegressionSGD(learning_rate=0.1, max_iterations=1000)\n",
    "    model_sgd.fit(X, y)\n",
    "\n",
    "    predictions = model_sgd.predict(X)\n",
    "    probabilities = model_sgd.predict_proba(X)\n",
    "\n",
    "    print(f\"Accuracy: {np.mean(predictions == y):.4f}\")\n",
    "\n",
    "    # Plot cost function\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(model_sgd.costs, label=\"SGDClassifier\")\n",
    "    plt.title(\"Cost Function Over Training (SGD)\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost (Log-loss)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# -------------------------\n",
    "# 1. Batch Gradient Descent\n",
    "# -------------------------\n",
    "class LogisticRegressionBatch:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            # Predictions\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = log_loss(y, y_pred)\n",
    "            self.costs.append(cost)\n",
    "\n",
    "            # Gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # Update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "# -------------------------\n",
    "# 2. SGD (stochastic, batch_size=1)\n",
    "# -------------------------\n",
    "def run_sgd(X, y, learning_rate=0.01, max_iterations=1000, batch_size=1):\n",
    "    n_samples = X.shape[0]\n",
    "    model = SGDClassifier(\n",
    "        loss=\"log_loss\",\n",
    "        learning_rate=\"constant\",\n",
    "        eta0=learning_rate,\n",
    "        max_iter=1,\n",
    "        tol=None,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    classes = np.unique(y)\n",
    "    costs = []\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Select a random mini-batch\n",
    "        idx = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        X_batch, y_batch = X[idx], y[idx]\n",
    "\n",
    "        model.partial_fit(X_batch, y_batch, classes=classes)\n",
    "\n",
    "        # Compute global log-loss on full data\n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "        cost = log_loss(y, y_pred_proba)\n",
    "        costs.append(cost)\n",
    "\n",
    "    return model, costs\n",
    "\n",
    "# -------------------------\n",
    "# 3. Run comparison\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(200, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "    max_iter = 2000\n",
    "\n",
    "    # Batch GD\n",
    "    batch_model = LogisticRegressionBatch(learning_rate=0.1, max_iterations=max_iter)\n",
    "    batch_model.fit(X, y)\n",
    "\n",
    "    # Stochastic GD (batch size = 1)\n",
    "    _, sgd_costs = run_sgd(X, y, learning_rate=0.1, max_iterations=max_iter, batch_size=1)\n",
    "\n",
    "    # Mini-batch GD (batch size = 32)\n",
    "    _, mb_costs = run_sgd(X, y, learning_rate=0.1, max_iterations=max_iter, batch_size=32)\n",
    "\n",
    "    # -------------------------\n",
    "    # Plot comparison\n",
    "    # -------------------------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(batch_model.costs, label=\"Batch GD\", linewidth=2)\n",
    "    plt.plot(sgd_costs, label=\"Stochastic GD (batch=1)\", alpha=0.7)\n",
    "    plt.plot(mb_costs, label=\"Mini-Batch GD (batch=32)\", alpha=0.7)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Log-loss (Cost)\")\n",
    "    plt.title(\"Comparison of Gradient Descent Variants\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Practical Example: Breast Cancer Tumor Classification\n",
    "\n",
    "Let's build a logistic regression model to predict whether a breast cancer tumor is **malignant** or **benign**. We will use the Breast Cancer Wisconsin dataset, which is conveniently included in the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "# Create a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "df['target'] = cancer_data.target # 0: malignant, 1: benign\n",
    "\n",
    "print(\"Feature Names:\", cancer_data.feature_names)\n",
    "print(\"\\nTarget Names:\", cancer_data.target_names)\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Our goal is to use the feature columns (like 'mean radius', 'mean texture', etc.) to predict the 'target' column. Note that in this dataset, `0` represents a malignant tumor and `1` represents a benign tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the Data\n",
    "\n",
    "We need to split our data into a training set (to build the model) and a testing set (to evaluate its performance on unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define our features (X) and target (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3: Train the Logistic Regression Model\n",
    "\n",
    "Now we'll use `scikit-learn`'s `LogisticRegression` class to train our model. For numerical stability, it's often a good idea to scale our features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(n_jobs=1)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "That's it! The model is now trained. The `.fit()` method found the best coefficients (β values) to map our input features to the probability of a tumor being benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the Model\n",
    "\n",
    "How well did our model do? We'll make predictions on our held-out test set and compare them to the actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display the Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer_data.target_names))\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer_data.target_names, \n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Interpreting the Results\n",
    "\n",
    "- **Accuracy**: The overall percentage of correct predictions. Our model is highly accurate!\n",
    "- **Classification Report**: \n",
    "    - **Precision**: Of all the tumors we *predicted* as malignant, how many actually were? (Measures false positives).\n",
    "    - **Recall (Sensitivity)**: Of all the tumors that *truly were* malignant, how many did we correctly identify? (Measures false negatives). This is often a critical metric in medical diagnostics.\n",
    "    - **F1-Score**: The harmonic mean of precision and recall.\n",
    "- **Confusion Matrix**: A visual breakdown of our predictions.\n",
    "    - **Top-Left**: True Negatives (Predicted Malignant, Was Malignant)\n",
    "    - **Top-Right**: False Positives (Predicted Benign, Was Malignant)\n",
    "    - **Bottom-Left**: False Negatives (Predicted Malignant, Was Benign)\n",
    "    - **Bottom-Right**: True Positives (Predicted Benign, Was Benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Interpretation\n",
    "\n",
    "Looking at the confusion matrix generated above:\n",
    "1. How many benign tumors were incorrectly classified as malignant (False Negatives)?\n",
    "2. Why might recall be a more important metric than precision for the 'malignant' class in this specific medical context?\n",
    "3. Compite the \"report\": Precission, Recall, F1-Score\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Impact of Test Size\n",
    "\n",
    "Go back to **Step 2: Split the Data**. Change the `test_size` from `0.2` to `0.3` (meaning 30% of the data will be used for testing). Re-run all the subsequent cells. Did the model's accuracy on the test set change? Why do you think this might happen?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Predicting a Single Observation\n",
    "\n",
    "Imagine you have a new tumor with the characteristics of the first row of our original dataset. Use the trained `model` and `scaler` to predict whether this single tumor is malignant or benign. \n",
    "\n",
    "**Hint**: You will need to select the first row from `X`, reshape it, scale it, and then use `model.predict()` and `model.predict_proba()`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base code\n",
    "\n",
    "# Get the first sample from the original (unscaled) dataset X\n",
    "single_sample = X.iloc[[0]] # Using [[0]] keeps it as a DataFrame\n",
    "\n",
    "# Scale the sample using the FITTED scaler\n",
    "# Your code here\n",
    "\n",
    "# Make a prediction (0 or 1)\n",
    "# Your code here\n",
    "\n",
    "# Get the probabilities\n",
    "# Your code here\n",
    "\n",
    "# print(f\"The predicted class is: {prediction[0]}\")\n",
    "# print(f\"The probability of each class is: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
