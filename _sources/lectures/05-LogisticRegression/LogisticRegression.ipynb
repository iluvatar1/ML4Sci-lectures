{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# An Introduction to Logistic Regression\n",
    "Logistic regression, in spite of its name, is a CLASSIFICATION tool.\n",
    "\n",
    "Please check the visually explained intro: <https://www.youtube.com/watch?v=3bvM3NyMiE0&t=0s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Motivation: Why Not Linear Regression for Classification?\n",
    "\n",
    "Linear regression is excellent for predicting continuous values, like the price of a house or the temperature tomorrow. But what if we want to predict a categorical outcome? For example:\n",
    "\n",
    "- Will a patient test positive or negative for a disease?\n",
    "- Is an email spam or not spam?\n",
    "- Is a tumor malignant or benign?\n",
    "\n",
    "These are **classification problems** with binary outcomes (Yes/No, 1/0, True/False).\n",
    "\n",
    "Let's see what happens if we try to fit a linear regression model to a binary outcome. Imagine we have data on tumor size and whether it's malignant (1) or benign (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "tumor_size = np.random.normal(10, 5, 100)\n",
    "is_malignant = (tumor_size + np.random.normal(0, 3, 100) > 12).astype(int)\n",
    "\n",
    "# Fit a linear regression line\n",
    "m, b = np.polyfit(tumor_size, is_malignant, 1)\n",
    "\n",
    "# Plot the data and the line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(tumor_size, is_malignant, label='Data (0=Benign, 1=Malignant)', alpha=0.7)\n",
    "plt.plot(tumor_size, m*tumor_size + b, color='red', label='Linear Regression Fit')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=1, color='gray', linestyle='--')\n",
    "plt.xlabel('Tumor Size')\n",
    "plt.ylabel('Malignant (1) or Benign (0)')\n",
    "plt.title('Why Linear Regression Fails for Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the linear regression line extends beyond 0 and 1. How would we interpret a prediction of 1.5? Or -0.5? It doesn't make sense as a probability. \n",
    "\n",
    "We need a model that outputs a value between 0 and 1, which can be interpreted as the **probability** of the outcome being 'Yes' (or 1).\n",
    "\n",
    "This is where **Logistic Regression** comes in. It uses a special S-shaped function, the **Sigmoid function**, to squash the output of a linear equation into the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Core Component: The Sigmoid Function\n",
    "\n",
    "The sigmoid function is a mathematical function that takes any real number and maps it to a value between 0 and 1. \n",
    "\n",
    "The formula is:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Where `z` is the output of our linear equation (e.g., `z = mx + b`).\n",
    "\n",
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "# prepare some data\n",
    "# create a new plot with a title and axis labels\n",
    "p = figure(title=\"The sigmoid function\", x_axis_label='z', y_axis_label=rf'$$\\sigma(z)$$')\n",
    "# add a line renderer with legend and line thickness to the plot\n",
    "p.line(z, sigma_z, line_width=2)\n",
    "p.line(z, 0.5, legend_label=\"Threshold at 0.5\", line_width=2, color='red')\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate values for z\n",
    "# Plot the sigmoid function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigma_z, color='blue')\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold at 0.5')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.xlabel('z (output of linear equation)')\n",
    "plt.ylabel('Probability (σ(z))')\n",
    "plt.title('The Sigmoid Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Key Observations:**\n",
    "- When `z` is large and positive, `e^-z` approaches 0, so `σ(z)` approaches 1.\n",
    "- When `z` is large and negative, `e^-z` becomes very large, so `σ(z)` approaches 0.\n",
    "- When `z = 0`, `e^0 = 1`, so `σ(z) = 1 / (1 + 1) = 0.5`.\n",
    "\n",
    "This is perfect for modeling probability! The output of the sigmoid function, `σ(z)`, can be interpreted as the probability of the positive class (e.g., the probability that a tumor is malignant).\n",
    "\n",
    "$$ P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n)}} $$\n",
    "\n",
    "We typically set a **decision boundary** or **threshold** at 0.5. \n",
    "- If `P(Y=1) > 0.5`, we classify the outcome as 1 (Malignant).\n",
    "- If `P(Y=1) <= 0.5`, we classify the outcome as 0 (Benign)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Conceptual\n",
    "Based on the sigmoid plot, if a new tumor size results in a `z` value of 5, would you classify it as benign or malignant? What about a `z` value of -2? Explain your reasoning.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Practical Example: Breast Cancer Tumor Classification\n",
    "\n",
    "Let's build a logistic regression model to predict whether a breast cancer tumor is **malignant** or **benign**. We will use the Breast Cancer Wisconsin dataset, which is conveniently included in the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "# Create a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "df['target'] = cancer_data.target # 0: malignant, 1: benign\n",
    "\n",
    "print(\"Feature Names:\", cancer_data.feature_names)\n",
    "print(\"\\nTarget Names:\", cancer_data.target_names)\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Our goal is to use the feature columns (like 'mean radius', 'mean texture', etc.) to predict the 'target' column. Note that in this dataset, `0` represents a malignant tumor and `1` represents a benign tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the Data\n",
    "\n",
    "We need to split our data into a training set (to build the model) and a testing set (to evaluate its performance on unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define our features (X) and target (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3: Train the Logistic Regression Model\n",
    "\n",
    "Now we'll use `scikit-learn`'s `LogisticRegression` class to train our model. For numerical stability, it's often a good idea to scale our features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "That's it! The model is now trained. The `.fit()` method found the best coefficients (β values) to map our input features to the probability of a tumor being benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the Model\n",
    "\n",
    "How well did our model do? We'll make predictions on our held-out test set and compare them to the actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display the Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer_data.target_names))\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer_data.target_names, \n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Interpreting the Results\n",
    "\n",
    "- **Accuracy**: The overall percentage of correct predictions. Our model is highly accurate!\n",
    "- **Classification Report**: \n",
    "    - **Precision**: Of all the tumors we *predicted* as malignant, how many actually were? (Measures false positives).\n",
    "    - **Recall (Sensitivity)**: Of all the tumors that *truly were* malignant, how many did we correctly identify? (Measures false negatives). This is often a critical metric in medical diagnostics.\n",
    "    - **F1-Score**: The harmonic mean of precision and recall.\n",
    "- **Confusion Matrix**: A visual breakdown of our predictions.\n",
    "    - **Top-Left**: True Negatives (Predicted Malignant, Was Malignant)\n",
    "    - **Top-Right**: False Positives (Predicted Benign, Was Malignant)\n",
    "    - **Bottom-Left**: False Negatives (Predicted Malignant, Was Benign)\n",
    "    - **Bottom-Right**: True Positives (Predicted Benign, Was Benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Interpretation\n",
    "\n",
    "Looking at the confusion matrix generated above:\n",
    "1. How many benign tumors were incorrectly classified as malignant (False Negatives)?\n",
    "2. Why might recall be a more important metric than precision for the 'malignant' class in this specific medical context?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Impact of Test Size\n",
    "\n",
    "Go back to **Step 2: Split the Data**. Change the `test_size` from `0.2` to `0.3` (meaning 30% of the data will be used for testing). Re-run all the subsequent cells. Did the model's accuracy on the test set change? Why do you think this might happen?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Predicting a Single Observation\n",
    "\n",
    "Imagine you have a new tumor with the characteristics of the first row of our original dataset. Use the trained `model` and `scaler` to predict whether this single tumor is malignant or benign. \n",
    "\n",
    "**Hint**: You will need to select the first row from `X`, reshape it, scale it, and then use `model.predict()` and `model.predict_proba()`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base code\n",
    "\n",
    "# Get the first sample from the original (unscaled) dataset X\n",
    "single_sample = X.iloc[[0]] # Using [[0]] keeps it as a DataFrame\n",
    "\n",
    "# Scale the sample using the FITTED scaler\n",
    "# Your code here\n",
    "\n",
    "# Make a prediction (0 or 1)\n",
    "# Your code here\n",
    "\n",
    "# Get the probabilities\n",
    "# Your code here\n",
    "\n",
    "# print(f\"The predicted class is: {prediction[0]}\")\n",
    "# print(f\"The probability of each class is: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Where to Find Interactive Complementary Material\n",
    "\n",
    "Reading code and text is great, but interacting with visualizations can deepen your understanding. Here are some excellent resources:\n",
    "\n",
    "*   **ML-Playground's Logistic Regression**: An interactive playground where you can add data points and see how the logistic curve and decision boundary adapt instantly. [Link](http://ml-playground.com/#logistic_regression)\n",
    "*   **Explained Visually - Logistic Regression**: A beautiful, scrolling visual explanation of the core concepts. [Link](https://setosa.io/ev/logistic-regression/)\n",
    "*   **R2D3.us - A Visual Introduction to Machine Learning**: While this covers more than just logistic regression, Part 2 provides an excellent visual and interactive breakdown of classification that is highly relevant. [Link](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
