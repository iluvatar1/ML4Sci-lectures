{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fast introduction to Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning\n",
    "\n",
    "> Machine learning is the field of study that gives computers the ability\n",
    "to learn without being explicitly programmed. Arthur Samuel, 1959\n",
    "\n",
    "Learning:\n",
    "> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. Tom Mitchell’s, 1997\n",
    "\n",
    "Machine learning is a branch of artificial intelligence that allows computers to learn from given information and perform new but similar tasks. \n",
    "\n",
    "<img src=\"./figs/ML-googletrends.png\" alt=\"ML trends\" width=80% align=\"center\" >\n",
    "\n",
    "\n",
    "**Artificial intelligence**: The computer performs actions defined as requiring intelligence -> Moving target\n",
    "- Search Based Heuristic Optimization\n",
    "- Evolutionary computation\n",
    "- Logic Programming (inductive logic programming, fuzzy logic)\n",
    "- robabilistic Reasoning Under Uncertainty (bayesian networks)\n",
    "- Computer Vision\n",
    "- Natural Language Processing\n",
    "- Robotics\n",
    "- Machine Learning\n",
    "\n",
    "Examples:\n",
    "- Self-driving cars\n",
    "- ChatGPT (LLM)\n",
    "- Healthcare: Diagnosis from scans\n",
    "- Finance: Fraud detection\n",
    "- Retail: Recommender systems\n",
    "- Transport: Autonomous vehicles\n",
    "- Creativity: AI art, music, writing\n",
    "\n",
    "\n",
    "<!-- <div style=\"text-align: center;\">\n",
    "    <img src=\"figs/AI-ML.png\" alt=\"Machine learning as a subarea of artificail intelligence. From: Understanding Deep Learning, Simon J.D. Prince\" width=\"600\">\n",
    "    <figcaption>From: Understanding Deep Learning, Simon J.D. Prince</figcaption>\n",
    "</div> -->\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical development\n",
    "- https://letsdatascience.com/learn/history/history-of-machine-learning/\n",
    "- https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/2-history-of-ML/README.md\n",
    "- https://www.inveniam.fr/a-brief-history-of-machine-learning\n",
    "- https://ahistoryofai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware was too costly, but improvements on both cpu and gpu made it practical, or at least attainable, to apply the different AI models.\n",
    "\n",
    "<img src=\"figs/transistors-per-microprocessor.png\" alt=\"transistor versus microprocesos\" width=\"60%\" align=\"center\">\n",
    "\n",
    "\n",
    "<img src=\"https://epochai.org/assets/images/posts/2022/gpu-perf/gpu-perf-banner.png\" alt=\"gpu-perf over time\" style=\"width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of learning\n",
    "<img src=\"./figs/AI-ML.png\" alt=\"ML trends\" width=40% align=\"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical, short, fast example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data creation\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "plt.title(\"Input data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised\n",
    "- Learn from labeled examples\n",
    "- Task: Prediction (classification or regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Train a classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "plt.title(\"Supervised Learning: Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised \n",
    "- No labels, find structure in data\n",
    "- Task: Clustering or dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "preds = kmeans.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=preds, cmap='cool')\n",
    "plt.title(\"Unsupervised: K-means Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "- Learn by trial and error\n",
    "- Agent interacts with environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://www.youtube.com/embed/L_4BPjLBF4E?si=4vNIAkUAz7tkTyiO\", width=\"560\", height=\"315\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical workflow\n",
    "<img src=\"./figs/ML-workflow.png\" alt=\"ML workflow\" width=\"50%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Dataset Collection**: Depends on the experiment or goals. What kind of data ? (categorical numerical) How much data? Units? reference data? data base? Data storage/access? \n",
    "2. **Dataset preprocessing**: Cleaning data. Missing data. Noise. Outliers. Normalization. Training and test sets. Or Train, validation (for hyper parameters), and test set. \n",
    "3. **Model training**: Depends on the actual approach. For supervised learning we need both input and output values. For unsupervised only input. No underfitting or overfitting. \n",
    "4. **Model evaluation**: Testing the training success, with some defined metrics. Maybe needs to redo some previous steps.\n",
    "\n",
    "### Core concepts\n",
    "- **Data**: examples used for learning\n",
    "- **Features**: inputs (e.g., age, temperature, pixels)\n",
    "- **Model**: function that maps input to output\n",
    "- **Training**: adjusting model to reduce error\n",
    "- **Testing**: evaluate model on new data\n",
    "- **Meta-Parameters**: Parameters controlling the model\n",
    "\n",
    "Beware of under/over fitting : See also last part of <https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x = np.linspace(0, 6, 30)\n",
    "y = np.sin(x) + 0.3 * np.random.randn(30)\n",
    "X = x[:, np.newaxis]\n",
    "\n",
    "# True function\n",
    "x_plot = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Underfitting (degree=1)\n",
    "plt.subplot(1, 3, 1)\n",
    "model_under = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "model_under.fit(X, y)\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.plot(x_plot, np.sin(x_plot), label='true function')\n",
    "plt.plot(x_plot, model_under.predict(x_plot), label='underfit model')\n",
    "plt.title(\"Underfitting\")\n",
    "plt.legend()\n",
    "\n",
    "# Good fit (degree=3)\n",
    "plt.subplot(1, 3, 2)\n",
    "model_good = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "model_good.fit(X, y)\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.plot(x_plot, np.sin(x_plot), label='true function')\n",
    "plt.plot(x_plot, model_good.predict(x_plot), label='good fit')\n",
    "plt.title(\"Good Fit\")\n",
    "plt.legend()\n",
    "\n",
    "# Overfitting (degree=15)\n",
    "plt.subplot(1, 3, 3)\n",
    "model_over = make_pipeline(PolynomialFeatures(15), LinearRegression())\n",
    "model_over.fit(X, y)\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.plot(x_plot, np.sin(x_plot), label='true function')\n",
    "plt.plot(x_plot, model_over.predict(x_plot), label='overfit model')\n",
    "plt.title(\"Overfitting, poor generalization\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithms\n",
    "- https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet\n",
    "- https://sites.google.com/view/datascience-cheat-sheets\n",
    "- https://github.com/SamBelkacem/AI-ML-cheatsheets\n",
    "- https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "<img src=\"./figs/ML-CheatSheet-01.webp\" alt=\"Some algs ML\" class=\"centerimg50\">\n",
    "<img src=\"figs/ML-Cheat-Sheet_2.png\" alt=\"ML Cheat sheet: https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet\" width=\"80%\" align=\"center\">\n",
    "\n",
    "\n",
    "### Classifier comparison:\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png\" alt=\"Classifiers\" class=\"centerimg30\">\n",
    "\n",
    "### Tensor flow playground\n",
    "Try: <https://playground.tensorflow.org>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Scientific Discovery\n",
    "\n",
    "\"AI is becoming a microscope for data: it helps scientists see patterns and predictions that were previously invisible.\"\n",
    "\n",
    "\n",
    "Machine Learning and AI are accelerating breakthroughs in scientific research by helping scientists extract patterns from massive datasets, automate complex processes, and even generate new hypotheses.\n",
    "\n",
    "| Field             | ML/AI Application                                                                   |\n",
    "| ----------------- | ----------------------------------------------------------------------------------- |\n",
    "| Astronomy         | Classifying galaxies from telescope data; finding exoplanets (e.g., Kepler mission) |\n",
    "| Physics           | Simulating particle collisions (e.g., CERN), anomaly detection in LHC data          |\n",
    "| Biology           | Protein folding prediction (e.g., AlphaFold), gene expression analysis              |\n",
    "| Chemistry         | Drug discovery by molecular property prediction                                     |\n",
    "| Climate Science   | Modeling weather and climate patterns; detecting extreme events                     |\n",
    "| Materials Science | Discovering new materials using generative models and property prediction           |\n",
    "| Neuroscience      | Brain activity decoding from EEG/fMRI signals                                       |\n",
    "\n",
    "**AlphaFold** by DeepMind predicts the 3D structure of proteins from amino acid sequences with remarkable accuracy — solving a 50-year grand challenge in biology.\n",
    "- <https://deepmind.google/science/alphafold/>\n",
    "- https://www.youtube.com/watch?v=gg7WjuFs8F4\n",
    "\n",
    "\n",
    "**Google Co-Scientist**\n",
    "- <https://blog.google/feed/google-research-ai-co-scientist/>\n",
    "- <https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Ethics\n",
    "- http://approximatelycorrect.com/2016/11/07/the-foundations-of-algorithmic-bias/\n",
    "- https://www.edx.org/course/data-science-ethics-michiganx-ds101x-1 \n",
    "- https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction?useskin=vector\n",
    "\n",
    "- Bias in algorithms (data reflects societal bias).\n",
    "- Transparency (black-box models).\n",
    "- Automation and the future of work.\n",
    "- AI alignment and safety.\n",
    "- Power usage:\n",
    "  <img src=\"https://www.researchgate.net/profile/Yuzhuo-Li-2/publication/384115745/figure/fig5/AS:11431281278937909@1726775834031/Reported-energy-consumption-of-training-different-LLM-models-with-respect-to-model.png\" alt=\"https://www.researchgate.net/publication/384115745_The_Unseen_AI_Disruptions_for_Power_Grids_LLM-Induced_Transients\" width=\"40%\" align=\"center\">\n",
    "  + https://www.nature.com/articles/s41598-024-76682-6\n",
    "  + https://birchtree.me/blog/another-study-on-llm-energy-use/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI in scientific discovery: examples\n",
    "Machine Learning is transforming how scientists explore and understand the world by revealing patterns, accelerating simulations, and automating classification.\n",
    "### 🔭 Example 1: Detecting Supernovae\n",
    "Classifying celestial events using features like brightness and spectral line strength.\n",
    "\n",
    "DATA: https://www.kaggle.com/competitions/PLAsTiCC-2018/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Prepare data\n",
    "X = supernova_data[['brightness', 'spectral_line_strength', 'redshift']]\n",
    "y = supernova_data['label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"Supernova Detection (Random Forest)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧬 Example 2: Classifying Protein Types\n",
    "Distinguishing enzymes from non-enzymes using biochemical features.\n",
    "\n",
    "DATA: https://archive.ics.uci.edu/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare data\n",
    "X = protein_data[['hydrophobicity', 'charge', 'molecular_weight']]\n",
    "y = LabelEncoder().fit_transform(protein_data['label'])  # enzyme=1, non-enzyme=0\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"Protein Classification (Logistic Regression)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚗️ Example 3: Predicting Molecular Boiling Point\n",
    "Using regression to estimate a physical property from structural features.\n",
    "\n",
    "DATA: https://moleculenet.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare data\n",
    "X = molecular_data[['num_atoms', 'molecular_weight', 'polarity']]\n",
    "y = molecular_data['boiling_point']\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"True Boiling Point\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Boiling Point Prediction (MSE = {mse:.2f})\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
