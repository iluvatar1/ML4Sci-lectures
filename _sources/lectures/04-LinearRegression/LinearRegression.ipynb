{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0d6e3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d5668",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Review: What is Machine Learning?\n",
    "\n",
    "At its core, **Machine Learning (ML)** is the science of getting computers to learn and act like humans do, and improve their learning over time in an autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\n",
    "\n",
    "\n",
    "\n",
    "1.  **Supervised Learning:** Learning from data that is **labeled**. You provide the algorithm with examples of inputs and their corresponding correct outputs. The goal is to learn a general rule that maps inputs to outputs. (This is our focus today).\n",
    "2.  **Unsupervised Learning:** Learning from data that is **unlabeled**. The algorithm tries to find patterns, structures, or clusters in the data on its own.\n",
    "3.  **Reinforcement Learning:** An agent learns to perform actions in an environment to maximize a cumulative reward. It learns by trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984eadc",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "> **Supervised Learning:** Given a dataset of input features **X** and corresponding output labels **y**, we want to learn a function `h` (for hypothesis) such that `h(X)` is a good predictor for **y**.\n",
    "\n",
    "For all models, you should always keep in mind the bias-variance tradeoff\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Bias_and_variance_contributing_to_total_error.svg\" width=50%>\n",
    "<figcaption> https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/250px-Bias_and_variance_contributing_to_total_error.svg.png </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "If your model has a low bias, then it might be overfitting the training data and then it will increase the predictions variance (imagine using a very large polynomial to do a fit). If your model has low variance, it might be underfitting, so the bias will be large. \n",
    "\n",
    "\n",
    "There are two primary types of supervised learning problems:\n",
    "\n",
    "### A. Regression: Predicting a Continuous Value\n",
    "The output `y` is a continuous, numerical value.\n",
    "- **Question:** Based on a material's temperature, what will its electrical resistance be?\n",
    "- **Question:** Given the mass of a star, what is its expected luminosity?\n",
    "- **Our main tool today:** **Linear Regression** See [Visually Explained: Linear regression](https://www.youtube.com/watch?v=CtsRRUddV2s)\n",
    "\n",
    "### B. Classification: Predicting a Discrete Category\n",
    "The output `y` is a discrete category or class label.\n",
    "- **Question:** Based on a cell's size and shape, is it cancerous or benign?\n",
    "- **Question:** Given the energy and momentum from a particle collider, did we detect an electron or a muon?\n",
    "- **A common tool:** **Logistic Regression** (despite its name, it's for classification!)\n",
    "\n",
    "| Feature | Linear Regression | Logistic Regression |\n",
    "| :---- | :---- | :---- |\n",
    "| **Problem Type** | Regression (predicting continuous values) | Classification (predicting categorical outcomes) |\n",
    "| **Output** | Continuous numerical value (e.g., price, temperature) | Probability (0 to 1), which is then mapped to a class |\n",
    "| **Dependent Variable** | Continuous | Categorical (binary or multi-class) |\n",
    "| **Underlying Function** | Linear equation: y=β0​+β1​x1​+...+βn​xn​ | Sigmoid (logistic) function applied to a linear equation: p=1+e−(β0​+β1​x1​+...+βn​xn​)1​ |\n",
    "| **Cost Function** | Mean Squared Error (MSE), Root Mean Squared Error (RMSE) | Log Loss (Binary Cross-Entropy), Cross-Entropy |\n",
    "| **Interpretation of Coefficients** | Change in the dependent variable for a one-unit change in the independent variable | Change in the log-odds of the dependent variable for a one-unit change in the independent variable |\n",
    "| **Assumptions** | Linearity, independence of errors, homoscedasticity, normality of residuals, no multicollinearity | Linearity of independent variables with log-odds, independence of observations, no multicollinearity |\n",
    "| **Common Use Cases** | Predicting house prices, sales forecasting, predicting exam scores, trend analysis | Spam detection, disease prediction (e.g., presence/absence), customer churn prediction, sentiment analysis |\n",
    "| **Evaluation Metrics** | MSE, RMSE, R-squared, MAE | Accuracy, Precision, Recall, F1-Score, ROC-AUC |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941f0b2-a606-464e-b0bc-0a054b8f304f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear Regression is one of the simplest and most interpretable machine learning models. It assumes a linear relationship between the input features and the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b870c3a-7310-473d-9bce-17a198618560",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#from aux.linear_regression_example_plot import generate_and_plot_regression_problems\n",
    "\n",
    "# linear_regression_plots.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # Required for 3D projection\n",
    "\n",
    "def generate_and_plot_regression_problems():\n",
    "    \"\"\"\n",
    "    Generates data and plots 2D and 3D linear regression problems.\n",
    "    Returns the matplotlib figure object.\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) # 1 row, 2 columns\n",
    "\n",
    "    # --- Left Subplot: 2D Linear Regression Problem ---\n",
    "    # Generate data for a straight line with noise\n",
    "    np.random.seed(42)\n",
    "    x_2d = np.linspace(0, 10, 50)\n",
    "    true_slope = 2.5\n",
    "    true_intercept = 5\n",
    "    y_true_2d = true_slope * x_2d + true_intercept\n",
    "    noise_2d = np.random.normal(0, 2, size=len(x_2d))\n",
    "    y_noisy_2d = y_true_2d + noise_2d\n",
    "\n",
    "    # Plot the true line\n",
    "    ax1.plot(x_2d, y_true_2d, 'r-', label='True Line')\n",
    "    # Plot the noisy points\n",
    "    ax1.scatter(x_2d, y_noisy_2d, color='blue', label='Noisy Data')\n",
    "    ax1.set_title('2D Linear Regression Problem')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # --- Right Subplot: 3D Plane Regression Problem ---\n",
    "    # Generate data for a plane with noise\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d') # Specify 3D projection\n",
    "    x_3d = np.linspace(-5, 5, 20)\n",
    "    y_3d = np.linspace(-5, 5, 20)\n",
    "    X_3d, Y_3d = np.meshgrid(x_3d, y_3d)\n",
    "    true_coeff_x = 1.2\n",
    "    true_coeff_y = -0.8\n",
    "    true_intercept_3d = 3\n",
    "    Z_true_3d = true_coeff_x * X_3d + true_coeff_y * Y_3d + true_intercept_3d\n",
    "    noise_3d = np.random.normal(0, 1.5, size=Z_true_3d.shape)\n",
    "    Z_noisy_3d = Z_true_3d + noise_3d\n",
    "\n",
    "    # Plot the true plane surface\n",
    "    ax2.plot_surface(X_3d, Y_3d, Z_true_3d, alpha=0.5, cmap='viridis', label='True Plane')\n",
    "    # Plot the noisy points\n",
    "    ax2.scatter(X_3d, Y_3d, Z_noisy_3d, color='red', s=20, label='Noisy Data') # s is marker size\n",
    "    ax2.set_title('3D Linear Regression Problem (Plane)')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.set_zlabel('Z')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # No plt.show() here, as we return the figure to be shown in Jupyter\n",
    "    return fig\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # This block only runs if the script is executed directly, not when imported\n",
    "#     fig = generate_and_plot_regression_problems()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "fig_regression_problems = generate_and_plot_regression_problems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09722a-48f9-4195-871d-1fae292378c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def generate_and_plot_regression_problems_plotly():\n",
    "    \"\"\"\n",
    "    Generates data and plots interactive 2D and 3D linear regression problems using Plotly.\n",
    "    Returns the Plotly figure object.\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots: one 2D, one 3D\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{'type': 'xy'}, {'type': 'surface'}]],\n",
    "        subplot_titles=('2D Linear Regression Problem', '3D Linear Regression Problem (Plane)'),\n",
    "        column_widths=[0.4, 0.9] \n",
    "    )\n",
    "\n",
    "    # --- Left Subplot: 2D Linear Regression Problem ---\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    x_2d = np.linspace(0, 10, 50)\n",
    "    true_slope = 2.5\n",
    "    true_intercept = 5\n",
    "    y_true_2d = true_slope * x_2d + true_intercept\n",
    "    noise_2d = np.random.normal(0, 2, size=len(x_2d))\n",
    "    y_noisy_2d = y_true_2d + noise_2d\n",
    "\n",
    "    # Add the true line trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_2d, y=y_true_2d, mode='lines', name='True Line', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # Add the noisy data points trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_2d, y=y_noisy_2d, mode='markers', name='Noisy Data', marker=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # --- Right Subplot: 3D Plane Regression Problem ---\n",
    "    # Generate data\n",
    "    x_3d = np.linspace(-5, 5, 20)\n",
    "    y_3d = np.linspace(-5, 5, 20)\n",
    "    X_3d, Y_3d = np.meshgrid(x_3d, y_3d)\n",
    "    true_coeff_x = 1.2\n",
    "    true_coeff_y = -0.8\n",
    "    true_intercept_3d = 3\n",
    "    Z_true_3d = true_coeff_x * X_3d + true_coeff_y * Y_3d + true_intercept_3d\n",
    "    noise_3d = np.random.normal(0, 1.5, size=Z_true_3d.shape)\n",
    "    Z_noisy_3d = Z_true_3d + noise_3d\n",
    "\n",
    "    # Add the true plane surface trace\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=X_3d, y=Y_3d, z=Z_true_3d, opacity=0.7, colorscale='viridis', name='True Plane', showscale=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    # Add the noisy 3D data points trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=X_3d.flatten(), y=Y_3d.flatten(), z=Z_noisy_3d.flatten(),\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, color='red'),\n",
    "            name='Noisy Data'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # --- Update layout and axis titles ---\n",
    "    fig.update_layout(\n",
    "        title_text='Linear Regression Examples',\n",
    "        height=500,\n",
    "        width=1200,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        )\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"X\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Generate and show the plots in the Jupyter Notebook\n",
    "fig_regression_problems_plotly = generate_and_plot_regression_problems_plotly()\n",
    "fig_regression_problems_plotly.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b550f66-a7aa-4010-9adc-52e703df4a7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Some applications to basic sciences\n",
    "Physics and Statistical Physics (Linear Regression/Ridge Regression)\n",
    "\n",
    "- Alexander Mozeika, Mansoor Sheikh, Fabian Aguirre-Lopez, Fabrizio Antenucci, and Anthony C. C. Coolen (2021). Exact results on high-dimensional linear regression via statistical physics. PHYSICAL REVIEW E, 103, 042142 (2021). DOI/Link <doi.org/10.1103/PhysRevE.103.042142>\n",
    "- J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová (2019). The statistical physics of ridge regression and the bias-variance trade-off. Proceedings of the National Academy of Sciences U.S.A., 116, 5451 (2019)\n",
    "- M. Advani and S. Ganguli (2016). Statistical physics of high-dimensional inference: The linear regression model. Physical Review X, 6, 031034 (2016)\n",
    "\n",
    "Materials Science and Engineering (Linear Regression/Knowledge Discovery)\n",
    "\n",
    "- Doreswamy, Hemanth K S, and Manohar M G (2011). Linear Regression Model for Knowledge Discovery in Engineering Materials. AIAA 2011, CS & IT 03, pp. 147–156 (2011) . DOI/Link: 10.5121/csit.2011.1313\n",
    "- A. M. Deml, R. O’Hayre, C. Wolverton, and V. Stevanović (2016). Predicting density functional theory total energies and enthalpies of formation of metal-nonmetal compounds by linear regression. Phys. Rev. B: Condens. Matter Mater. Phys., 93, 085142 (2016)\n",
    "\n",
    "Biology and Genomics (Linear Models/RNA-Seq and Microarray)\n",
    "\n",
    "- M. E. Ritchie, B. Phipson, D. Wu, Y. Hu, C. W. Law, W. Shi, and G. K. Smyth (2015). limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Research, 43(7), e47 (2015)\n",
    "- C. W. Law, Y. Chen, W. Shi, and G. K. Smyth (2014). Voom: precision weights unlock linear model analysis tools for RNA-seq read counts. Genome Biology, 15, R29 (2014)\n",
    "\n",
    "Chemistry, Analytical Chemistry, and Drug Design (Multiple Linear Regression - MLR)\n",
    "\n",
    "- J. I. García, H. García-Marín, J. A. Mayoral, and P. Pérez (2013). Quantitative structure-property relationships prediction of some physic-chemical properties of glycerol based solvents. Green Chemistry, 15, 2283–2293 (2013) \n",
    "- A. G. A. Jameel, N. Naser, A-H. Emwas, S. Dooley, and S. M. Sarathy (2016). Predicting fuel ignition quality using 1H NMR spectroscopy and multiple linear regression.  Energy & Fuels, 30, 9819–9835 (2016)\n",
    "\n",
    "Environmental Science and Planning (Linear and Spatial Regression Models)\n",
    "\n",
    "- Sanqing He, Yanan Sun, Ningyi Zeng, Lei Wang, Zihan Cao, and Zhen He (2025). Regional divergence in the urban form-carbon emission nexus: a comparative analysis of linear and non-linear spatial modeling approaches for 286 Chinese cities. Frontiers in Environmental Science, 13:1658538 (2025).  10.3389/fenvs.2025.1658538"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeab71a-a99c-4057-8de9-51f9e26fc5a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Goal\n",
    "To find the \"best-fit\" line that describes the data. For a single input feature `x`, the equation of the line is:\n",
    "\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x $$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ (y-hat) is the **predicted value**.\n",
    "- $x$ is the **input feature**.\n",
    "- $\\theta_0$ (theta-zero) is the **y-intercept** (also called the bias). It's the value of $\\hat{y}$ when $x=0$.\n",
    "- $\\theta_1$ (theta-one) is the **slope** or **coefficient**. It represents the change in $\\hat{y}$ for a one-unit change in $x$.\n",
    "\n",
    "Our goal is to find the optimal values for $\\theta_0$ and $\\theta_1$ that make our line fit the data as closely as possible. We are going to follow to paths to do this: the classical exact solution, and the ML (gradient descent) way.\n",
    "\n",
    "\n",
    "| Aspect | Classical (Analytical) | Machine Learning (Gradient Descent) |\n",
    "|--------|----------------------|-------------------------------------|\n",
    "| **Mathematical Foundation** | Calculus: $\\partial/\\partial\\theta = 0$ | Optimization: $\\theta = \\theta -\\alpha \\nabla\\theta$ |\n",
    "| **Solution Type** | Exact (closed-form) | Iterative approximation |\n",
    "| **Computation Time** | Fast (single calculation) | Slower (many iterations) |\n",
    "| **Memory Requirements** | Can handle large datasets | May need batch processing |\n",
    "| **Scalability** | Limited by matrix operations | Scales to massive datasets |\n",
    "| **Interpretability** | Direct mathematical insight | Requires convergence analysis |\n",
    "| **Flexibility** | Fixed to linear relationships | Easily extended (regularization, non-linear) |\n",
    "| **Noise Handling** | Assumes well-behaved data | Naturally robust to outliers |\n",
    "| **Implementation** | `np.linalg.lstsq()` or `statsmodels` | Custom loops or `SGDRegressor` |\n",
    "| **When to Use** | Small-medium datasets, interpretability needed | Large datasets, part of ML pipeline |\n",
    "\n",
    "**The Key Insight**\n",
    "\n",
    "Both methods minimize the **same cost function**:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "- **Classical**: Solves $\\nabla$MSE = 0 analytically\n",
    "- **ML**: Follows $\\nabla$MSE downhill iteratively\n",
    "\n",
    "They're different paths to the same mathematical destination!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98f5f8-58e8-4571-9e33-a17211f59ab6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### How Do We Find the \"Best\" Line? Classical approach\n",
    "For this simple example, computing both parameters is \"easily\" done by defining a metric to minimize, computing its partial derivatives, making them null and then computing the parameters. The distance from a giving predicted pint from its corresponding data is $(\\hat y_i - y_i)^2$, and it is squared to take into account values over or under estimating the data. We can define the **MEAN SQUARED ERROR** as\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta (\\theta_0, \\theta_1) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\sum_{i=1}^{n} ((\\theta_0 + \\theta_1 x_i) - y_i)^2,\n",
    "\\end{equation}\n",
    "and the goal is to minimize it. \n",
    "\n",
    "In this simple case, we can compute $\\partial \\Delta/\\partial \\theta_0 = 0$ and $\\partial \\Delta/\\partial \\theta_1 = 0$, and from it arrive to \n",
    "\\begin{align}\n",
    "\\theta_1 &= \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{n\\sum x_i^2 - (\\sum x_i)^2},\\\\\\\\\n",
    "\\theta_0 &= \\frac{\\sum y_i - \\theta_1\\sum x_i}{n}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Ordinary Least Squares(OLS) formulation, and is good for simple problems. You can use python `statmodels` to get even more info about your solution and parameters. But generalizing it to more dimensions, for instance, might be more difficult and cumbersome, so maybe we can re-formulate the problem in another way: \"learning\" the parameters so the model can be tested on the data and its predictive power be used for other data. It is important to not blindly apply OLS, since it can fail when some assumptions are not fulfilled (more at the end) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09c060-f754-457f-9eae-76d9fcc8f794",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "# Simulate nonlinear relation + heteroscedastic noise\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_true = 3 * np.sin(x / 2) + 0.5 * x\n",
    "noise = np.random.normal(0, 1 + 0.5 * x, size=x.shape)\n",
    "y = y_true + noise\n",
    "\n",
    "# Fit simple linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = x.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(x, y, label='Data', alpha=0.6)\n",
    "plt.plot(x, y_pred, color='red', label='OLS Fit')\n",
    "plt.plot(x, y_true, color='green', linestyle='dashed', label='True Relation')\n",
    "plt.legend()\n",
    "plt.title(\"When linear fit poorly captures true relation\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "resid = y - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, resid, alpha=0.6)\n",
    "plt.hlines(0, min(y_pred), max(y_pred), color='black')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted: Nonlinearity and Heteroscedasticity visible')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e830633-bc10-44ef-9b62-c17f96bcb2de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### How do we find the \"Best\" Line? Learning the coefficients\n",
    "\n",
    "We need a way to quantify how \"wrong\" our line is. To do so, we define the same  **Cost Function** (or Loss Function) as before:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta (\\theta_0, \\theta_1) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\sum_{i=1}^{n} ((\\theta_0 + \\theta_1 x_i) - y_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    ":::{note}\n",
    "There are several definitions for the loss, like with absolute value, or with the square root of the quantity shown. Each one has different advantages and disadvantages, like slower/faster convergence, sensitivity to outliers and so on.\n",
    ":::\n",
    "\n",
    "To optimize this, we will follow and approach that is ubiquitous in machine learning: start with some random values, compute the loss function and its gradient, adjust the coefficient values according to the loss magnitude and iterate. This implies that e are using back propagation! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1642a-0307-4e37-ab5e-9783dd8910de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Optimization with Gradient Descent\n",
    "\n",
    "**Analogy:** Imagine you are a hiker in a foggy valley and you want to get to the lowest point. You can't see the whole valley, but you can feel the slope of the ground right under your feet. What do you do? You take a step in the steepest downward direction.\n",
    "\n",
    "This is exactly what Gradient Descent does:\n",
    "1.  Start with some random values for $\\theta_0$ and $\\theta_1$.\n",
    "2.  Calculate the gradient (the \"slope\") of the cost function at that point.\n",
    "3.  Take a small step in the opposite direction of the gradient (downhill).\n",
    "4.  Repeat until you reach the bottom (the minimum), where the slope is zero.\n",
    "\n",
    "```{tip}\n",
    "For a nice visualization of gradient descent, check: <https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html>\n",
    "``` \n",
    "\n",
    "The size of the \"step\" you take is called the **learning rate** (alpha, $\\alpha$). A small learning rate will converge slowly, while a large one might overshoot the minimum. See <https://www.youtube.com/watch?v=gsfbWn4Gy5Q>\n",
    "\n",
    "\n",
    "### What is happening at each step? \n",
    "- Give some initial value to params\n",
    "- For $N$ steps:\n",
    "  + Predict : $\\hat y = \\theta_0 + \\theta_1 x$\n",
    "  + compute error and loss: $\\Delta = \\dfrac{1}{2m} \\sum (\\hat y - y)^2$ \n",
    "  + compute gradients: $\\dfrac{\\partial \\Delta}{\\partial \\theta_0}$, $\\dfrac{\\partial \\Delta}{\\partial \\theta_1}$ (autodiff)\n",
    "  + improve params: $\\theta = \\theta -\\alpha \\nabla_\\theta$ (back-propagation, $\\alpha$ is the learning rate)\n",
    "\n",
    "To do so, we will implement a simple function to do the iterations, and then test it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58fa80-c82a-44ed-aa3a-ea9d0d5bc41b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A basic example: Hooke's Law\n",
    "\n",
    "Hooke's Law is a fundamental principle in physics that states the force (`F`) needed to extend or compress a spring by some distance (`x`) is directly proportional to that distance.\n",
    "\n",
    "$$ F = kx $$\n",
    "\n",
    "This is a perfect linear relationship! We can use linear regression to find the spring constant `k` from experimental data. Let's assume we conducted an experiment and got some noisy measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba452ac-aecb-463f-9b60-b461298d6bd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5079b7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Generate some experimental data\n",
    "import numpy as np\n",
    "\n",
    "N = 20\n",
    "\n",
    "# Let's assume the true spring constant k is 4.5 N/m\n",
    "k_true = 4.5\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# Displacement (x) in meters. This is our feature X.\n",
    "# The .reshape(-1, 1) is needed because scikit-learn expects 2D arrays for features.\n",
    "xdata = np.linspace(0, 2, N)\n",
    "x_displacement = xdata.reshape(-1, 1)\n",
    "\n",
    "# Force (F) in Newtons. This is our target y.\n",
    "# We'll calculate the true force and add some random \"measurement noise\"\n",
    "noise = np.random.normal(0, 0.5, x_displacement.shape)\n",
    "y_force = k_true * x_displacement + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bccef-a8b4-4e9a-a453-a99fa6edd183",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "y_plot = k_true * x_displacement + noise\n",
    "# Setup for inline plotting in a Jupyter Notebook\n",
    "output_notebook()\n",
    "\n",
    "# Create a figure with all properties set at once\n",
    "p = figure(\n",
    "    title=\"Hooke's Law: Force vs. Displacement\",\n",
    "    x_axis_label=\"Displacement (x) [m]\",\n",
    "    y_axis_label=\"Force (F) [N]\",\n",
    "    width=800, height=400  # Define size\n",
    ")\n",
    "\n",
    "# Add the scatter glyph with styling\n",
    "p.scatter(x_displacement[:, 0], y_force[:, 0], legend_label='Experimental Data', color='blue', line_color='black', size=10)\n",
    "\n",
    "# Show the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c01a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This looks like a good candidate for linear regression! The data points roughly follow a straight line.\n",
    "### Linear regression using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b3d16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build and train the model using Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a linear regression model object\n",
    "# Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using our data\n",
    "# The .fit() method is where the 'learning' (Gradient Descent) happens!\n",
    "model.fit(x_displacement, y_force) # USE ALL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0743d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "\n",
    "# Get the learned parameters (theta_0 and theta_1)\n",
    "# .intercept_ is an array, so we take the first element\n",
    "theta_0 = model.intercept_[0]\n",
    "# .coef_ is a 2D array, so we access it with [0][0]\n",
    "theta_1 = model.coef_[0][0]\n",
    "\n",
    "print(f\"The model has learned the following equation:\")\n",
    "print(f\"Force = {theta_0:.3f} + {theta_1:.3f} * Displacement\\n\")\n",
    "\n",
    "print(f\"The estimated spring constant (k) is: {theta_1:.3f} N/m\")\n",
    "print(f\"The true spring constant was: {k_true} N/m\")\n",
    "\n",
    "y_predicted = theta_0 + theta_1*x_displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3c6f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "That's pretty close! Our model successfully estimated the spring constant from the noisy data. The small non-zero intercept `theta_0` is a result of the random noise we added; in a perfect world, it would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f309a-a116-4a50-8bdc-893374ec4370",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "# --- Setup and Plotting ---\n",
    "output_notebook()\n",
    "\n",
    "# Create a figure with all properties set at once\n",
    "p = figure(\n",
    "    title=\"Hooke's Law with Model Fit\",\n",
    "    x_axis_label=\"Displacement (x) [m]\",\n",
    "    y_axis_label=\"Force (F) [N]\",\n",
    "    width=800, height=400\n",
    ")\n",
    "\n",
    "# Plot the original data\n",
    "p.scatter(x_displacement[:, 0], y_force[:, 0], legend_label='Experimental Data', color='blue', line_color='black', size=10)\n",
    "\n",
    "# Plot the regression line\n",
    "p.line(x_displacement[:, 0], y_predicted[:, 0], legend_label='Linear Regression Fit', color='red', line_width=3)\n",
    "\n",
    "# Display the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3d74c-cb50-42f3-be0f-ed08026ca8df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### What is happening at each step? \n",
    "Let's implement a simple function to do the iterations, and then test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a461bc8-3eab-4f2a-b55c-4be46257c4b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def step(theta_0:float, theta_1:float, N:int = 1, alpha: float = 0.1, verbose:bool = False) -> (float, float):\n",
    "    # util function to print\n",
    "    mylog = lambda msg: print(msg) if verbose else None\n",
    "    ytheo = k_true * x_displacement\n",
    "\n",
    "    for ii in np.arange(0, N):\n",
    "        mylog(\"Prediction with full data\")\n",
    "        ypred = theta_0 + theta_1*x_displacement \n",
    "        \n",
    "        mylog(\"Computing loss\")\n",
    "        error = ypred-ytheo\n",
    "        loss = np.power(error, 2).sum()/2\n",
    "        mylog(f\"{loss=}\")\n",
    "        \n",
    "        mylog(\"Computing the gradients\")\n",
    "        grad_0 = error.mean()\n",
    "        grad_1 = (error*x_displacement).mean()\n",
    "        mylog(f\"{grad_0=}, {grad_1=}\")\n",
    "        \n",
    "        mylog(\"Improving paramether estimation\")\n",
    "        # NOTE: learning rate hyper paramemeter alpha\n",
    "        theta_0 = theta_0 - alpha*grad_0\n",
    "        theta_1 = theta_1 - alpha*grad_1\n",
    "        mylog(f\"{theta_0=}, {theta_1=}\")\n",
    "\n",
    "        mylog(\"\")\n",
    "\n",
    "    return theta_0, theta_1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9993c-3329-4ede-823d-9e49e3e4355b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_0 = 1.0\n",
    "theta_1 = 1.0\n",
    "print(step(theta_0, theta_1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f71bb1-6ae2-4a59-bde9-39b540730a5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now with more steps\n",
    "print(step(theta_0, theta_1, N=1000, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38f4d6-035e-45e6-a190-9856cf21eae1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}Plotting the parameters as functions of the learning rate}\n",
    "Let's plot the parameters as functions of the iterations and the learning rate\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be0d5c-49d0-40ee-8c79-cd021ce1ca33",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d84dae06e25c36707aa3789ce252781b",
     "grade": true,
     "grade_id": "cell-4a548389cb352a24",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Params\n",
    "NMAX = 1000\n",
    "alphas = [0.01, 0.1, 0.5, 0.9] # CHANGE THIS\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c68ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Practice Exercises\n",
    "\n",
    "Now it's your turn! Apply what you've learned to new scientific datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9ed79-7ec0-4e8b-af2a-174182058298",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "````{exercise} Tensorflow/pytorch\n",
    ":label: tensorflow\n",
    "Implement the same example but using `tensorflow` and `pytorch`. Compare easy of use.\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906b62e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{exercise} Biology - Brain vs. Body Weight\n",
    "\n",
    "Allometry is the study of the relationship of body size to shape, anatomy, and physiology. It is a well-known fact that the brain weight of mammals generally increases with body weight. Let's model this relationship.\n",
    "\n",
    "**Task:**\n",
    "1.  Load the provided data for various mammal species.\n",
    "2.  The relationship is often modeled on a log-log scale. First, amke a plot to justify that. Then, transform both `body_wt` and `brain_wt` by taking their natural logarithm (`np.log()`).\n",
    "3.  Fit a linear regression model to the log-transformed data.\n",
    "4.  Print the equation of your model.\n",
    "5.  Plot the log-transformed data as a scatter plot and overlay your regression line.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944aea3-5b21-4f3f-9f2f-fe45a9408fbb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4b3447a0a9e6b8bfef164e47b38691",
     "grade": false,
     "grade_id": "cell-d01476741beba7bc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data for Exercise 1\n",
    "body_wt = np.array([3.385, 0.48, 1.35, 465.0, 36.33, 27.66, 1.04, 4.235, 10.55, 0.55, 1.0, 600.0, 3.5, 3.5, 6.8, 35.0, 3.92, 572.0, 180.0, 2.5, 1.92, 119.5, 85.0, 0.75, 14.83, 192.0])\n",
    "brain_wt = np.array([44.5, 15.5, 8.1, 423.0, 119.5, 115.0, 5.5, 25.6, 73.5, 2.4, 6.6, 812.0, 10.8, 12.3, 37.0, 57.0, 17.5, 655.0, 157.0, 12.1, 11.4, 75.0, 62.0, 4.7, 48.0, 180.0])\n",
    "\n",
    "# 1. (Data is already loaded)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c935-62fe-4bb8-b86b-66da1386f90d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Train-test split\n",
    "In order to test a model, it is customary to split the data set into train-test sets. The goal is to use the train data to train the model, and then use the test data, which corresponds to data not seen before, and check for the model performance. An overfitted model (small train error) will have a large variance (large test error), so its predictions will vary greatly when tested on new data. In general, a small train error does not guarantee a small test error.  \n",
    "\n",
    "To split the data into the train and test datasets, we can use the `train_test_split` function (check the manual). Then we train on the train data, and then we compare the predictions on the test data, suing different metrics. This is an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e77b7-442c-4358-8f1e-3d6df685dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- Step 1: Prepare Your Data ---\n",
    "# Let's create some sample data for this example.\n",
    "# Replace these with your actual 'x' and 'y' arrays.\n",
    "np.random.seed(42)\n",
    "# Create a single feature 'x'. It needs to be a 2D array for sklearn.\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "# Create 'y' with a linear relationship to 'x' plus some noise\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "\n",
    "# --- Step 2: Split Data into Training and Testing Sets ---\n",
    "# We'll use 80% of the data for training and 20% for testing.\n",
    "# 'test_size=0.2' specifies the proportion of the data for the test set.\n",
    "# 'random_state' ensures that the split is the same every time you run the code,\n",
    "# making the results reproducible.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of training data (X_train): {X_train.shape}\")\n",
    "print(f\"Shape of testing data (X_test): {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- Step 3: Create and Train the Linear Regression Model ---\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# You can inspect the learned parameters (optional)\n",
    "print(f\"\\nModel Intercept (theta_0): {model.intercept_[0]}\")\n",
    "print(f\"Model Coefficient (theta_1): {model.coef_[0][0]}\")\n",
    "\n",
    "\n",
    "# --- Step 4: Make Predictions on the Test Set ---\n",
    "# Use the trained model to predict the y-values for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# --- Step 5: Compute Metrics to Evaluate the Model ---\n",
    "# Calculate the R-squared (R²) score.\n",
    "# This metric measures how well the model's predictions approximate the real values.\n",
    "# An R² of 1 indicates a perfect fit.\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# You can also calculate other metrics like Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation ---\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460352a0-c7b5-4436-afeb-9e6e6b8c2218",
   "metadata": {},
   "source": [
    ":::{exercise} Splitting data into train and test subsets\n",
    "Use one of the previous examples to actually split the data into train and test sets, using sklearn functions, and then compute metrics like $R^2$.\n",
    "BUT, do not use sklearn LinearRegression, since it uses the exact least square formulation. Use `SGDRegressor`, <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html>, and partial_fit, to get the loss function after each iteration. Finally, plot the loss function as a function of the iteration. Do not forget to standarize the data. gradient descent is sensible to that. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64e72f-7988-4743-ab63-c71a9a976358",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df33e5993bc1ece3295db928917742b8",
     "grade": false,
     "grade_id": "cell-e5ee5dfb05a55068",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce5285",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Conclusion & What's Next?\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Supervised learning uses **labeled data** (X, y) to learn a predictive function.\n",
    "- **Regression** predicts continuous values, while **Classification** predicts discrete categories.\n",
    "- **Linear Regression** finds the best-fit line by minimizing a **cost function** (like MSE).\n",
    "- **Gradient Descent** is the optimization algorithm used to find the model parameters that minimize the cost.\n",
    "- Libraries like **Scikit-Learn** make it incredibly easy to implement these powerful models.\n",
    "\n",
    "**What's Next?**\n",
    "- What if our data isn't linear? We can use **Polynomial Regression**.\n",
    "- How do we handle classification problems? We'll use models like **Logistic Regression** and **Support Vector Machines**.\n",
    "- What happens when we have many features? We need to be careful about **overfitting** and use techniques like **regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30dedd-054d-45e0-8ea2-2ad72acb599b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Regularization\n",
    "The bias-variance trade-off shows that is important to not go to the extremes when fitting a model. For example, when the model performs well on the training data but does not work well on the test data, we might need to add a `regularization` so penalize the cost function to improve the trade-off. One example is the so-called ridge regularization, where \n",
    "\\begin{equation}\n",
    "\\Delta(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat y(x_i) - y_i \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2,\n",
    "\\end{equation}\n",
    "where $\\lambda$ is the hyper-regularization parameter. This controls the magnitude of the coefficients preventing overfitting. When $\\lambda$ is large, the parameters $\\theta$ decrease, which shrinks the impact of variables no so correlated with the output. When $\\lambda$ decreases, we basically converge to the usual linear regression.  \n",
    "\n",
    "There are several other regularization techniques, such as lasso regression (L1),\n",
    "\\begin{equation}\n",
    "\\Delta(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x_i) - y_i \\right)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|, \n",
    "\\end{equation}\n",
    "Elastic Net regression, where both previous regressions are combines, and so on.\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compare regularization methods\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print(f\"{name} CV R²: {scores.mean():.3f} (±{scores.std()*2:.3f})\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e995c4-11ab-4678-9fe0-9fd7a3d9bef6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercises\n",
    ":::{exercise} Ridge, lasso and elastic net\n",
    "Implement ridge regularization into our step by step approach. Check the role of several values. Now do the same for lasso, and then for Elastic Net.\n",
    ":::\n",
    "\n",
    ":::{exercise} Ransac regression with outliers\n",
    "Look for ransac regression in sklearn and implement an example showing how ransac can ignore outliers in data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ab8f7-c47b-405b-a9be-5557db955058",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "REF: <https://www.digitalocean.com/community/tutorials/multiple-linear-regression-python>\n",
    "\n",
    "Until now, we explore a single variable linear regression. But, in general, we have multiple variables problems. In this case, we want to study a model like\n",
    "\\begin{equation}\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon = \\vec \\beta \\cdot \\vec V + \\epsilon,\n",
    "\\end{equation}\n",
    "where $\\vec \\beta$ are the coefficients we want to optimize, and we have $n$ \"independent\" variables. \n",
    "\n",
    "There are several assumptions here to be able to apply linear regression, and there are some tests that you can use to check for them. \n",
    "\n",
    "In this section we will still use the traditional Linear Regression model from scikit learn. The goal is to learn some metrics and techniques that could help understandad better the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891ea6a-9011-4f86-8e91-c1b9a886a7ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Assumptions of Multiple Linear Regression\n",
    "\n",
    "| Assumption | Description | Test(s) to Check | Python Implementation (`pandas` + `seaborn`) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. Linearity** | The relationship between predictors (X) and the outcome (y) is linear. | **Scatter Plots** or **Fitted vs. Residuals Plot**. Look for a random scatter. Remember that you can tranform the data | `import seaborn as sns`<br>`# model is a fitted OLS model`<br>`residuals = model.resid`<br>`fitted = model.fittedvalues`<br>`sns.residplot(x=fitted, y=residuals, lowess=True)` |\n",
    "| **2. Independence of Residuals** | The residuals (errors) are independent of each other (no autocorrelation). | **Durbin-Watson Test**. Look for a value around 2. | `from statsmodels.stats.stattools import durbin_watson`<br>`dw_stat = durbin_watson(model.resid)`<br>`print(f\"Durbin-Watson: {dw_stat:.2f}\")` |\n",
    "| **3. Homoscedasticity** | Residuals have constant variance across all levels of X. | **Breusch-Pagan Test** or **White Test**. Look for a p-value > 0.05. Visual check with **Residuals vs. Fitted Plot**. | `import statsmodels.stats.api as sms`<br>`bp_test = sms.het_breuschpagan(model.resid, model.model.exog)`<br>`print(f\"Breusch-Pagan p-value: {bp_test[1]:.4f}\")` |\n",
    "| **4. Normality of Residuals** | The residuals are approximately normally distributed. | **Jarque-Bera Test** or **Q-Q Plot**. Points on the Q-Q plot should follow the line. | `import statsmodels.api as sm`<br>`sm.qqplot(model.resid, line='s')`<br>`# Jarque-Bera result is in model.summary()` |\n",
    "| **5. No Perfect Multicollinearity** | Independent variables are not highly correlated with each other. | **A) Correlation Matrix (Preliminary)**<br><br>**B) Variance Inflation Factor (VIF) (Definitive)** | `## A) Correlation Matrix`<br>`import seaborn as sns`<br>`import matplotlib.pyplot as plt`<br>`# df is your full DataFrame (X's and y)`<br>`corr_matrix = df.corr()`<br>`sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')`<br>`plt.show()`<br><br>`## B) VIF`<br>`from statsmodels.stats.outliers_influence import variance_inflation_factor`<br>`# X is a DataFrame of predictors only`<br>`vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]` |\n",
    "\n",
    "It is also useful to check for the correlation between the dependent variable $Y$ and the independent variables $\\vec X$. Large positive or negative correlations allow to select the most important variables for applying linear regression. \n",
    "\n",
    "The following code shows an example of this:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85a436-0562-4c72-9a94-0c71b69520bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample data representing a \"good\" scenario\n",
    "# X1 and X2 are good predictors of y\n",
    "# X3 is a weak predictor\n",
    "# X1 and X2 have low correlation with each other\n",
    "np.random.seed(42)\n",
    "X1 = np.random.rand(100) * 10\n",
    "X2 = np.random.rand(100) * 5\n",
    "X3 = np.random.rand(100) * 2 # Weak predictor\n",
    "noise = np.random.normal(0, 3, 100)\n",
    "\n",
    "y = 2 + 3 * X1 - 4 * X2 + 0.5 * X3 + noise # X3 has a small effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bea67-649e-44a0-9d5d-f604fd2aa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important line of code in any analysis\n",
    "plt.scatter(X1, y)\n",
    "plt.scatter(X2, y)\n",
    "plt.scatter(X3, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2d892-020e-4615-93fb-e0a005541374",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'y': y})\n",
    "\n",
    "# --- Generate the Correlation Matrix ---\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# --- Visualize it with a Heatmap ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Full Correlation Matrix (for y and X)')\n",
    "plt.show()\n",
    "\n",
    "# --- Explicitly Analyze the Correlations ---\n",
    "print(\"--- 1. Correlation with Dependent Variable (y) ---\")\n",
    "print(\"We want these values to be high (far from zero).\")\n",
    "print(corr_matrix['y'].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n--- 2. Correlation Among Independent Variables (X) ---\")\n",
    "print(\"We want the off-diagonal values here to be low (close to zero).\")\n",
    "print(df[['X1', 'X2', 'X3']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d48cb-5976-4e07-b6d8-71c1666183f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Full example Workflow in Python\n",
    "\n",
    "Here is a quick summary of how you would typically check these assumptions after fitting a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a062095-04ef-4756-8555-1c1ef6661407",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# 1. Prepare your data (assuming you have a pandas DataFrame `df`)\n",
    "# Let's create some sample data for demonstration\n",
    "np.random.seed(42)\n",
    "X1 = np.random.rand(100) * 10\n",
    "X2 = 0.5 * X1 + np.random.normal(0, 1, 100) # X2 is somewhat correlated with X1\n",
    "y = 2 + 3 * X1 + 5 * X2 + np.random.normal(0, 5, 100)\n",
    "X = pd.DataFrame({'X1': X1, 'X2': X2})\n",
    "X = sm.add_constant(X) # Add a constant for the intercept\n",
    "\n",
    "# 2. Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd1370-a771-47df-a67d-fec50880bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = model.pvalues[model.pvalues < 0.05].index\n",
    "print(significant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21ab5e-c6da-4cf8-8252-33e5faf60723",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 3. Check Assumptions\n",
    "\n",
    "# --- Linearity & Homoscedasticity (Visual Check) ---\n",
    "print(\"Checking for Linearity and Homoscedasticity...\")\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "sns.residplot(x=fitted, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 2})\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# --- Homoscedasticity (Statistical Test) ---\n",
    "print(\"Checking for Homoscedasticity (Breusch-Pagan Test)...\")\n",
    "bp_test = sms.het_breuschpagan(model.resid, model.model.exog)\n",
    "print(f\"Breusch-Pagan Test p-value: {bp_test[1]:.4f}\")\n",
    "if bp_test[1] > 0.05:\n",
    "    print(\"Result: No evidence of heteroscedasticity (Good).\")\n",
    "else:\n",
    "    print(\"Result: Evidence of heteroscedasticity found (Bad).\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Independence of Residuals ---\n",
    "print(\"Checking for Independence of Residuals (Durbin-Watson Test)...\")\n",
    "dw_stat = durbin_watson(model.resid)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.2f}\")\n",
    "if 1.5 < dw_stat < 2.5:\n",
    "    print(\"Result: No significant autocorrelation (Good).\")\n",
    "else:\n",
    "    print(\"Result: Potential autocorrelation detected (Bad).\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Normality of Residuals ---\n",
    "print(\"Checking for Normality of Residuals (Q-Q Plot and Jarque-Bera)...\")\n",
    "sm.qqplot(model.resid, line='45')\n",
    "plt.title(\"Q-Q Plot of Residuals\")\n",
    "plt.show()\n",
    "# The Jarque-Bera test result is in the model summary `Prob(JB)`\n",
    "jb_prob = float(model.summary2().tables[2].iloc[0, 3])\n",
    "print(f\"Jarque-Bera test probability: {jb_prob}\")\n",
    "if jb_prob > 0.05:\n",
    "     print(\"Result: Residuals appear to be normally distributed (Good).\")\n",
    "else:\n",
    "     print(\"Result: Residuals may not be normally distributed (Bad).\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Multicollinearity ---\n",
    "print(\"Checking for Multicollinearity (VIF)...\")\n",
    "# Note: We check VIF on the design matrix X without the constant\n",
    "X_no_const = X.drop('const', axis=1)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_no_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_no_const.values, i) for i in range(len(X_no_const.columns))]\n",
    "print(vif_data)\n",
    "if all(vif_data[\"VIF\"] < 5):\n",
    "    print(\"\\nResult: No significant multicollinearity detected (Good).\")\n",
    "else:\n",
    "    print(\"\\nResult: Potential multicollinearity detected (Bad).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb011b-50c8-4b5a-a8fc-4f7016704673",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Data pre-processing and post-processing tips\n",
    "What happens if there is missing data? or the independent variables are of very different magnitudes? this could affect the actual analysis, so it is better to perform a data cleaning or pre-processing stage. \n",
    "\n",
    "For example, for missing data detection, you can use\n",
    "```python\n",
    "print(df.isnull().sum())\n",
    "```\n",
    "If you find any missing data, you have to carefully analyze what to do. \n",
    "\n",
    "You should also use the correlation matrix to select the more relevant variables to use in the model.\n",
    "\n",
    "Additionally, it is useful to standardize data so their mean become 0 and its variance 1. This ensures that no variable dominates the model. To do so, you can use a scaller like\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print the scaled data\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "After this, you can apply some statistical test to check for the multiple linear assumptions. \n",
    "\n",
    "Finally, you can also split your data into training and testing subsets, to be able to check for model prediction, by using the `train_test_split` function:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# The 'LinearRegression' model is initialized and fitted to the training data.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# The model is used to predict the target variable for the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared:\", r2_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Cross-validation\n",
    "After the initial results and tests, you can use cross-validation to check for your model performance with unseen data, by splitting your data in k groups ad computing R2 as\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean CV R^2:\", scores.mean())\n",
    "\n",
    "# Line Plot for Cross-Validation Scores\n",
    "plt.plot(range(1, 6), scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('R-squared')\n",
    "plt.title('Cross-Validation R-squared Scores')\n",
    "plt.show()\n",
    "```\n",
    "### Feature selection\n",
    "It is possible to recursively try to eliminate features until some k-features are selected. This is called recursive features elimination:\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=3)\n",
    "rfe.fit(X_scaled, y)\n",
    "print(\"Selected Features:\", rfe.support_)\n",
    "\n",
    "# Bar Plot of Feature Rankings\n",
    "feature_ranking = pd.DataFrame({\n",
    "   'Feature': selected_features,\n",
    "   'Ranking': rfe.ranking_\n",
    "})\n",
    "feature_ranking.sort_values(by='Ranking').plot(kind='bar', x='Feature', y='Ranking', legend=False)\n",
    "plt.title('Feature Ranking (Lower is Better)')\n",
    "plt.ylabel('Ranking')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e5a70-9fed-4e7c-916b-a8c9fa2bded6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercises\n",
    "\n",
    "For the following exercises, perform a full analysis with explicit statistical tests computation and interpretation. Explain why you use some of the features, or why you need to use all of them. Plot correlation matrices and so on. Also perform a previous pre-processing stage.   \n",
    "\n",
    "```{exercise} A large number of features model\n",
    "Use [sklearn.datasets.make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression) to generate a 100 features model but 10 relevant features. \n",
    "```\n",
    "\n",
    ":::{exercise} Star dataset\n",
    "Use the star dataset to try to predict luminosity: <https://www.kaggle.com/datasets/waqi786/stars-dataset>. \n",
    ":::\n",
    "\n",
    ":::{exercise} Polynomial regression\n",
    "Generate a random set following the model\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\epsilon\n",
    "\\end{equation}\n",
    "and apply (polynomial) linear regression to get the coefficients.  Check <https://www.geeksforgeeks.org/machine-learning/python-implementation-of-polynomial-regression/>\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f9439-f033-49af-9062-29e920615eda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
