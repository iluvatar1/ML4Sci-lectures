{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "What is Principal Component Analysis?\n",
    "- Many scientific datasets have many correlated measurements: spectra (many wavelengths), climate grids (many locations), images (many pixels), recordings from many sensors or neurons.\n",
    "- PCA finds orthogonal directions (principal components) that explain the most variance, which helps visualization, noise reduction, compression, and discovering latent degrees of freedom.\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while preserving as much variance as possible. Think of it as finding the best camera angle to capture the most information about a 3D object in a 2D photograph.\n",
    "\n",
    "Key Concepts:\n",
    "- Dimensionality Reduction: Transform high-dimensional data to lower dimensions\n",
    "- Variance Preservation: Keep the most important patterns in the data\n",
    "- Data Visualization: Make complex datasets interpretable\n",
    "- Noise Reduction: Filter out less important variations\n",
    "\n",
    "| Field | Application | Benefits |\n",
    "|-------|-------------|----------|\n",
    "| **Spectroscopy** | Identify key wavelengths in complex spectra | Reduce noise, find characteristic peaks |\n",
    "| **Genomics** | Find patterns in gene expression data | Identify gene clusters, reduce computational complexity |\n",
    "| **Climate Science** | Understand weather patterns from multiple variables | Visualize climate patterns, identify trends |\n",
    "| **Chemistry** | Analyze molecular properties and reactions | Optimize reaction conditions, understand structure-property relationships |\n",
    "| **Physics** | Process sensor data from experiments | Extract signals from noise, identify fundamental modes |\n",
    "\n",
    "\n",
    ":::{exercise} Understanding Dimensionality\n",
    "\n",
    "**Scenario**: A researcher has measurements of temperature, humidity, pressure, and wind speed from 1000 weather stations. They want to create a 2D map showing weather patterns.\n",
    "\n",
    "**Question**: How could PCA help in this situation?\n",
    "\n",
    "**Tasks**:\n",
    "1. Identify the original dimensionality of the data\n",
    "2. Explain what the principal components might represent\n",
    "3. Discuss potential limitations of reducing to 2D\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visual demo: 2D correlated data and PC1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.normal(size=n)\n",
    "y = 2.0 * x + 0.8 * np.random.normal(size=n)\n",
    "X = np.vstack([x,y]).T\n",
    "Xc = X - X.mean(axis=0)\n",
    "\n",
    "C = np.cov(Xc, rowvar=False)\n",
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "order = eigvals.argsort()[::-1]\n",
    "eigvals = eigvals[order]\n",
    "eigvecs = eigvecs[:, order]\n",
    "\n",
    "pc1 = eigvecs[:,0]\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Xc[:,0], Xc[:,1], s=15)\n",
    "origin = np.zeros(2)\n",
    "for s in [-3,3]:\n",
    "    p = s * np.sqrt(eigvals[0]) * pc1\n",
    "    plt.plot([origin[0], p[0]], [origin[1], p[1]], linewidth=3)\n",
    "plt.xlabel('x (centered)')\n",
    "plt.ylabel('y (centered)')\n",
    "plt.title('2D correlated data and first principal component')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### Step 1: Data Centering\n",
    "\\begin{equation}\n",
    "\\hat X = X - \\mu\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mu$ is the mean of each variable.\n",
    "\n",
    "**Why center the data?**\n",
    "- Ensures PCA finds directions of maximum variance from the center\n",
    "- Prevents variables with larger scales from dominating\n",
    "\n",
    "### Step 2: Covariance Matrix\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{n-1}\\  \\hat X^T \\times \\hat X.\n",
    "\\end{equation}\n",
    "\n",
    "Where $n$ is the number of observations.\n",
    "\n",
    "**The covariance matrix captures**:\n",
    "- How much each variable varies (diagonal elements)\n",
    "- How variables co-vary together (off-diagonal elements)\n",
    "\n",
    "### Step 3: Eigendecomposition\n",
    "\\begin{equation}\n",
    "C\\times \\vec v = \\lambda \\vec v\n",
    "\\end{equation}\n",
    "Where:\n",
    "- $\\vec v$ are eigenvectors (principal component directions)\n",
    "- $\\lambda$ are eigenvalues (variance captured by each component)\n",
    "\n",
    "### Step 4: Variance Explained\n",
    "\n",
    "Variance Explained by \n",
    "\\begin{equation}\n",
    "PC_i = \\frac{\\lambda_i}{\\sum_j\\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    ":::{exercise}Covariance Understanding\n",
    "**Question**: If two variables have a covariance of 0, what does this mean for PCA?\n",
    "\n",
    "Consider this covariance matrix:\n",
    "```\n",
    "C = [4.0  0.0]\n",
    "    [0.0  1.0]\n",
    "```\n",
    "\n",
    "**Tasks**:\n",
    "1. What are the eigenvalues?\n",
    "2. What are the eigenvectors?\n",
    "3. How much variance does each PC explain?\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step-by-Step PCA Process\n",
    "\n",
    "### Example: Analyzing Plant Growth Data\n",
    "\n",
    "Suppose we measure 4 variables for 100 plants:\n",
    "- Height (cm)\n",
    "- Leaf area (cm²)\n",
    "- Root depth (cm)\n",
    "- Biomass (g)\n",
    "\n",
    "```python\n",
    "# Step 1: Organize data matrix X (100 × 4)\n",
    "# Rows = observations (plants)\n",
    "# Columns = variables (measurements)\n",
    "\n",
    "# Step 2: Standardize data (often necessary)\n",
    "X_scaled = (X - mean(X)) / std(X)\n",
    "\n",
    "# Step 3: Compute covariance matrix\n",
    "C = cov(X_scaled)\n",
    "\n",
    "# Step 4: Find eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = eig(C)\n",
    "\n",
    "# Step 5: Sort by eigenvalues (descending)\n",
    "# Step 6: Choose number of components\n",
    "# Step 7: Transform data\n",
    "PC_scores = X_scaled @ eigenvectors\n",
    "```\n",
    "\n",
    ":::{exercise} Plant Growth Interpretation**\n",
    "\n",
    "Given the following PCA results for plant data:\n",
    "\n",
    "**PC1 Loadings**: Height (0.6), Leaf Area (0.5), Root Depth (0.4), Biomass (0.5)\n",
    "**PC2 Loadings**: Height (-0.3), Leaf Area (0.2), Root Depth (0.8), Biomass (-0.5)\n",
    "\n",
    "**Questions**:\n",
    "1. What does PC1 represent biologically?\n",
    "2. What does PC2 represent?\n",
    "3. Which variables contribute most to each PC?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Animated PCA visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.normal(size=n)\n",
    "y = 2.0 * x + 0.8 * np.random.normal(size=n)\n",
    "X = np.vstack([x,y]).T\n",
    "Xc = X - X.mean(axis=0)\n",
    "\n",
    "C = np.cov(Xc, rowvar=False)\n",
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "order = eigvals.argsort()[::-1]\n",
    "eigvals = eigvals[order]\n",
    "eigvecs = eigvecs[:, order]\n",
    "V = eigvecs\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "sc1 = ax[0].scatter(Xc[:,0], Xc[:,1], s=15)\n",
    "X_pca = Xc.dot(V)\n",
    "sc2 = ax[1].scatter(X_pca[:,0], X_pca[:,1], s=15)\n",
    "ax[0].set_title('Original space')\n",
    "ax[1].set_title('PCA space')\n",
    "for a in ax:\n",
    "    a.set_xlim(-6,6)\n",
    "    a.set_ylim(-6,6)\n",
    "    a.grid(True)\n",
    "\n",
    "line_pc1, = ax[0].plot([], [], 'r-', lw=2, label='PC1')\n",
    "line_pc2, = ax[0].plot([], [], 'g-', lw=2, label='PC2')\n",
    "ax[0].legend()\n",
    "\n",
    "def init():\n",
    "    line_pc1.set_data([], [])\n",
    "    line_pc2.set_data([], [])\n",
    "    return sc1, sc2, line_pc1, line_pc2\n",
    "\n",
    "def update(frame):\n",
    "    theta = frame\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta),  np.cos(theta)]])\n",
    "    X_rot = Xc.dot(R.T)\n",
    "    sc1.set_offsets(X_rot)\n",
    "    line_pc1.set_data([0, R[0,0]*3], [0, R[1,0]*3])\n",
    "    line_pc2.set_data([0, R[0,1]*3], [0, R[1,1]*3])\n",
    "    return sc1, sc2, line_pc1, line_pc2\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, np.pi/2, 30),\n",
    "                    init_func=init, blit=True, interval=100)\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interactive 3D PCA viewer using plotly and ipywidgets\n",
    "# If plotly or ipywidgets are not installed, this will instruct the user.\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception as e:\n",
    "    print('Missing required packages for interactive viewer. Please install: plotly, ipywidgets.')\n",
    "    print('Error:', e)\n",
    "    raise\n",
    "\n",
    "# Load data and compute PCA\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "target_names = data.target_names\n",
    "pca = PCA(n_components=X.shape[1])\n",
    "scores = pca.fit_transform(X)\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "import pandas as pd\n",
    "df_scores = pd.DataFrame(scores, columns=[f'PC{i+1}' for i in range(scores.shape[1])])\n",
    "df_scores['target'] = [target_names[i] for i in y]\n",
    "\n",
    "def make_3d_plot(pc_x=1, pc_y=2, pc_z=3):\n",
    "    fig = px.scatter_3d(df_scores, x=f'PC{pc_x}', y=f'PC{pc_y}', z=f'PC{pc_z}',\n",
    "                        color='target', symbol='target', title=f'PC{pc_x} vs PC{pc_y} vs PC{pc_z} (explained var: {explained[pc_x-1]:.2f}, {explained[pc_y-1]:.2f}, {explained[pc_z-1]:.2f})')\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.update_layout(width=800, height=600)\n",
    "    return fig\n",
    "\n",
    "pc_options = [1,2,3,4]\n",
    "pc_x = widgets.Dropdown(options=pc_options, value=1, description='PC X:')\n",
    "pc_y = widgets.Dropdown(options=pc_options, value=2, description='PC Y:')\n",
    "pc_z = widgets.Dropdown(options=pc_options, value=3, description='PC Z:')\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def update_plot(change=None):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        fig = make_3d_plot(pc_x.value, pc_y.value, pc_z.value)\n",
    "        #display(fig)\n",
    "        fig.show()\n",
    "\n",
    "pc_x.observe(update_plot, names='value')\n",
    "pc_y.observe(update_plot, names='value')\n",
    "pc_z.observe(update_plot, names='value')\n",
    "\n",
    "controls = widgets.HBox([pc_x, pc_y, pc_z])\n",
    "#display(controls)\n",
    "display(controls, out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Interpretation Guidelines\n",
    "\n",
    "### Choosing Number of Components\n",
    "\n",
    "**Method 1: Cumulative Variance**\n",
    "- Keep components explaining 80-95% of variance\n",
    "- Depends on application requirements\n",
    "\n",
    "**Method 2: Scree Plot (Elbow Method)**\n",
    "- Plot eigenvalues vs. component number\n",
    "- Look for \"elbow\" where decrease slows\n",
    "\n",
    "**Method 3: Kaiser Criterion**\n",
    "- Keep components with eigenvalues > 1 (if data is standardized)\n",
    "- Only applies when variables are standardized\n",
    "\n",
    "### Loading Interpretation\n",
    "\n",
    "**Loading Values**:\n",
    "- **|loading| > 0.7**: Strong relationship\n",
    "- **0.3 < |loading| < 0.7**: Moderate relationship\n",
    "- **|loading| < 0.3**: Weak relationship\n",
    "\n",
    "**Signs Matter**:\n",
    "- **Positive loading**: Variable increases with PC\n",
    "- **Negative loading**: Variable decreases with PC\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "| **Mistake** | **Correct** |\n",
    "|---|---|\n",
    "|Treating PCs as original variables|PCs are linear combinations of original variables|\n",
    "|Ignoring scaling/standardization|Consider whether to standardize based on variable units|\n",
    "|Over-interpreting small components|Focus on components explaining meaningful variance|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A detailed Worked Example - The Iris Dataset\n",
    "\n",
    "The Iris dataset is a classic in machine learning and statistics. It contains 150 samples from three species of Iris flowers (Setosa, Versicolor, and Virginica). For each sample, four features were measured: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "Our goal is to see if we can use PCA to 'summarize' these four measurements and visualize the separation between the species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Setup - Importing Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data manipulation, numerical computation, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Set some default plotting styles for better looking visuals\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data # The feature matrix\n",
    "y = iris.target # The labels (species)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Let's look at the first 5 rows of the data\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"\\nFirst 5 rows of data:\\n\", X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 1: Standardize the Data\n",
    "\n",
    "PCA is sensitive to the scale of the features. We need to standardize our data so that each feature has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(\"First 5 rows of scaled data:\\n\", X_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 2: Perform PCA\n",
    "\n",
    "Now we apply PCA. We'll ask `scikit-learn` to find the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize PCA and fit the scaled data\n",
    "# n_components specifies how many dimensions we want to reduce to.\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the model and transform the data to the new coordinate system\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3: Analyze and Visualize the Results\n",
    "\n",
    "#### Scree Plot\n",
    "\n",
    "First, let's see how much variance our two new components capture. A scree plot is perfect for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(f\"Variance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "print(f\"Total variance explained by first two components: {np.sum(explained_variance):.2%}\")\n",
    "\n",
    "# To make a full scree plot, we can re-run PCA without specifying n_components\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1), pca_full.explained_variance_ratio_ * 100, alpha=0.7, align='center', label='Individual explained variance')\n",
    "plt.step(range(1, len(pca_full.explained_variance_ratio_) + 1), np.cumsum(pca_full.explained_variance_ratio_) * 100, where='mid', label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Percentage')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Scree Plot for Iris Dataset')\n",
    "plt.xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Observation:** The first two components capture over 95% of the total variance in the data! This means our 2D plot will be a very good representation of the original 4D data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores Plot\n",
    "\n",
    "Next, we create a scores plot. This is a scatter plot of our samples in the new PCA space. We will color each point according to its true species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA Scores Plot of Iris Dataset')\n",
    "plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Observation:** The three species are very well separated in the PCA plot. The *Setosa* species is a distinct cluster, while *Versicolor* and *Virginica* are also mostly separated, though they have some overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biplot\n",
    "\n",
    "Finally, the biplot helps us understand *why* the samples are separated. It overlays the original feature vectors (loadings) on top of the scores plot. This tells us how the original variables contribute to the principal components.\n",
    "\n",
    "We will use a helper function to create a clean biplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def biplot(score, coeff, labels=None):\n",
    "    \"\"\"\n",
    "    Creates a biplot visualization.\n",
    "    \n",
    "    score: The transformed data (scores), e.g., X_pca.\n",
    "    coeff: The eigenvectors (loadings), e.g., pca.components_.T.\n",
    "    labels: The names of the original features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    xs = score[:, 0]\n",
    "    ys = score[:, 1]\n",
    "    n = coeff.shape[0]\n",
    "    \n",
    "    # Plot the scores\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "        plt.scatter(xs[y == i], ys[y == i], color=color, alpha=0.7, label=target_name)\n",
    "\n",
    "    # Plot the loadings\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i, 0]*4, coeff[i, 1]*4, color='r', alpha=0.9, head_width=0.05)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i, 0] * 4.2, coeff[i, 1] * 4.2, \"Var\" + str(i + 1), color='black', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i, 0] * 4.2, coeff[i, 1] * 4.2, labels[i], color='black', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})')\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})')\n",
    "    plt.title('Biplot of Iris Dataset')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "# Call the function with our data\n",
    "# Note: we need to transpose pca.components_ to get the loadings in the right shape\n",
    "biplot(X_pca, np.transpose(pca.components_), labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Interpretation of the Biplot:**\n",
    "\n",
    "*   **PC1 (the horizontal axis):** All four variables have vectors pointing to the right, but `petal length`, `petal width`, and `sepal length` point most strongly in this direction. This suggests PC1 is a measure of **overall flower size**. Larger flowers (like Virginica) are on the right (high PC1 score), and smaller flowers (like Setosa) are on the left (low PC1 score).\n",
    "*   **PC2 (the vertical axis):** This axis shows an interesting contrast. `sepal width` points up, while `petal width` and `petal length` point down. This component separates flowers with wide sepals relative to their petal size from those with the opposite characteristics. This explains the separation between Versicolor and Virginica, which have similar overall sizes (PC1) but different shapes (PC2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Details\n",
    "\n",
    "### PCA Assumptions and Limitations\n",
    "\n",
    "**Assumptions**:\n",
    "- Linear relationships between variables\n",
    "- Data follows (approximately) multivariate normal distribution\n",
    "- Variables are continuous\n",
    "\n",
    "**Limitations**:\n",
    "- **Linear combinations only**: Cannot capture nonlinear patterns\n",
    "- **Variance-based**: May not preserve class separability\n",
    "- **Global method**: Same transformation for all data points\n",
    "\n",
    "**When PCA May Not Work Well**:\n",
    "- Highly nonlinear data (consider kernel PCA)\n",
    "- Categorical variables (consider correspondence analysis)\n",
    "- When rare events are important (PCA focuses on major patterns)\n",
    "\n",
    "### Robust PCA\n",
    "\n",
    "**Problem**: Standard PCA is sensitive to outliers.\n",
    "\n",
    "**Solutions**:\n",
    "- **Robust PCA**: Use median instead of mean, robust covariance estimation\n",
    "- **Sparse PCA**: Assume many loadings are exactly zero\n",
    "- **Kernel PCA**: Handle nonlinear relationships\n",
    "\n",
    "### PCA vs. Other Techniques\n",
    "\n",
    "| Method | Best For | Limitation |\n",
    "|--------|----------|------------|\n",
    "| **PCA** | Continuous data, linear relationships | Linear only |\n",
    "| **Factor Analysis** | Identifying latent variables | Assumes specific model |\n",
    "| **ICA** | Separating mixed signals | Assumes independence |\n",
    "| **t-SNE** | Visualization, nonlinear | Not for dimensionality reduction |\n",
    "| **UMAP** | Large datasets, preserving structure | Complex parameter tuning |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Exercises\n",
    "\n",
    "Now it's your turn! Apply the techniques learned above to the following datasets. For each exercise, you'll need to:\n",
    "1. Load the data using `pandas`.\n",
    "2. Select the feature columns.\n",
    "3. Standardize the features.\n",
    "4. Perform PCA.\n",
    "5. Create and interpret a scores plot and/or a biplot.\n",
    "\n",
    "*(Note: You will need to have the `.csv` files in the same directory as this notebook for the code to work.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genomics - Cancer Subtype Identification\n",
    "**Dataset:** `cancer_data.csv`\n",
    "**Task:** Perform PCA on the gene expression data. Can you identify distinct clusters in a scores plot? What might they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cancer_df = pd.read_csv('cancer_data.csv')\n",
    "    print(\"Cancer data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select all columns except 'Sample_ID' and 'Subtype' as your features (X)\n",
    "    # X_cancer = ...\n",
    "    \n",
    "    # 2. Standardize X_cancer\n",
    "    # X_cancer_scaled = ...\n",
    "    \n",
    "    # 3. Perform PCA (n_components=2)\n",
    "    # pca_cancer = ...\n",
    "    # X_cancer_pca = ...\n",
    "    \n",
    "    # 4. Create a scores plot. You can color by the 'Subtype' column to see if the clusters match.\n",
    "    # plt.figure(...)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"File 'cancer_data.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemistry - Classifying Olive Oils\n",
    "**Dataset:** `olive_oil_spectra.csv`\n",
    "**Task:** Use PCA on the spectral data. Can you distinguish oils from different regions? Which wavelengths (columns) are most important for this separation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    oil_df = pd.read_csv('olive_oil_spectra.csv')\n",
    "    print(\"Olive oil data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the wavelength columns as features.\n",
    "    # X_oil = ...\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "\n",
    "    # 3. Create a biplot. It might be too cluttered to label all the variables (wavelengths),\n",
    "    #    so focus on the scores plot part and the general direction of the loadings cloud.\n",
    "    #    Color the points by the 'Region' column.\n",
    "    # ...\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'olive_oil_spectra.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Environmental Science - Air Pollution Sources\n",
    "**Dataset:** `air_pollution.csv`\n",
    "**Task:** Use PCA to identify patterns in air quality data. Create a biplot and interpret the loadings. Can you hypothesize what physical processes PC1 and PC2 represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    air_df = pd.read_csv('air_pollution.csv')\n",
    "    print(\"Air pollution data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the pollutant and meteorological columns as features.\n",
    "    # X_air = ...\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "\n",
    "    # 3. Create a biplot and interpret the loadings.\n",
    "    #    Look at how variables like Ozone, NOx, and Temperature are related in the PCA space.\n",
    "    # ...\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'air_pollution.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agriculture - Crop Yield Analysis\n",
    "**Dataset:** `crop_data.csv`\n",
    "**Task:** Perform PCA on the input variables (everything except `Crop_Yield`). Then, plot the PC1 scores against `Crop_Yield`. Is there a relationship? This demonstrates using PCA for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    crop_df = pd.read_csv('crop_data.csv')\n",
    "    print(\"Crop data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the input variables as features.\n",
    "    # input_vars = ['Rainfall', 'Sunlight_Hours', 'Fertilizer_Amount', 'Soil_pH']\n",
    "    # X_crop = crop_df[input_vars]\n",
    "    # y_crop = crop_df['Crop_Yield']\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "    # X_crop_pca = ...\n",
    "\n",
    "    # 3. Create a scatter plot of the first principal component vs. Crop_Yield.\n",
    "    # plt.figure(...)\n",
    "    # plt.scatter(X_crop_pca[:, 0], y_crop)\n",
    "    # plt.xlabel('Principal Component 1')\n",
    "    # plt.ylabel('Crop Yield')\n",
    "    # plt.title('PC1 vs. Crop Yield')\n",
    "    # plt.show()\n",
    "    # What does the relationship (or lack thereof) tell you?\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'crop_data.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pharmaceutical Analysis\n",
    "**Scenario**: A pharmaceutical company measured 50 chemical properties of 200 drug candidates to predict effectiveness.\n",
    "\n",
    "**Data Structure**:\n",
    "- Rows: 200 drug candidates\n",
    "- Columns: 50 chemical properties (molecular weight, logP, polar surface area, etc.)\n",
    "\n",
    "**Tasks**:\n",
    "1. **Preprocessing**: Should you standardize the variables? Why?\n",
    "2. **Analysis**: Apply PCA and determine how many components to retain\n",
    "3. **Interpretation**: If PC1 loads heavily on molecular weight, logP, and size-related properties, what does this PC represent?\n",
    "4. **Application**: How would you use PCA scores to select promising drug candidates?\n",
    "\n",
    "**Expected Results**:\n",
    "- PC1 (30%): \"Molecular size\" - larger, more lipophilic molecules\n",
    "- PC2 (20%): \"Polarity\" - hydrophilic vs. hydrophobic character\n",
    "- PC3 (15%): \"Complexity\" - structural complexity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Structure Analysis\n",
    "**Scenario**: You have 3D coordinates for all atoms in a protein from molecular dynamics simulations (1000 time points).\n",
    "\n",
    "**Challenge**: Identify main modes of protein flexibility.\n",
    "\n",
    "**Tasks**:\n",
    "1. **Data Setup**: How would you arrange the coordinate data for PCA?\n",
    "2. **Preprocessing**: What preprocessing steps are crucial?\n",
    "3. **Interpretation**: What do the principal components represent physically?\n",
    "4. **Validation**: How would you verify your results make biological sense?\n",
    "\n",
    "**Solution Approach**:\n",
    "- **Data matrix**: time_points × (3 × number_of_atoms)\n",
    "- **Preprocessing**: Center each structure, possibly align to remove rotation\n",
    "- **PC1**: Often the \"breathing\" mode (overall expansion/contraction)\n",
    "- **PC2-PC3**: Hinge motions, domain movements\n",
    "\n",
    "### Astronomical Data\n",
    "**Scenario**: Telescope survey measured brightness in 20 wavelength bands for 10,000 stars.\n",
    "\n",
    "**Goal**: Classify star types using PCA.\n",
    "\n",
    "**Questions**:\n",
    "1. How many principal components might you expect to need?\n",
    "2. What would the principal components represent physically?\n",
    "3. How would you use PCA results for star classification?\n",
    "\n",
    "**Hints**:\n",
    "- Different star types have characteristic spectra\n",
    "- Temperature affects overall brightness curve shape\n",
    "- Chemical composition affects specific absorption lines\n",
    "- PC1 likely relates to stellar temperature\n",
    "- PC2-PC3 might capture metallicity, surface gravity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways**:\n",
    "\n",
    "1. **PCA transforms data** to new coordinates that maximize variance\n",
    "2. **Principal components are linear combinations** of original variables\n",
    "3. **Interpretation requires examining loadings** and variance explained\n",
    "4. **Standardization is crucial** when variables have different scales\n",
    "5. **PCA is exploratory** - use it to understand data structure\n",
    "6. **Limitations exist** - PCA assumes linear relationships\n",
    "\n",
    "**When to Use PCA**:\n",
    "- High-dimensional numerical data\n",
    "- Variables are correlated\n",
    "- Want to visualize or reduce dimensionality\n",
    "- Need to identify main patterns\n",
    "\n",
    "**When to Avoid PCA**:\n",
    "- Variables are already uncorrelated\n",
    "- All components are equally important\n",
    "- Nonlinear relationships dominate\n",
    "- Small sample size relative to variables\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "**Books**:\n",
    "- \"The Elements of Statistical Learning\" - Hastie, Tibshirani, Friedman\n",
    "- \"Pattern Recognition and Machine Learning\" - Bishop\n",
    "- \"Applied Multivariate Statistical Analysis\" - Johnson & Wichern\n",
    "\n",
    "**Online Resources**:\n",
    "- Scikit-learn PCA documentation\n",
    "- StatQuest PCA videos (Josh Starmer)\n",
    "- Andrew Ng's Machine Learning Course (PCA section)\n",
    "\n",
    "**Research Papers**:\n",
    "- Jolliffe, I.T. \"Principal Component Analysis\" (comprehensive review)\n",
    "- Ringner, M. \"What is principal component analysis?\" (Nature Biotechnology)\n",
    "- Domain-specific PCA applications in your field of interest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
