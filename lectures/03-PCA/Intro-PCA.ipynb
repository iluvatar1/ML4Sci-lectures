{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# An Introduction to Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Imagine you are an environmental scientist studying a lake. At a single point in the lake, you have a probe that measures 50 different things simultaneously: water temperature, pH, dissolved oxygen, turbidity, conductivity, and concentrations of 45 different chemical pollutants. You collect this 50-dimensional data point every minute for a year.\n",
    "\n",
    "Now you have millions of data points, each with 50 features. Your goal is simple: 'How did the lake's overall health change over the year?'\n",
    "\n",
    "Answering this is impossible by plotting 50 separate graphs. Many of these features are redundant (e.g., several pollutants might come from the same source and always appear together). How can we boil these 50 correlated features down to just two or three 'meta-features' that capture the most important patterns of change? For instance, perhaps we could discover a 'Pollution Event Axis' and a 'Seasonal Temperature Axis'.\n",
    "\n",
    "This is the core problem that Principal Component Analysis solves. It is a powerful method for finding the most meaningful 'meta-features' (the Principal Components) in complex, high-dimensional data, allowing us to visualize patterns and understand the hidden structure of our measurements.\n",
    "\n",
    "## Dimensionality reduction methods\n",
    "The common goal of dimensionality reduction is to transform high-dimensional data into a lower-dimensional space while preserving as much of the meaningful structure and properties of the original data as possible. This is done to simplify datasets, reduce computational and storage costs, and make data easier to visualize and analyze. The following list show some of the more relevant methods used:\n",
    "\n",
    "- **Principal Component Analysis (PCA)** A linear technique that transforms the data into a new coordinate system of orthogonal axes called principal components. These components are ordered by the amount of variance they capture from the original data. Reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- **Linear Discriminant Analysis (LDA)** A supervised linear technique that finds a linear combination of features that best separates two or more classes of objects. It aims to maximize the between-class variance while minimizing the within-class variance. Reference: https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "- **t-Distributed Stochastic Neighbor Embedding (t-SNE)** A non-linear method primarily used for visualizing high-dimensional data. It models data points by their similarities in both high and low dimensions, preserving the local structure of the data. Reference: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "- **Uniform Manifold Approximation and Projection (UMAP)** A modern non-linear technique that constructs a graph representation of the data in high dimensions and then optimizes a similar graph in a lower dimension. It is known for its speed and ability to preserve both local and global data structure. Reference: https://umap-learn.readthedocs.io/en/latest/\n",
    "- **Kernel PCA** An extension of PCA that uses kernel functions to perform non-linear dimensionality reduction. It implicitly maps the data to a higher-dimensional space where linear separation is possible, then applies PCA. Reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "- **Autoencoders** A type of neural network used for unsupervised learning. It consists of an \"encoder\" that compresses the input into a low-dimensional code and a \"decoder\" that reconstructs the input from this code. The compressed code is the lower-dimensional representation. Reference: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "- **Feature Selection** This is a category of methods that select a subset of the original features rather than creating new ones. Techniques include filter methods (e.g., chi-squared test), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., LASSO regularization). Reference: https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "- **ISO MAP** Preserves geodesic distances on data manifold using neighborhood graph + MDS.\n",
    "\n",
    "A nice visualization to compare: https://projector.tensorflow.org/\n",
    "\n",
    "\n",
    "| Method | Advantages | Disadvantages |\n",
    "| :--- | :--- | :--- |\n",
    "| **PCA** | Simple, fast, and removes correlated features. Effective for linear data. | Can be sensitive to data scaling. Performs poorly with non-linear data. |\n",
    "| **LDA** | Improves classification performance by focusing on class separability. | Supervised (requires labels). Assumes data is normally distributed. |\n",
    "| **t-SNE** | Excellent at visualizing local clusters in high-dimensional data. | Computationally expensive. Can lose global structure information. |\n",
    "| **UMAP** | Very fast and scalable. Preserves both local and global data structure. | Can be sensitive to hyperparameters. Interpretation can be nuanced. |\n",
    "| **Kernel PCA**| Can capture complex, non-linear relationships in the data. | More computationally intensive than PCA. Requires careful kernel selection. |\n",
    "| **Autoencoders**| Can learn highly complex, non-linear representations. Flexible for various data types. | Can be prone to overfitting. Requires a large amount of training data. |\n",
    "| **Feature Selection**| Improves model interpretability by using original features. Reduces overfitting. | May discard features that are useful only in combination with others. |\n",
    "|**ISO Map**|aptures global nonlinear structure; preserves manifold geometry well if assumptions hold.|(Classic references like “Isomap: Learning Nonlinear Structure from High‑Dimensional Data”, etc.) — not explicitly in the small set I pulled but widely cited.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction to PCA\n",
    "What is Principal Component Analysis?\n",
    "- Many scientific datasets have many correlated measurements: spectra (many wavelengths), climate grids (many locations), images (many pixels), recordings from many sensors or neurons.\n",
    "- PCA finds orthogonal directions (principal components) that explain the most variance, which helps visualization, noise reduction, compression, and discovering latent degrees of freedom.\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while preserving as much variance as possible. Think of it as finding the best camera angle to capture the most information about a 3D object in a 2D photograph.\n",
    "\n",
    "Key Concepts:\n",
    "- Dimensionality Reduction: Transform high-dimensional data to lower dimensions\n",
    "- Variance Preservation: Keep the most important patterns in the data\n",
    "- Data Visualization: Make complex datasets interpretable\n",
    "- Noise Reduction: Filter out less important variations\n",
    "\n",
    "| Field | Application | Benefits |\n",
    "|-------|-------------|----------|\n",
    "| **Spectroscopy** | Identify key wavelengths in complex spectra | Reduce noise, find characteristic peaks |\n",
    "| **Genomics** | Find patterns in gene expression data | Identify gene clusters, reduce computational complexity |\n",
    "| **Climate Science** | Understand weather patterns from multiple variables | Visualize climate patterns, identify trends |\n",
    "| **Chemistry** | Analyze molecular properties and reactions | Optimize reaction conditions, understand structure-property relationships |\n",
    "| **Physics** | Process sensor data from experiments | Extract signals from noise, identify fundamental modes |\n",
    "\n",
    "\n",
    ":::{exercise} Understanding Dimensionality\n",
    "\n",
    "**Scenario**: A researcher has measurements of temperature, humidity, pressure, and wind speed from 1000 weather stations. They want to create a 2D map showing weather patterns.\n",
    "\n",
    "**Question**: How could PCA help in this situation?\n",
    "\n",
    "**Tasks**:\n",
    "1. Identify the original dimensionality of the data\n",
    "2. Explain what the principal components might represent\n",
    "3. Discuss potential limitations of reducing to 2D\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data generation\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.normal(size=n)\n",
    "y = 2.0 * x + 0.8 * np.random.normal(size=n)\n",
    "X = np.vstack([x,y]).T\n",
    "Xc = X - X.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Xc[:,0], Xc[:,1], s=15)\n",
    "plt.xlabel('x (centered)')\n",
    "plt.ylabel('y (centered)')\n",
    "plt.title('2D data')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This data is clearly positively correlated, as can be also be seen from a seaborn pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the centered data\n",
    "df = pd.DataFrame(Xc, columns=['x', 'y'])\n",
    "\n",
    "# Pairplot to visualize pairwise relationships and distributions\n",
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Some info can be drawn from the so-called covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the covariance matrix, and its eigen values and vectors, ordered\n",
    "# rowvar = False means vars are in columns in this case\n",
    "C = np.cov(Xc, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the covariance matrix as a heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(C, annot=True, cmap=\"coolwarm\", cbar=True, square=True, fmt=\".2f\", \n",
    "            xticklabels=[\"x\", \"y\"], yticklabels=[\"x\", \"y\"])\n",
    "plt.title(\"Covariance Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, there is more info embedded in the correlation matrix. You can compute the correlation coefficient, defined as \n",
    "\\begin{equation}\n",
    "Corr(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, the condition number\n",
    "\\begin{equation}\n",
    "\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n",
    "\\end{equation}\n",
    "also helps to interpret the covariance matrix. For instance, if $\\kappa$ is very large, it suggest that the problem is ill-represented and there might be multi-linearities (high correlation among features). \n",
    "\n",
    "Its determinant speaks about the volume in feature space, and small value might indicate that the data is redundant. \n",
    "\n",
    "Furthermore, the Covariance matrix  eigen-values (explained variance) and eigen-vectors (principal components) are also meaningfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Visual demo: 2D correlated data and PC1\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "order = eigvals.argsort()[::-1]\n",
    "eigvals = eigvals[order]\n",
    "eigvecs = eigvecs[:, order]\n",
    "\n",
    "# Visualize data and eigen directions\n",
    "pc1 = eigvecs[:,0]\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Xc[:,0], Xc[:,1], s=15)\n",
    "origin = np.zeros(2)\n",
    "c = {-3: 'r', 3 : 'k'}\n",
    "for s in [-3,3]:\n",
    "    p = s * np.sqrt(eigvals[0]) * pc1\n",
    "    plt.plot([origin[0], p[0]], [origin[1], p[1]], linewidth=3, c=c[s])\n",
    "plt.xlabel('x (centered)')\n",
    "plt.ylabel('y (centered)')\n",
    "plt.title('2D correlated data and first principal component')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "\n",
    "# Data generation\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.normal(size=n)\n",
    "y = 2.0 * x + 0.8 * np.random.normal(size=n)\n",
    "X = np.vstack([x, y]).T\n",
    "Xc = X - X.mean(axis=0)\n",
    "\n",
    "# Compute the covariance matrix, and its eigen values and vectors\n",
    "C = np.cov(Xc, rowvar=False)\n",
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "order = eigvals.argsort()[::-1]\n",
    "eigvals = eigvals[order]\n",
    "eigvecs = eigvecs[:, order]\n",
    "\n",
    "\n",
    "# Check orthogonality: the dot product should be 0 for orthogonal vectors\n",
    "print(f\"{eigvecs[:, 0]=}   {eigvecs[:, 1]=}\")\n",
    "dot_product = np.dot(eigvecs[:, 0], eigvecs[:, 1])\n",
    "print(f\"Dot product of PC1 and PC2: {dot_product}\")\n",
    "\n",
    "# print the eigevals normalized by the total sum\n",
    "print(f\"Eigenvals proportion: {eigvals/np.sum(eigvals)}\")\n",
    "\n",
    "# First principal component\n",
    "pc1 = eigvecs[:, 0]\n",
    "\n",
    "# Bokeh plot setup\n",
    "output_notebook()  # Ensure the plot is rendered inline\n",
    "\n",
    "# Create figure\n",
    "p = figure(width=600, height=600, title=\"2D Correlated Data and PC1 and PC2\", match_aspect=True)\n",
    "p.scatter(Xc[:, 0], Xc[:, 1], size=8, color=\"blue\", alpha=0.5)\n",
    "\n",
    "# Plot the principal component lines\n",
    "origin = np.array([0, 0])\n",
    "color = {-3:\"red\", 3:\"black\"}\n",
    "for s in [-3, 3]:\n",
    "    p.line([origin[0], s * np.sqrt(eigvals[0]) * pc1[0]], \n",
    "           [origin[1], s * np.sqrt(eigvals[0]) * pc1[1]], \n",
    "           line_width=3, color=color[s], alpha=0.6)\n",
    "\n",
    "# Plot the second principal component lines\n",
    "# First principal component\n",
    "pc2 = eigvecs[:, 1]\n",
    "color = {-3:\"orange\", 3:\"cyan\"}\n",
    "for s in [-3, 3]:\n",
    "    p.line([origin[0], s * np.sqrt(eigvals[1]) * pc2[0]], \n",
    "           [origin[1], s * np.sqrt(eigvals[1]) * pc2[1]], \n",
    "           line_width=3, color=color[s], alpha=0.9)\n",
    "\n",
    "\n",
    "# Labels and formatting\n",
    "p.xaxis.axis_label = \"x (centered)\"\n",
    "p.yaxis.axis_label = \"y (centered)\"\n",
    "p.grid.grid_line_color = \"white\"\n",
    "p.title.text_font_size = '32px'\n",
    "p.grid.grid_line_color = \"gray\"  # Grid line color\n",
    "p.grid.grid_line_alpha = 0.3  # Transparency of grid lines\n",
    "p.grid.grid_line_width = 1  # Width of grid lines\n",
    "\n",
    "# Show plot\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is the key for PCA method: finding the eigen values and eigen vectors give some special directions where the data varies the most and allow to reduce the dimensionality of the data (in this case, using the first principal component will be enough) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Example applications to basic sciences\n",
    "### Biology & Molecular Biology\n",
    "\n",
    "- Higa, G. S., de Oliveira, L. G., Luns, K. F., de Jesus, R. E., Ferreira, R. C., da Silva, J. A., Cunha, P. H., & Pires, D. S. (2024). Application of Principal Component Analysis as a Prediction Model for Feline Sporotrichosis. Animals, 14(12), 1696.  <https://www.mdpi.com/2306-7381/12/1/32>\n",
    "\n",
    "- Sfriso, P., & Crave, A. (2022). Principal Component Analysis and Related Methods for Investigating the Dynamics of Biological Macromolecules. J - Multidisciplinary Scientific Journal, 5(2), 298–317. <https://www.mdpi.com/2571-8800/5/2/21>\n",
    "\n",
    "### Environmental Science & Earth Science\n",
    "- Huang, J., Lu, D., Lin, W., & Yang, Q. (2024). Enhancing slope stability prediction through integrated PCA-SSA-SVM modeling: a case study of LongLian expressway. Frontiers in Earth Science, 12. DOI: https://doi.org/10.3389/feart.2024.1429601\n",
    "\n",
    "- Kaur, L., & Godara, P. (2025). Application of Principal Component Analysis (PCA) in groundwater quality evaluation: A case study of arid region. Energy & Environment Management, 1(21). <https://ojs.awpublishing.org/eem/article/view/17>\n",
    "\n",
    "- Kahangwa, C. (2022). Application of Principal Component Analysis, Cluster Analysis, Pollution Index and Geoaccumulation Index in Pollution Assessment with Heavy Metals from Gold Mining Operations, Tanzania. Journal of Geoscience and Environment Protection, 10(4), 303-317. <https://www.scirp.org/journal/paperinformation?paperid=116892>\n",
    " \n",
    "### Chemistry\n",
    "\n",
    "- Moreira, M., Hillenkamp, M., Divitini, G., Tizei, L. H. G., Ducati, C., Cotta, M. A., Rodrigues, V., & Ugarte, D. (2025). Improving Quantitative EDS Chemical Analysis of Alloy Nanoparticles by PCA Denoising: Part I, Reducing Reconstruction Bias. Microscopy and Microanalysis. DOI: <https://academic.oup.com/mam/article-abstract/28/2/338/6911396?redirectedFrom=fulltext>\n",
    "\n",
    "### Physics\n",
    "\n",
    "- Yao, Z., Yang, J., & Lin, H.-Q. (2025). Principal component analysis for percolation with and without data preprocessing. Physical Review E, 111(4).DOI: <https://journals.aps.org/pre/abstract/10.1103/PhysRevE.111.045303>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mathematical Foundation\n",
    "PCA is often viewed as looking for the directions with the most variance. But it can also be formulated as  an optimization problem, and this explains why is the covariance matrix actually there. First, let's assume all data is centered, that is it has mean 0. Now, let's also assume that there is a new given unit vector $u$ that could be more useful that some of the original axis we were using. The projection of the features vector $x$ on $u$ is \n",
    "\\begin{equation}\n",
    "x' = (u\\cdot x) u = (u^T x) u = x_u u.\n",
    "\\end{equation}\n",
    "The dot product represents the amount of information we are retaining, it is maximum when both vectors are in the same direction, and null when they are orthogonal. Since it could be negative, we can define the aount of preserved information as \n",
    "\\begin{equation}\n",
    "(u\\cdot x)^2,\n",
    "\\end{equation}\n",
    "or, better, the mean preserved info over all samples as \n",
    "\\begin{equation}\n",
    "\\frac{1}{n}\\sum_i^n (u\\cdot x_i)^2 = \\frac{1}{n} \\sum_i (x_{i,u})^2 = \\frac{1}{n} \\sum_i x_{i,u}^T x_{i,u} = \\frac{1}{n} \\sum_i (x_i^T u)(u^T x_i) = \\frac{1}{n} \\sum_i (x_i^T x_i) u u^T = \\frac{1}{n} \\sum_i (x_i^T x_i) u^2, = C u^2,   \n",
    "\\end{equation}\n",
    "where we have replaced the definition of the covariance matrix\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{n} \\sum_i (x_i^T x_i).\n",
    "\\end{equation}\n",
    "Therefore, to find the direction that preserve the most information about our original data, we must solve the optimization problem\n",
    "\\begin{equation}\n",
    "\\max Cu^2\n",
    "\\end{equation}\n",
    "subject to \n",
    "\\begin{equation}\n",
    "u^2 = 1.\n",
    "\\end{equation}\n",
    "This can be donde using the method of Lagrange multipliers, and some matrix calculus allows us to write it here simply:\n",
    "\\begin{equation}\n",
    "\\max Cu^2, \\text{s.t, \\ \\ } u^2 = 1,\n",
    "\\end{equation}\n",
    "deriving with respect to u (yes, it is a vector, see matrix calculus), and introducing a Lagrange multiplier $\\lambda$ for the constraint\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial u} (Cu^2 - \\lambda (u^2 - 1)) = 0,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "2Cu - 2\\lambda u = 0,\n",
    "\\end{equation}\n",
    "and, finally\n",
    "\\begin{equation}\n",
    "Cu = \\lambda u, \n",
    "\\end{equation}\n",
    "which is exactly the eigen-value problem for the covariance matrix. Therefore, the directions that preserve the most information about our original data are exactly the eigen-vectors of the covariance matrix.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## General algorithm\n",
    "### Step 1: Data Centering\n",
    "\\begin{equation}\n",
    "\\hat X = X - \\mu\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mu$ is the mean of each variable.\n",
    "\n",
    "**Why center the data?**\n",
    "- Ensures PCA finds directions of maximum variance from the center\n",
    "- Prevents variables with larger scales from dominating\n",
    "\n",
    "### Step 2: Covariance Matrix\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{n-1}\\  \\hat X^T \\times \\hat X.\n",
    "\\end{equation}\n",
    "\n",
    "Where $n$ is the number of observations.\n",
    "\n",
    "**The covariance matrix captures**:\n",
    "- How much each variable varies (diagonal elements)\n",
    "- How variables co-vary together (off-diagonal elements)\n",
    "\n",
    ":::{exercise}\n",
    "You are given the following three (centered) data points for two features, 'Gene A expression' and 'Gene B expression':\n",
    "\n",
    "    Sample 1: (-2, -1)\n",
    "\n",
    "    Sample 2: (0, 0)\n",
    "\n",
    "    Sample 3: (2, 1)\n",
    "\n",
    "Without using a calculator, do you expect the covariance between Gene A and Gene B to be positive, negative, or near zero? Why?\n",
    "Calculate the 2x2 covariance matrix for this data. Does the result confirm your intuition?\n",
    ":::\n",
    "\n",
    "### Step 3: Eigendecomposition\n",
    "\\begin{equation}\n",
    "C\\times \\vec v = \\lambda \\vec v\n",
    "\\end{equation}\n",
    "Where:\n",
    "- $\\vec v$ are eigenvectors (principal component directions)\n",
    "- $\\lambda$ are eigenvalues (variance captured by each component)\n",
    "\n",
    "### Step 4: Variance Explained\n",
    "\n",
    "Variance Explained by \n",
    "\\begin{equation}\n",
    "PC_i = \\frac{\\lambda_i}{\\sum_j\\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    ":::{exercise}Covariance Understanding\n",
    "**Question**: If two variables have a covariance of 0, what does this mean for PCA?\n",
    "\n",
    "Consider this covariance matrix:\n",
    "```\n",
    "C = [4.0  0.0]\n",
    "    [0.0  1.0]\n",
    "```\n",
    "\n",
    "- What are the eigenvalues?\n",
    "- What are the eigenvectors?\n",
    "- How much variance does each PC explain?\n",
    "\n",
    ":::{exercise} The scree plot decision\n",
    "An ecologist runs a PCA on a dataset with 10 features related to forest health. The explained variance for each component is:\n",
    "\n",
    "    PC1: 45%\n",
    "\n",
    "    PC2: 30%\n",
    "\n",
    "    PC3: 8%\n",
    "\n",
    "    PC4: 7%\n",
    "\n",
    "    PC5: 4%\n",
    "\n",
    "    PC6-PC10: <1% each\n",
    "\n",
    "- Create a scree plot (bar chart of explained variance).\n",
    "- Using the \"elbow method,\" how many principal components would you choose to retain for further analysis?\n",
    "- What is the cumulative variance explained by your chosen number of components? Is this likely sufficient?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Advanced Details\n",
    "\n",
    "### PCA Assumptions and Limitations\n",
    "\n",
    "**Assumptions**:\n",
    "- Linear relationships between variables\n",
    "- Data follows (approximately) multivariate normal distribution\n",
    "- Variables are continuous\n",
    "\n",
    "**Limitations**:\n",
    "- **Linear combinations only**: Cannot capture nonlinear patterns\n",
    "- **Variance-based**: May not preserve class separability\n",
    "- **Global method**: Same transformation for all data points\n",
    "\n",
    "**When PCA May Not Work Well**:\n",
    "- Highly nonlinear data (consider kernel PCA)\n",
    "- Categorical variables (consider correspondence analysis)\n",
    "- When rare events are important (PCA focuses on major patterns)\n",
    "\n",
    "### Robust PCA\n",
    "\n",
    "**Problem**: Standard PCA is sensitive to outliers.\n",
    "\n",
    "**Solutions**:\n",
    "- **Robust PCA**: Use median instead of mean, robust covariance estimation\n",
    "- **Sparse PCA**: Assume many loadings are exactly zero\n",
    "- **Kernel PCA**: Handle nonlinear relationships\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## PCA applied to the previous data using scikit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Data generation\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.normal(size=n)\n",
    "y = 2.0 * x + 0.8 * np.random.normal(size=n)\n",
    "X = np.vstack([x, y]).T\n",
    "\n",
    "#Xc = X - X.mean(axis=0)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"\\nVariance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "print(f\"Total variance explained: {np.sum(explained_variance):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## PCA applied to real data \n",
    "\n",
    "Sky data survey. To generate the data, go to <https://skyserver.sdss.org/dr14/en/tools/search/form/searchform.aspx> and write or use the default query. To get the full data, use the following sql command\n",
    "```sql\n",
    "    \n",
    "SELECT TOP 10000\n",
    "    p.ra, p.dec, p.u, p.g, p.r, p.i, p.z,\n",
    "    p.run, p.rerun, p.camcol, p.field,\n",
    "    s.specobjid, s.class, s.z as redshift\n",
    "FROM PhotoObj AS p\n",
    "JOIN SpecObj AS s ON s.bestobjid = p.objid\n",
    "WHERE\n",
    "    p.u BETWEEN 0 AND 19.6\n",
    "    AND p.g BETWEEN 0 AND 20\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- 1. DATA ACQUISITION ---\n",
    "# Load the data from the CSV file you downloaded from the SDSS SkyServer.\n",
    "#file_path = 'SDSS_DR14.csv' # Or whatever you named the downloaded file\n",
    "file_path = 'skyserverdata2.csv' # Or whatever you named the downloaded file\n",
    "# url = \"https://drive.google.com/file/d/1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O/view?usp=drive_link\"\n",
    "\n",
    "try:\n",
    "    # Use skiprows=1 to ignore the first line (\"#Table1\")\n",
    "    sdss_df = pd.read_csv(file_path, skiprows=1)\n",
    "    print(\"SDSS dataset loaded successfully and parsed correctly!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: '{file_path}'. Please make sure you have downloaded the data and placed it in the correct directory.\")\n",
    "    sdss_df = pd.DataFrame()\n",
    "\n",
    "# Proceed only if the DataFrame was loaded successfully\n",
    "if not sdss_df.empty:\n",
    "    # --- 2. DATA EXPLORATION AND PREPARATION ---\n",
    "    print(\"\\nDataset Shape:\", sdss_df.shape)\n",
    "    print(\"\\nColumns:\", sdss_df.columns)\n",
    "\n",
    "    # Define our features and the target variable for coloring the plot\n",
    "    features = ['u', 'g', 'r', 'i', 'z']\n",
    "    target = 'class'\n",
    "\n",
    "    # --- Safety Check ---\n",
    "    # Verify that all required columns exist in the DataFrame before proceeding.\n",
    "    if target in sdss_df.columns and all(col in sdss_df.columns for col in features):\n",
    "        \n",
    "        X = sdss_df[features]\n",
    "        y = sdss_df[target]\n",
    "\n",
    "        print(\"\\nNumber of samples for each class:\")\n",
    "        print(y.value_counts())\n",
    "\n",
    "        # Standardize the feature data\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        print(\"\\nData has been standardized.\")\n",
    "\n",
    "        # --- 3. APPLYING PCA ---\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print(f\"\\nVariance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "        print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "        print(f\"Total variance explained: {np.sum(explained_variance):.2%}\")\n",
    "\n",
    "        # --- 4. VISUALIZATION AND INTERPRETATION ---\n",
    "\n",
    "        # Create a new DataFrame for easier plotting with seaborn\n",
    "        pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "        pca_df['class'] = y.values # Add the class labels for coloring\n",
    "\n",
    "        # --- PLOT 1: SCORES PLOT (COLORED) ---\n",
    "        # Scores mean the data is plotted on the new coordinate system : PC1xPC2\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        sns.scatterplot(\n",
    "            x='PC1', y='PC2',\n",
    "            hue='class',\n",
    "            data=pca_df,\n",
    "            palette='viridis',\n",
    "            alpha=0.7,\n",
    "            s=40 # marker size\n",
    "        )\n",
    "        plt.title('PCA of SDSS Astronomical Objects (based on colors)', fontsize=16)\n",
    "        plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})', fontsize=12)\n",
    "        plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})', fontsize=12)\n",
    "        plt.legend(title='Object Class')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # --- PLOT 2: LOADINGS PLOT ---\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # In scikit-learn, pca.components_ are the loadings. We transpose for easier plotting.\n",
    "        loadings = pca.components_.T\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.05, head_length=0.05, color='red', alpha=0.8)\n",
    "            plt.text(loadings[i, 0]*1.15, loadings[i, 1]*1.15, feature, color='black', ha='center', va='center', fontsize=14)\n",
    "\n",
    "        plt.xlim(-0.8, 0.8)\n",
    "        plt.ylim(-0.8, 0.8)\n",
    "        plt.xlabel(f'PC1 ({explained_variance[0]:.2%})')\n",
    "        plt.ylabel(f'PC2 ({explained_variance[1]:.2%})')\n",
    "        plt.title('Loadings Plot', fontsize=16)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(f\"The target column '{target}' or one of the feature columns was not found in the data.\")\n",
    "        print(\"Please ensure your downloaded file contains all necessary columns.\")\n",
    "        print(\"Required columns: 'u', 'g', 'r', 'i', 'z', 'class'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a bokeh plotting version with some additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import column\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable Bokeh output in notebook\n",
    "output_notebook()\n",
    "\n",
    "# --- 1. DATA ACQUISITION ---\n",
    "file_path = 'skyserverdata2.csv'  # Or whatever you named the downloaded file\n",
    "# url = \"https://drive.google.com/file/d/1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O/view?usp=drive_link\"\n",
    "#file_path = \"https://drive.google.com/file/d/1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O/view\"\n",
    "file_path = \"https://drive.usercontent.google.com/download?id=1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O&export=download\"\n",
    "#file_path = \"https://drive.google.com/uc?id=1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O&export=download\"\n",
    "\n",
    "try:\n",
    "    # Use skiprows=1 to ignore the first line (\"#Table1\")\n",
    "    sdss_df = pd.read_csv(file_path, skiprows=1)\n",
    "    print(\"SDSS dataset loaded successfully and parsed correctly!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: '{file_path}'. Please make sure you have downloaded the data and placed it in the correct directory.\")\n",
    "    sdss_df = pd.DataFrame()\n",
    "\n",
    "# Proceed only if the DataFrame was loaded successfully\n",
    "if not sdss_df.empty:\n",
    "    # --- 2. DATA EXPLORATION AND PREPARATION ---\n",
    "    print(\"\\nDataset Shape:\", sdss_df.shape)\n",
    "    print(\"\\nColumns:\", sdss_df.columns.tolist())\n",
    "\n",
    "    # Define our features and the target variable for coloring the plot\n",
    "    features = ['u', 'g', 'r', 'i', 'z']\n",
    "    target = 'class'\n",
    "\n",
    "    # --- Safety Check ---\n",
    "    if target in sdss_df.columns and all(col in sdss_df.columns for col in features):\n",
    "        \n",
    "        X = sdss_df[features]\n",
    "        y = sdss_df[target]\n",
    "\n",
    "        print(\"\\nNumber of samples for each class:\")\n",
    "        print(y.value_counts())\n",
    "\n",
    "        # Remove any rows with NaN values\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        print(f\"\\nAfter removing NaN values: {len(X)} samples\")\n",
    "\n",
    "        # Standardize the feature data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        print(\"\\nData has been standardized.\")\n",
    "\n",
    "        # --- 3. APPLYING PCA ---\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print(f\"\\nVariance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "        print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "        print(f\"Total variance explained: {np.sum(explained_variance):.2%}\")\n",
    "\n",
    "        # --- 4. VISUALIZATION AND INTERPRETATION ---\n",
    "\n",
    "        # Create a new DataFrame for easier plotting with Bokeh\n",
    "        pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "        pca_df['class'] = y.values  # Add the class labels for coloring\n",
    "\n",
    "        # Get unique classes and assign colors\n",
    "        unique_classes = sorted(y.unique())\n",
    "        n_classes = len(unique_classes)\n",
    "        colors = Category10[max(3, n_classes)][:n_classes]\n",
    "        color_map = dict(zip(unique_classes, colors))\n",
    "        \n",
    "        # Add color column to dataframe\n",
    "        pca_df['color'] = [color_map[cls] for cls in pca_df['class']]\n",
    "\n",
    "        # --- PLOT 1: SCORES PLOT (COLORED) ---\n",
    "        print(\"\\nGenerating PCA scores plot...\")\n",
    "        \n",
    "        # Create hover tool\n",
    "        hover = HoverTool(tooltips=[\n",
    "            (\"Class\", \"@class\"),\n",
    "            (\"PC1\", \"@PC1{0.00}\"),\n",
    "            (\"PC2\", \"@PC2{0.00}\")\n",
    "        ])\n",
    "\n",
    "        # Prepare the data for the scatter plot (scores plot)\n",
    "        source = ColumnDataSource(pca_df)\n",
    "\n",
    "        p1 = figure(\n",
    "            width=800, \n",
    "            height=600, \n",
    "            title=\"PCA of SDSS Astronomical Objects (based on ugriz colors)\",\n",
    "            tools=[hover, \"pan\", \"wheel_zoom\", \"box_zoom\", \"reset\", \"save\"]\n",
    "        )\n",
    "        \n",
    "        # Create scatter plot for each class separately to get proper legend\n",
    "        for cls in unique_classes:\n",
    "            class_data = pca_df[pca_df['class'] == cls]\n",
    "            class_source = ColumnDataSource(class_data)\n",
    "            p1.circle(\n",
    "                x='PC1', y='PC2', \n",
    "                source=class_source, \n",
    "                size=8, \n",
    "                color=color_map[cls],\n",
    "                legend_label=str(cls), \n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        p1.xaxis.axis_label = f'Principal Component 1 ({explained_variance[0]:.1%} variance)'\n",
    "        p1.yaxis.axis_label = f'Principal Component 2 ({explained_variance[1]:.1%} variance)'\n",
    "        p1.legend.title = 'Object Class'\n",
    "        p1.legend.location = \"top_right\"\n",
    "        p1.legend.click_policy = \"hide\"  # Allow hiding/showing classes by clicking legend\n",
    "        \n",
    "        show(p1)\n",
    "\n",
    "        # --- PLOT 2: LOADINGS PLOT ---\n",
    "        print(\"\\nGenerating loadings plot...\")\n",
    "        \n",
    "        # In scikit-learn, pca.components_ are the loadings (transposed)\n",
    "        loadings = pca.components_.T  # Shape: (n_features, n_components)\n",
    "        \n",
    "        # Create the loadings plot\n",
    "        p2 = figure(\n",
    "            width=800, \n",
    "            height=600, \n",
    "            title=\"PCA Loadings Plot - Feature Contributions\",\n",
    "            tools=\"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "        )\n",
    "        \n",
    "        # Draw arrows from origin to each loading point\n",
    "        for i, feature in enumerate(features):\n",
    "            x_end = loadings[i, 0]\n",
    "            y_end = loadings[i, 1]\n",
    "            \n",
    "            # Draw arrow line\n",
    "            p2.line([0, x_end], [0, y_end], line_width=2, color='red', alpha=0.8)\n",
    "            \n",
    "            # Draw arrow head (simple triangle)\n",
    "            p2.triangle([x_end], [y_end], size=10, color='red', alpha=0.8)\n",
    "            \n",
    "            # Add feature labels\n",
    "            p2.text(\n",
    "                x=[x_end * 1.1], y=[y_end * 1.1], \n",
    "                text=[feature], \n",
    "                text_font_size=\"12pt\", \n",
    "                text_align=\"center\",\n",
    "                text_baseline=\"middle\"\n",
    "            )\n",
    "\n",
    "        # Add grid lines at origin\n",
    "        p2.line([-1, 1], [0, 0], line_color='gray', line_alpha=0.3)\n",
    "        p2.line([0, 0], [-1, 1], line_color='gray', line_alpha=0.3)\n",
    "        \n",
    "        # Add unit circle for reference\n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        circle_x = np.cos(theta)\n",
    "        circle_y = np.sin(theta)\n",
    "        p2.line(circle_x, circle_y, line_color='gray', line_alpha=0.3, line_dash='dashed')\n",
    "\n",
    "        p2.xaxis.axis_label = f'PC1 ({explained_variance[0]:.1%} variance)'\n",
    "        p2.yaxis.axis_label = f'PC2 ({explained_variance[1]:.1%} variance)'\n",
    "        \n",
    "        # Set equal aspect ratio and appropriate ranges\n",
    "        max_range = max(abs(loadings.max()), abs(loadings.min())) * 1.2\n",
    "        p2.x_range.start = -max_range\n",
    "        p2.x_range.end = max_range\n",
    "        p2.y_range.start = -max_range\n",
    "        p2.y_range.end = max_range\n",
    "        \n",
    "        show(p2)\n",
    "\n",
    "        # --- INTERPRETATION ---\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INTERPRETATION OF RESULTS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nPCA Summary:\")\n",
    "        print(f\"- Total variance explained by first 2 components: {np.sum(explained_variance):.1%}\")\n",
    "        print(f\"- PC1 explains {explained_variance[0]:.1%} of variance\")\n",
    "        print(f\"- PC2 explains {explained_variance[1]:.1%} of variance\")\n",
    "        \n",
    "        print(f\"\\nLoadings (feature contributions):\")\n",
    "        loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=features)\n",
    "        print(loadings_df.round(3))\n",
    "        \n",
    "        print(f\"\\nTop contributors to PC1:\")\n",
    "        pc1_contrib = abs(loadings_df['PC1']).sort_values(ascending=False)\n",
    "        for feature, contrib in pc1_contrib.items():\n",
    "            print(f\"  {feature}: {contrib:.3f}\")\n",
    "            \n",
    "        print(f\"\\nTop contributors to PC2:\")\n",
    "        pc2_contrib = abs(loadings_df['PC2']).sort_values(ascending=False)\n",
    "        for feature, contrib in pc2_contrib.items():\n",
    "            print(f\"  {feature}: {contrib:.3f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(f\"The target column '{target}' or one of the feature columns was not found in the data.\")\n",
    "        print(\"Please ensure your downloaded file contains all necessary columns.\")\n",
    "        print(\"Required columns: 'u', 'g', 'r', 'i', 'z', 'class'\")\n",
    "        if not sdss_df.empty:\n",
    "            print(f\"Available columns: {sdss_df.columns.tolist()}\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot proceed without data. Please check the file path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} The Importance of Standardization \n",
    "Take the original Python script and find the line where the data is scaled. Comment it and perform again the PCA analysis. Is there any consequence? Replot and analyze. How much variance is capture now by PC1 and PC2? Do you still see the quasars separation in the scores plot?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Incorporating a Physical Measurement\n",
    "Investigate how adding a non-color, physical feature like 'redshift' changes the PCA results and the interpretability of the components. Redshift is a measure of how much an object's light has been stretched as it travels through the expanding universe; it is strongly correlated with distance..  In the original script, add 'redshift' to the list of features to be included in the analysis. Re-run the script. \n",
    "- What is the new total explained variance captured by PC1 and PC2? Has it increased or decreased?\n",
    "- Look at the new Scores Plot. Is the separation between the three classes (STAR, GALAXY, QSO) better or worse than in the original plot? Pay close attention to the Star/Galaxy overlap.\n",
    "- Examine the new Loadings Plot. Where does the new 'redshift' arrow point? What does its direction and length tell you about its contribution to PC1 and PC2?\n",
    "- Based on the new loadings, what do PC1 and PC2 now represent? Has the interpretation of \"overall brightness\" or \"color contrast\" changed? What physical reason might explain why adding redshift was so effective (or ineffective) at separating the classes?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} What about more components?\n",
    "- Plot the scores for the first 3 and 4 principal components.\n",
    "- Analyze how the classes (STAR, GALAXY, QSO) are distributed in the 3D space\n",
    "- Calculate the cumulative explained variance for the first 3 or 4 components. How much of the total variance is explained by these components?\n",
    "- Does adding more components improves the separation between classes and whether the additional components add significant information?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise} Compare the performance of PCA with other dimensionality reduction methods like t-SNE or UMAP.\n",
    "- Apply t-SNE or UMAP to the original feature set (without PCA) and visualize the results.\n",
    "- Compare the ability of t-SNE/UMAP to separate the classes with PCA.\n",
    "- Discuss the advantages and limitations of PCA versus non-linear dimensionality reduction methods like t-SNE and UMAP.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Interpretation Guidelines\n",
    "\n",
    "### Choosing Number of Components\n",
    "\n",
    "**Method 1: Cumulative Variance**\n",
    "- Keep components explaining 80-95% of variance\n",
    "- Depends on application requirements\n",
    "\n",
    "**Method 2: Scree Plot (Elbow Method)**\n",
    "- Plot eigenvalues vs. component number\n",
    "- Look for \"elbow\" where decrease slows\n",
    "\n",
    "**Method 3: Kaiser Criterion**\n",
    "- Keep components with eigenvalues > 1 (if data is standardized)\n",
    "- Only applies when variables are standardized\n",
    "\n",
    "### Loading Interpretation\n",
    "\n",
    "**Loading Values**:\n",
    "- **|loading| > 0.7**: Strong relationship\n",
    "- **0.3 < |loading| < 0.7**: Moderate relationship\n",
    "- **|loading| < 0.3**: Weak relationship\n",
    "\n",
    "**Signs Matter**:\n",
    "- **Positive loading**: Variable increases with PC\n",
    "- **Negative loading**: Variable decreases with PC\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "| **Mistake** | **Correct** |\n",
    "|---|---|\n",
    "|Treating PCs as original variables|PCs are linear combinations of original variables|\n",
    "|Ignoring scaling/standardization|Consider whether to standardize based on variable units|\n",
    "|Over-interpreting small components|Focus on components explaining meaningful variance|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A detailed Worked Example - The Iris Dataset\n",
    "\n",
    "The Iris dataset is a classic in machine learning and statistics. It contains 150 samples from three species of Iris flowers (Setosa, Versicolor, and Virginica). For each sample, four features were measured: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "Our goal is to see if we can use PCA to 'summarize' these four measurements and visualize the separation between the species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Setup - Importing Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data manipulation, numerical computation, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Set some default plotting styles for better looking visuals\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data # The feature matrix\n",
    "y = iris.target # The labels (species)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Let's look at the first 5 rows of the data\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"\\nFirst 5 rows of data:\\n\", X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 1: Standardize the Data\n",
    "\n",
    "PCA is sensitive to the scale of the features. We need to standardize our data so that each feature has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(\"First 5 rows of scaled data:\\n\", X_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 2: Perform PCA\n",
    "\n",
    "Now we apply PCA. We'll ask `scikit-learn` to find the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize PCA and fit the scaled data\n",
    "# n_components specifies how many dimensions we want to reduce to.\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the model and transform the data to the new coordinate system\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3: Analyze and Visualize the Results\n",
    "\n",
    "#### Scree Plot\n",
    "\n",
    "First, let's see how much variance our two new components capture. A scree plot is perfect for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(f\"Variance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "print(f\"Total variance explained by first two components: {np.sum(explained_variance):.2%}\")\n",
    "\n",
    "# To make a full scree plot, we can re-run PCA without specifying n_components\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1), pca_full.explained_variance_ratio_ * 100, alpha=0.7, align='center', label='Individual explained variance')\n",
    "plt.step(range(1, len(pca_full.explained_variance_ratio_) + 1), np.cumsum(pca_full.explained_variance_ratio_) * 100, where='mid', label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Percentage')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Scree Plot for Iris Dataset')\n",
    "plt.xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Observation:** The first two components capture over 95% of the total variance in the data! This means our 2D plot will be a very good representation of the original 4D data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Scores Plot\n",
    "\n",
    "Next, we create a scores plot. This is a scatter plot of our samples in the new PCA space. We will color each point according to its true species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA Scores Plot of Iris Dataset')\n",
    "plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Observation:** The three species are very well separated in the PCA plot. The *Setosa* species is a distinct cluster, while *Versicolor* and *Virginica* are also mostly separated, though they have some overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Biplot\n",
    "\n",
    "Finally, the biplot helps us understand *why* the samples are separated. It overlays the original feature vectors (loadings) on top of the scores plot. This tells us how the original variables contribute to the principal components.\n",
    "\n",
    "We will use a helper function to create a clean biplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def biplot(score, coeff, labels=None):\n",
    "    \"\"\"\n",
    "    Creates a biplot visualization.\n",
    "    \n",
    "    score: The transformed data (scores), e.g., X_pca.\n",
    "    coeff: The eigenvectors (loadings), e.g., pca.components_.T.\n",
    "    labels: The names of the original features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    xs = score[:, 0]\n",
    "    ys = score[:, 1]\n",
    "    n = coeff.shape[0]\n",
    "    \n",
    "    # Plot the scores\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "        plt.scatter(xs[y == i], ys[y == i], color=color, alpha=0.7, label=target_name)\n",
    "\n",
    "    # Plot the loadings\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i, 0]*4, coeff[i, 1]*4, color='r', alpha=0.9, head_width=0.05)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i, 0] * 4.2, coeff[i, 1] * 4.2, \"Var\" + str(i + 1), color='black', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i, 0] * 4.2, coeff[i, 1] * 4.2, labels[i], color='black', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})')\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})')\n",
    "    plt.title('Biplot of Iris Dataset')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "# Call the function with our data\n",
    "# Note: we need to transpose pca.components_ to get the loadings in the right shape\n",
    "biplot(X_pca, np.transpose(pca.components_), labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Interpretation of the Biplot:**\n",
    "\n",
    "*   **PC1 (the horizontal axis):** All four variables have vectors pointing to the right, but `petal length`, `petal width`, and `sepal length` point most strongly in this direction. This suggests PC1 is a measure of **overall flower size**. Larger flowers (like Virginica) are on the right (high PC1 score), and smaller flowers (like Setosa) are on the left (low PC1 score).\n",
    "*   **PC2 (the vertical axis):** This axis shows an interesting contrast. `sepal width` points up, while `petal width` and `petal length` point down. This component separates flowers with wide sepals relative to their petal size from those with the opposite characteristics. This explains the separation between Versicolor and Virginica, which have similar overall sizes (PC1) but different shapes (PC2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Application Exercises\n",
    "\n",
    "Now it's your turn! Apply the techniques learned above to the following datasets. For each exercise, you'll need to:\n",
    "1. Load the data using `pandas`.\n",
    "2. Select the feature columns.\n",
    "3. Standardize the features.\n",
    "4. Perform PCA.\n",
    "5. Create and interpret a scores plot and/or a biplot.\n",
    "\n",
    "*(Note: You will need to have the `.csv` files in the same directory as this notebook for the code to work.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Genomics - Cancer Subtype Identification\n",
    "**Dataset:** `cancer_data.csv`\n",
    "**Task:** Perform PCA on the gene expression data. Can you identify distinct clusters in a scores plot? What might they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    cancer_df = pd.read_csv('cancer_data.csv')\n",
    "    print(\"Cancer data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select all columns except 'Sample_ID' and 'Subtype' as your features (X)\n",
    "    # X_cancer = ...\n",
    "    \n",
    "    # 2. Standardize X_cancer\n",
    "    # X_cancer_scaled = ...\n",
    "    \n",
    "    # 3. Perform PCA (n_components=2)\n",
    "    # pca_cancer = ...\n",
    "    # X_cancer_pca = ...\n",
    "    \n",
    "    # 4. Create a scores plot. You can color by the 'Subtype' column to see if the clusters match.\n",
    "    # plt.figure(...)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"File 'cancer_data.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemistry - Classifying Olive Oils\n",
    "**Dataset:** `olive_oil_spectra.csv`\n",
    "**Task:** Use PCA on the spectral data. Can you distinguish oils from different regions? Which wavelengths (columns) are most important for this separation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    oil_df = pd.read_csv('olive_oil_spectra.csv')\n",
    "    print(\"Olive oil data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the wavelength columns as features.\n",
    "    # X_oil = ...\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "\n",
    "    # 3. Create a biplot. It might be too cluttered to label all the variables (wavelengths),\n",
    "    #    so focus on the scores plot part and the general direction of the loadings cloud.\n",
    "    #    Color the points by the 'Region' column.\n",
    "    # ...\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'olive_oil_spectra.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Environmental Science - Air Pollution Sources\n",
    "**Dataset:** `air_pollution.csv`\n",
    "**Task:** Use PCA to identify patterns in air quality data. Create a biplot and interpret the loadings. Can you hypothesize what physical processes PC1 and PC2 represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    air_df = pd.read_csv('air_pollution.csv')\n",
    "    print(\"Air pollution data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the pollutant and meteorological columns as features.\n",
    "    # X_air = ...\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "\n",
    "    # 3. Create a biplot and interpret the loadings.\n",
    "    #    Look at how variables like Ozone, NOx, and Temperature are related in the PCA space.\n",
    "    # ...\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'air_pollution.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agriculture - Crop Yield Analysis\n",
    "**Dataset:** `crop_data.csv`\n",
    "**Task:** Perform PCA on the input variables (everything except `Crop_Yield`). Then, plot the PC1 scores against `Crop_Yield`. Is there a relationship? This demonstrates using PCA for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    crop_df = pd.read_csv('crop_data.csv')\n",
    "    print(\"Crop data loaded successfully!\")\n",
    "    # Your code here\n",
    "    # 1. Select the input variables as features.\n",
    "    # input_vars = ['Rainfall', 'Sunlight_Hours', 'Fertilizer_Amount', 'Soil_pH']\n",
    "    # X_crop = crop_df[input_vars]\n",
    "    # y_crop = crop_df['Crop_Yield']\n",
    "\n",
    "    # 2. Standardize and perform PCA.\n",
    "    # ...\n",
    "    # X_crop_pca = ...\n",
    "\n",
    "    # 3. Create a scatter plot of the first principal component vs. Crop_Yield.\n",
    "    # plt.figure(...)\n",
    "    # plt.scatter(X_crop_pca[:, 0], y_crop)\n",
    "    # plt.xlabel('Principal Component 1')\n",
    "    # plt.ylabel('Crop Yield')\n",
    "    # plt.title('PC1 vs. Crop Yield')\n",
    "    # plt.show()\n",
    "    # What does the relationship (or lack thereof) tell you?\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'crop_data.csv' not found. Please make sure it's in the same directory as the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pharmaceutical Analysis\n",
    "**Scenario**: A pharmaceutical company measured 50 chemical properties of 200 drug candidates to predict effectiveness.\n",
    "\n",
    "**Data Structure**:\n",
    "- Rows: 200 drug candidates\n",
    "- Columns: 50 chemical properties (molecular weight, logP, polar surface area, etc.)\n",
    "\n",
    "**Tasks**:\n",
    "1. **Preprocessing**: Should you standardize the variables? Why?\n",
    "2. **Analysis**: Apply PCA and determine how many components to retain\n",
    "3. **Interpretation**: If PC1 loads heavily on molecular weight, logP, and size-related properties, what does this PC represent?\n",
    "4. **Application**: How would you use PCA scores to select promising drug candidates?\n",
    "\n",
    "**Expected Results**:\n",
    "- PC1 (30%): \"Molecular size\" - larger, more lipophilic molecules\n",
    "- PC2 (20%): \"Polarity\" - hydrophilic vs. hydrophobic character\n",
    "- PC3 (15%): \"Complexity\" - structural complexity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Structure Analysis\n",
    "**Scenario**: You have 3D coordinates for all atoms in a protein from molecular dynamics simulations (1000 time points).\n",
    "\n",
    "**Challenge**: Identify main modes of protein flexibility.\n",
    "\n",
    "**Tasks**:\n",
    "1. **Data Setup**: How would you arrange the coordinate data for PCA?\n",
    "2. **Preprocessing**: What preprocessing steps are crucial?\n",
    "3. **Interpretation**: What do the principal components represent physically?\n",
    "4. **Validation**: How would you verify your results make biological sense?\n",
    "\n",
    "**Solution Approach**:\n",
    "- **Data matrix**: time_points × (3 × number_of_atoms)\n",
    "- **Preprocessing**: Center each structure, possibly align to remove rotation\n",
    "- **PC1**: Often the \"breathing\" mode (overall expansion/contraction)\n",
    "- **PC2-PC3**: Hinge motions, domain movements\n",
    "\n",
    "### Astronomical Data\n",
    "**Scenario**: Telescope survey measured brightness in 20 wavelength bands for 10,000 stars.\n",
    "\n",
    "**Goal**: Classify star types using PCA.\n",
    "\n",
    "**Questions**:\n",
    "1. How many principal components might you expect to need?\n",
    "2. What would the principal components represent physically?\n",
    "3. How would you use PCA results for star classification?\n",
    "\n",
    "**Hints**:\n",
    "- Different star types have characteristic spectra\n",
    "- Temperature affects overall brightness curve shape\n",
    "- Chemical composition affects specific absorption lines\n",
    "- PC1 likely relates to stellar temperature\n",
    "- PC2-PC3 might capture metallicity, surface gravity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways**:\n",
    "\n",
    "1. **PCA transforms data** to new coordinates that maximize variance\n",
    "2. **Principal components are linear combinations** of original variables\n",
    "3. **Interpretation requires examining loadings** and variance explained\n",
    "4. **Standardization is crucial** when variables have different scales\n",
    "5. **PCA is exploratory** - use it to understand data structure\n",
    "6. **Limitations exist** - PCA assumes linear relationships\n",
    "\n",
    "**When to Use PCA**:\n",
    "- High-dimensional numerical data\n",
    "- Variables are correlated\n",
    "- Want to visualize or reduce dimensionality\n",
    "- Need to identify main patterns\n",
    "\n",
    "**When to Avoid PCA**:\n",
    "- Variables are already uncorrelated\n",
    "- All components are equally important\n",
    "- Nonlinear relationships dominate\n",
    "- Small sample size relative to variables\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "**Books**:\n",
    "- \"The Elements of Statistical Learning\" - Hastie, Tibshirani, Friedman\n",
    "- \"Pattern Recognition and Machine Learning\" - Bishop\n",
    "- \"Applied Multivariate Statistical Analysis\" - Johnson & Wichern\n",
    "\n",
    "**Online Resources**:\n",
    "- Scikit-learn PCA documentation\n",
    "- StatQuest PCA videos (Josh Starmer)\n",
    "- Andrew Ng's Machine Learning Course (PCA section)\n",
    "\n",
    "**Research Papers**:\n",
    "- Jolliffe, I.T. \"Principal Component Analysis\" (comprehensive review)\n",
    "- Ringner, M. \"What is principal component analysis?\" (Nature Biotechnology)\n",
    "- Domain-specific PCA applications in your field of interest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
