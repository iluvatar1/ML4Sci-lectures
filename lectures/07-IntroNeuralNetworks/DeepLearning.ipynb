{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Deep Learning\n",
    "\n",
    "This notebook provides an introduction to the fundamental concepts of deep learning. \n",
    "\n",
    "**Goals:**\n",
    "*   Understand what \"deep\" means in deep learning.\n",
    "*   Visualize the structure of a deep neural network.\n",
    "*   Grasp the mathematics of forward and backward propagation.\n",
    "*   Apply these concepts to a practical problem and compare them to a traditional ML model.\n",
    "*   See examples of how deep learning is used in the basic sciences.\n",
    "\n",
    "We will use both **scikit-learn** and **PyTorch**. **GPU support** is enabled for PyTorch examples if available.\n",
    "\n",
    "**Why Deep Learning?**\n",
    "- Automatic feature extraction from raw data.\n",
    "- Scales to large datasets.\n",
    "- Can approximate any continuous function (Universal Approximation Theorem).\n",
    "\n",
    "**Applications in Basic Sciences:**\n",
    "- Physics: Predicting particle trajectories.\n",
    "- Chemistry: Predicting molecular properties.\n",
    "- Biology: Classifying cells in microscopy images.\n",
    "\n",
    "**Quick Links**\n",
    "- [TensorFlow Playground](https://playground.tensorflow.org/)\n",
    "- [3Blue1Brown â€” What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [MIT Lecture: Deep Learning Basics](https://youtu.be/n1ViNeWhC24)\n",
    "- [Distill.pub Momentum Visualization](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Machine Learning to Deep Learning: The Next Step\n",
    "\n",
    "You've already encountered powerful machine learning algorithms like linear/logistic regression and SVMs. These models are excellent for many tasks, but they often require manual **feature engineering**. This means the data scientist must carefully select and craft the input features for the model to work well.\n",
    "\n",
    "Deep learning models, on the other hand, can learn these features automatically. This is achieved through the use of \"deep\" neural networks, which have multiple layers. Each layer learns to recognize progressively more complex features from the data.\n",
    "\n",
    "### Visualizing a Deep Neural Network\n",
    "\n",
    "Here is a simple visualization of a deep network. It has an input layer, two \"hidden\" layers (which makes it *deep*), and an output layer.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Input Layer\n",
    "        I1(Input 1)\n",
    "        I2(Input 2)\n",
    "        I3(...)\n",
    "    end\n",
    "    subgraph Hidden Layer 1\n",
    "        H1_1(Neuron)\n",
    "        H1_2(Neuron)\n",
    "        H1_3(Neuron)\n",
    "    end\n",
    "    subgraph Hidden Layer 2\n",
    "        H2_1(Neuron)\n",
    "        H2_2(Neuron)\n",
    "    end\n",
    "    subgraph Output Layer\n",
    "        O1(Output)\n",
    "    end\n",
    "\n",
    "    I1 --> H1_1; I1 --> H1_2; I1 --> H1_3;\n",
    "    I2 --> H1_1; I2 --> H1_2; I2 --> H1_3;\n",
    "    I3 --> H1_1; I3 --> H1_2; I3 --> H1_3;\n",
    "\n",
    "    H1_1 --> H2_1; H1_1 --> H2_2;\n",
    "    H1_2 --> H2_1; H1_2 --> H2_2;\n",
    "    H1_3 --> H2_1; H1_3 --> H2_2;\n",
    "\n",
    "    H2_1 --> O1;\n",
    "    H2_2 --> O1;\n",
    "```\n",
    "\n",
    "---\n",
    ":::{exercise} Remembering previous cases\n",
    "Think about a dataset you have worked with before. Would a deep learning approach have been beneficial? Why or why not? Consider the size of the dataset, the complexity of the patterns, and whether you had to do a lot of feature engineering.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example shows the power of a deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Non-linear dataset: y = sin(x) + noise\n",
    "np.random.seed(0)\n",
    "X = np.linspace(-2*np.pi, 2*np.pi, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(*X.shape)\n",
    "\n",
    "# Linear regression\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "y_pred_lin = lin_reg.predict(X)\n",
    "\n",
    "# Simple NN\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(20,), activation='tanh', max_iter=5000, random_state=0)\n",
    "mlp.fit(X, y.ravel())\n",
    "y_pred_mlp = mlp.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='gray', alpha=0.5, label='Data')\n",
    "plt.plot(X, y_pred_lin, label='Linear Regression', color='red')\n",
    "plt.plot(X, y_pred_mlp, label='Neural Network', color='blue')\n",
    "plt.legend()\n",
    "plt.title(\"Linear vs Neural Network Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{exercise}\n",
    "Play with the number of hidden layers, and the activation function\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A pytorch implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Toy dataset\n",
    "X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_torch = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 20),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_torch)\n",
    "    loss = loss_fn(y_pred, y_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. The Core of Deep Learning: Deep Neural Networks\n",
    "\n",
    "The process of passing input data through the network to get an output is called **forward propagation**. For each neuron, we calculate a weighted sum of the outputs from the previous layer, add a bias, and then pass this result through a non-linear **activation function**.\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "For a single neuron *j* in layer *l*, its output *a<sub>j</sub><sup>(l)</sup>* is:\n",
    "\n",
    "$z_j^{(l)} = \\sum_k (w_{jk}^{(l)} \\cdot a_k^{(l-1)}) + b_j^{(l)}$\n",
    "\n",
    "$a_j^{(l)} = g(z_j^{(l)})$\n",
    "\n",
    "Where:\n",
    "- $a_k^{(l-1)}$ is the activation of the *k*-th neuron in the previous layer.\n",
    "- $w_{jk}^{(l)}$ is the weight of the connection from neuron *k* to neuron *j*.\n",
    "- $b_j^{(l)}$ is the bias of neuron *j*.\n",
    "- $g$ is the activation function.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "The most common activation functions are Sigmoid, Tanh, and **ReLU (Rectified Linear Unit)**. ReLU is the most popular choice for hidden layers in deep learning today because it helps mitigate a problem called the \"vanishing gradient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the common activation functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z, relu(z))\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z, tanh(z))\n",
    "plt.title('Tanh Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise} Relu activation\n",
    "If a neuron in a hidden layer uses a ReLU activation function and its input *z* is -5, what will be its output? What if the input *z* is 5?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning from Mistakes: Backpropagation\n",
    "\n",
    "How does the network learn the correct values for its weights and biases?\n",
    "\n",
    "1.  It first makes a prediction using **forward propagation**.\n",
    "2.  It measures how wrong that prediction is using a **loss function** (e.g., Mean Squared Error or Cross-Entropy).\n",
    "3.  It calculates the gradient of the loss with respect to every weight and bias in the network. This is done efficiently via an algorithm called **backpropagation**, which is essentially an application of the chain rule from calculus.\n",
    "4.  It uses an **optimizer** (like Gradient Descent) to update the weights and biases in the direction that minimizes the loss.\n",
    "\n",
    "This cycle is repeated many times with the training data. The core of backpropagation is figuring out how much each parameter contributed to the error, and propagating this error information \"backward\" from the output layer to the input layer.\n",
    "\n",
    "The error $\\delta$ in a hidden layer *l* is calculated based on the errors in the next layer *l+1*:\n",
    "\n",
    "$\\delta^{(l)} = ((W^{(l+1)})^T \\delta^{(l+1)}) \\odot g'(z^{(l)})$\n",
    "\n",
    "Where $\\odot$ is element-wise multiplication and $g'(z^{(l)})$ is the derivative of the activation function.\n",
    "\n",
    ":::{exercise} Relu and linear activation\n",
    "Why is the derivative of the activation function ($g'$) important in the backpropagation equation above? What would happen if we used a linear activation function (where $g'(z)$ is just a constant) in all hidden layers?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Applications in Basic Sciences\n",
    "\n",
    "Deep learning is revolutionizing scientific research:\n",
    "\n",
    "-   **Biology:** DeepMind's **AlphaFold** uses a deep learning model to predict the 3D structure of proteins from their amino acid sequence, a grand challenge in biology.\n",
    "-   **Chemistry:** Deep learning can predict the properties of molecules, accelerating drug discovery and materials science.\n",
    "-   **Physics:** At the Large Hadron Collider (LHC), physicists use deep learning to classify particles and find anomalies in the immense stream of data from collisions.\n",
    "-   **Astrophysics:** Neural networks are used to classify galaxies, find exoplanets, and detect gravitational waves in noisy sensor data.\n",
    "\n",
    "The following is some data from physics, to be used as training for a nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Physics: projectile motion\n",
    "def projectile_data(n=100):\n",
    "    g = 9.81\n",
    "    angles = np.random.uniform(20, 70, n) * np.pi / 180\n",
    "    speeds = np.random.uniform(10, 30, n)\n",
    "    t = np.linspace(0, 2, 20)\n",
    "    X_data, y_data = [], []\n",
    "    for v, a in zip(speeds, angles):\n",
    "        x = v*np.cos(a)*t\n",
    "        y_pos = v*np.sin(a)*t - 0.5*g*t**2\n",
    "        mask = y_pos >= 0\n",
    "        X_data.extend(np.column_stack([x[mask], t[mask]]))\n",
    "        y_data.extend(y_pos[mask])\n",
    "    return np.array(X_data), np.array(y_data).reshape(-1,1)\n",
    "\n",
    "X_phys, y_phys = projectile_data(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Exercises & Mini-Project\n",
    "\n",
    "Now, let's put it all into practice. We will use the famous Iris dataset. The goal is to classify a flower as one of three species based on four measurements. We will build a simple Deep Neural Network for this and compare its performance to a classic SVM.\n",
    "\n",
    "**1. Conceptual Exercise: From Logistic Regression to a Neural Network**\n",
    "Explain how a single neuron with a sigmoid activation function is mathematically equivalent to logistic regression.\n",
    "\n",
    "**2. Conceptual Exercise: Choosing the Right Model**\n",
    "You are given a dataset of 500 patient records with 15 features to predict the likelihood of a certain disease. The relationships are expected to be complex but the dataset is small. Would you choose an SVM or a deep neural network as your first model to try? Justify your answer.\n",
    "\n",
    "**3. Conceptual Exercise: Activation Functions in Practice**\n",
    "You are building a deep neural network for classifying images into 10 different categories.\n",
    "   - What activation function would you likely use for the hidden layers? Why?\n",
    "   - What activation function would you use for the output layer? (Hint: The output needs to represent the probability for 10 different classes).\n",
    "\n",
    "**4. Conceptual Exercise: Backpropagation Intuition**\n",
    "If the loss of your neural network is very high after the first few training batches, what does this imply about the initial (randomly set) weights and biases? How does backpropagation help?\n",
    "\n",
    "**5. Mini-Project: Iris Classification - DNN vs. SVM**\n",
    "Follow the code below to load the data, train both an SVM and a DNN, and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import all necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mini-Project Step 1: Load and Prepare the Data ---\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features. This is important for both SVM and Neural Networks.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape)\n",
    "print(\"Test data shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mini-Project Step 2: Train and Evaluate the SVM Model ---\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Support Vector Machine (SVM) Accuracy: {accuracy_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mini-Project Step 3: Build, Train, and Evaluate the Deep Neural Network ---\n",
    "\n",
    "# Build the model using Keras Sequential API\n",
    "# This is a simple DNN with two hidden layers.\n",
    "# Input layer (implicit) -> Dense(10) -> Dense(10) -> Output(3)\n",
    "dnn_model = keras.Sequential([\n",
    "    # Input layer shape is defined by the data, so we specify it in the first layer\n",
    "    keras.layers.Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    \n",
    "    # First hidden layer with 10 neurons and ReLU activation\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    \n",
    "    # Output layer with 3 neurons (one for each Iris class)\n",
    "    # 'softmax' activation is used for multi-class classification to get probability distribution\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# We specify the optimizer, loss function, and metrics to track.\n",
    "dnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, train the DNN model\n",
    "# An \"epoch\" is one full pass through the entire training dataset.\n",
    "# We will use the test set for validation during training to monitor performance.\n",
    "history = dnn_model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "loss, accuracy_dnn = dnn_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nDeep Neural Network (DNN) Accuracy: {accuracy_dnn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of Mini-Project\n",
    "\n",
    "Compare the accuracy of the SVM and the DNN. On a simple, small dataset like Iris, a classic SVM often performs just as well as, or even better than, a neural network. The real power of deep learning becomes apparent with much larger and more complex datasets (e.g., thousands of images, text documents, or scientific sensor data) where the model can learn intricate hierarchical features that would be impossible to engineer by hand.\n",
    "\n",
    "**Experiment further!** Go back and change the DNN architecture. What happens if you:\n",
    "-   Change the number of neurons in the hidden layers (e.g., to 5 or 20)?\n",
    "-   Add another hidden layer?\n",
    "-   Train for more or fewer epochs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
