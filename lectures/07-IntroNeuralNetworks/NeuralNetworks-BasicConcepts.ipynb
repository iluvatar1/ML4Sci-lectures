{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Neural Networks Introduction\n",
    "\n",
    "- <https://ml-visualized.com/chapter4/neural_network.html>\n",
    "- https://www.quantamagazine.org/how-can-ai-id-a-cat-an-illustrated-guide-20250430/\n",
    "\n",
    "An artificial neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. \n",
    "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. Neural networks can adapt to changing input; so the network generates the best possible \n",
    "result without needing to redesign the output criteria. In general, neural networks are useful to approximate non-linear, multi-dimensional functions. Or basically functions where we don't have any info about their functional form just inputs and outputs.\n",
    "Early models resembled the biological neural networks (see the preceptron)\n",
    "\n",
    "[Neural networks](https://en.wikipedia.org/wiki/Neural_network?useskin=vector) are usually arranged into layers, where information passing from layer to the next one firm the first one (input layer), to maybe some intermediate layers (the hidden layers), \n",
    "to the last one (the output layer). For each neuron, the input is a linear combination of the outputs of the previous layer connecting to it, and the neuron output is gien by the so-called \n",
    "activation function. The strenghst or weights of the network connections change dynamically so to approach optimally (given a minimization procedure) the data used during training usually following an algorithm like [back-propagation](https://en.wikipedia.org/wiki/Backpropagation?useskin=vector).  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\" alt=\"Image Description\" width=\"600\">\n",
    "    <figcaption>From: \"https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\"</figcaption>\n",
    "</div>\n",
    "\n",
    "![Neural network with several layeres (Credit http://alexlenail.me/NN-SVG/index.html)](fig/nn.svg)\n",
    "\n",
    "The input for each network is computed as \n",
    "$$\n",
    "z = b + \\sum_i w_i X_i,\n",
    "$$\n",
    "where $b$ is the bias, $w_i$ are the weights, and $X_i$ are the outputs of the neurons in the previous layer and connected t =o this one. Then , the output for this neuron is computed aas\n",
    "$$\n",
    "X = f(z),\n",
    "$$ \n",
    "where $f$ is some [activation function](https://en.wikipedia.org/wiki/Activation_function?useskin=vector). Examples for [activations funtions](https://en.wikipedia.org/wiki/Activation_function?useskin=vector#Folding_activation_functions) are\n",
    "- ReLu\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- Linear\n",
    "- ...\n",
    "\n",
    "The evolution of the training process is measured in *epochs* (an iteration, basically). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "These are some recommended tools to get familiar with neural networks:\n",
    "- A visual introduction: https://www.youtube.com/watch?v=UOvPeC8WOt8\n",
    "- But what is a neural network? | Chapter 1, Deep learning: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "- Neural netoworks full playlist: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "- Visualization of a fully connected neural network, version 1: https://www.youtube.com/watch?v=Tsvxx-GGlTg\n",
    "- Watching Neural Networks Learn: https://www.youtube.com/watch?v=TkwXa7Cvfr8\n",
    "- Neural network Visualization: http://alexlenail.me/NN-SVG/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tensorflow playground\n",
    "Now let's play a bit with a neural network: https://playground.tensorflow.org\n",
    "\n",
    "![Neural network with several layeres (Credit http://alexlenail.me/NN-SVG/index.html)](fig/tensorflow-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Some applications to basi sciences\n",
    "\n",
    "### **Biology & Bioinformatics**\n",
    "\n",
    "1.  Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... & Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature, 596*(7873), 583–589. [https://doi.org/10.1038/s41586-021-03819-2](https://doi.org/10.1038/s41586-021-03819-2)\n",
    "\n",
    "2.  Avsec, Ž., Agarwal, V., Visentin, D., Ledsam, J. R., Grinsztajn, L., Kelcy, M., ... & Kundaje, A. (2021). Effective gene expression prediction from DNA sequence using deep learning. *Nature Methods, 18*(10), 1173–1182. <https://doi.org/10.1038/s41592-021-01252-x>\n",
    "\n",
    "3.  Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., ... & Rives, A. (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. *Science, 379*(6637), 1123-1130. [https://doi.org/10.1126/science.ade2574](https://doi.org/10.1126/science.ade2574)\n",
    "\n",
    "4.  Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisen, H. E., ... & Baker, D. (2023). De novo design of protein structure and function with RFdiffusion. *Nature, 620*(7976), 1089–1100. [https://doi.org/10.1038/s41586-023-06415-8](https://doi.org/10.1038/s41586-023-06415-8)\n",
    "\n",
    "### **Chemistry & Materials Science**\n",
    "\n",
    "-  Satorras, R., Hoogeboom, E., & Welling, M. (2021). E(n) equivariant graph neural networks. *Proceedings of the 38th International Conference on Machine Learning, PMLR 139*, 9323-9332. [https://proceedings.mlr.press/v139/satorras21a.html](https://proceedings.mlr.press/v139/satorras21a.html)\n",
    "\n",
    "- Żurański, A. M., Martinez Alvarado, J. I., Shields, B. J., & Doyle, A. G. (2021). Predicting Reaction Yields via Supervised Learning. Accounts of Chemical Research, 54(8), 1856–1865. <https://doi.org/10.1021/acs.accounts.0c00770>\n",
    "\n",
    "- Merchant, A., Batzner, S., Schoenholz, S. S., Aykol, M., Cheon, G., & Cubuk, E. D. (2023). Scaling deep learning for materials discovery. Nature, 624(7990), 80–85. https://doi.org/10.1038/s41586-023-06735-9\n",
    "\n",
    "- Kulik, H. J., & Tiwary, P. (2022). Artificial intelligence in computational materials science. MRS Bulletin, 47(9), 927–929. https://doi.org/10.1557/s43577-022-00431-1\n",
    "\n",
    "### **Physics**\n",
    "\n",
    "- Iten, R., Metger, T., Wilming, H., del Rio, L., & Renner, R. (2020). Discovering Physical Concepts with Neural Networks. Physical Review Letters, 124(1). https://doi.org/10.1103/physrevlett.124.010508\n",
    "\n",
    "- Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., & Yang, L. (2021). Physics-informed machine learning. Nature Reviews Physics, 3(6), 422–440. https://doi.org/10.1038/s42254-021-00314-5\n",
    "\n",
    "- Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., & Hoyer, S. (2021). Machine learning–accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21). https://doi.org/10.1073/pnas.2101784118\n",
    "\n",
    "- Villaescusa-Navarro, F., Anglés-Alcázar, D., Genel, S., Spergel, D. N., S. Somerville, R., Dave, R., Pillepich, A., Hernquist, L., Nelson, D., Torrey, P., Narayanan, D., Li, Y., Philcox, O., La Torre, V., Maria Delgado, A., Ho, S., Hassan, S., Burkhart, B., Wadekar, D., … Bryan, G. L. (2021). The CAMELS Project: Cosmology and Astrophysics with Machine-learning Simulations. The Astrophysical Journal, 915(1), 71. https://doi.org/10.3847/1538-4357/abf7ba\n",
    "\n",
    "- Hartnett, G. S., Parker, E., & Geist, E. (2018). Replica symmetry breaking in bipartite spin glasses and neural networks. Physical Review E, 98(2). https://doi.org/10.1103/physreve.98.022116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Simple neural network: A perceptron\n",
    "Based on https://www.youtube.com/watch?v=kft1AJ9WVDk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is what we want to train our neural network with:\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/inputs.png)\n",
    "\n",
    "And we want to predict the new output (try to guess the rule)\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/newoutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural network that we are going to use (you can also use http://alexlenail.me/NN-SVG/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nnv import NNV\n",
    "\n",
    "layersList = [\n",
    "    {\"title\":\"input\", \"units\": 3, \"color\": \"darkBlue\"},\n",
    "    {\"title\":\"hidden 1\\n(sigmoid)\", \"units\": 1, \"edges_color\":\"red\", \"edges_width\":2},\n",
    "    {\"title\":\"output\\n(sigmoid)\", \"units\": 1,\"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand better the training, let's show explicitly the weights\n",
    "![weightds](fig/neuralnetwork/weights.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\phi$ is called the activation function, and there are several proposals to it. We will use a sigmoid function\n",
    "$$\n",
    "f(x) = \\dfrac{1}{1+\\exp(-x)},\n",
    "$$\n",
    "where $x = \\sum x_i w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_context('poster')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "xdata = np.linspace(-6.0, 6.0, 100)\n",
    "plt.plot(xdata, sigmoid(xdata))\n",
    "# Highlight x=0 and y=0 axes\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=2.5)  # Horizontal line at y=0\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=2.5)  # Vertical line at x=0\n",
    "# Add labels to the x-axis and y-axis\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(rf\"sigmoid(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Basic Implementation\n",
    "For this very basic nn, we will:\n",
    "- set the input or start of the algorithm:\n",
    "    + Random weights $w_i$\n",
    "    + Set the training inputs and outputs\n",
    "- Create an iteration function to perform the training for `nsteps` (initially 1)\n",
    "\n",
    "The we just iterate once and check what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def get_training_inputs():\n",
    "    return np.array([[0, 0, 1],\n",
    "                     [1, 1, 1], \n",
    "                     [1, 0, 1],\n",
    "                     [0, 1, 1]])\n",
    "\n",
    "def get_training_outputs():\n",
    "    return np.array([0, 1, 1, 0]).reshape(4, 1)\n",
    "\n",
    "def get_init_weights():\n",
    "    \"\"\"\n",
    "    Initially, simply return random weights in [-1, 1)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(-1.0, 1.0, size=(3, 1))\n",
    "\n",
    "def training_one_step(training_inputs, training_outputs, initial_weights):\n",
    "    # Forward pass\n",
    "    # iter only once\n",
    "    input_layer = training_inputs\n",
    "    outputs = sigmoid(np.dot(input_layer, initial_weights))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()\n",
    "print(inputs_t)\n",
    "print(outputs_t)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = training_one_step(inputs_t, outputs_t, weights)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after one step training:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Improving the training\n",
    "These results are not optimal, and depend a lot on the initial weights. Also, we are not yet comparing with the expecting output for the training data. We are now going to include it and add correction terms to the weights, so we will be using back-propagation. Our algorithm is now:\n",
    "- Take each input from the training data.\n",
    "- Compute the error, i.e. the difference between the output and the expected one, `output - expectedoutput`, $\\Delta = \\hat y - y$. \n",
    "- According to the error, adjust the weights\n",
    "- Repeat this many times, hopefully getting convergence , and also being able to apply our nn to new cases not used already.\n",
    "\n",
    "But how to adjust the weights? There are several techniques based on the actual error $\\Delta$. \n",
    "\n",
    "First, we will define a cost/loss function as the typical MSE\n",
    "\\begin{equation}\n",
    "COST = \\frac{1}{2} (\\hat y - y)^2,\n",
    "\\end{equation}\n",
    "where $\\hat y$ is the predicted value and $y$ is the expected one.  Notice that we are not adding up because we only have one output. \n",
    "\n",
    "Second, we will apply gradient descent approximation to our problem, in order to improve our coefficients for our data. This means that we will propagate back the errors into the coefficients!\n",
    "\\begin{equation}\n",
    "w_i' = w_i - \\alpha \\frac{\\partial COST}{\\partial w_i},\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the learning rate. Taking into account that \n",
    "\\begin{equation}\n",
    "\\hat y = \\phi(z) = \\frac{1}{1 + \\exp(-z)},\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "z = \\sum w_i x_i,\n",
    "\\end{equation}\n",
    "then, by using the chain rule, we have\n",
    "\\begin{equation}\n",
    "\\frac{\\partial COST}{\\partial w_i} = \\frac{\\partial COST}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z} \\frac{\\partial z}{\\partial w_i} = (\\hat y - y)\\phi (1-\\phi)x_i. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def train_nn(training_inputs, training_outputs, initial_weights, niter, errors_data, alpha = 1.0):\n",
    "    \"\"\"\n",
    "    training_inputs: asdasdasda\n",
    "    ...\n",
    "    errors_data: output - stores the errors per iteration\n",
    "    \"\"\"\n",
    "    w = initial_weights\n",
    "    for ii in range(niter):\n",
    "        # Forward propagation\n",
    "        input_layer = training_inputs\n",
    "        outputs = sigmoid(np.dot(input_layer, w))\n",
    "        # Backward propagation\n",
    "        errors = outputs - training_outputs\n",
    "        deltaw = errors*sigmoid_prime(outputs)\n",
    "        deltaw = np.dot(input_layer.T, deltaw)\n",
    "        w = w - alpha*deltaw\n",
    "        # Save errors for plotting later\n",
    "        errors_data[ii] = errors.reshape((4,))\n",
    "    return outputs, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITER = 50000\n",
    "errors = np.zeros((NITER, 4))\n",
    "outputs, weights = train_nn(inputs_t, outputs_t, weights, NITER, errors, alpha=0.9)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after training:\")\n",
    "print(outputs)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax[0].plot(range(NITER), errors)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Errors\")\n",
    "ax[1].loglog(range(NITER), np.abs(errors))\n",
    "ax[1].set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our network is very well trained, But how does it perform with a new input? let's check with `[1, 0, 0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weights)\n",
    "#print(weights.shape)\n",
    "input_new = np.array([1, 0, 0]).reshape(3, 1)\n",
    "#print(input_new)\n",
    "#print(input_new.shape)\n",
    "#print(np.sum(weights*input_new))\n",
    "print(sigmoid(np.sum(weights*input_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is basically one, as expected.\n",
    "There are more topics related to this that we have not used, like more layers, more neurons per hidden layer, bias on the activation function, and a lot of other details, but hopefully you now see how a neural network works on the core.\n",
    "\n",
    "Recommended lectures:\n",
    "- 3blue1brown Neural Networks: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "- Neural networks from scratch: https://www.youtube.com/watch?v=9RN2Wr8xvro\n",
    "- Backprop basic: https://www.youtube.com/watch?v=wqPt3qjB6uA\n",
    "- https://www.youtube.com/watch?v=khUVIZ3MON8&t=0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
