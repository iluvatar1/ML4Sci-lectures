
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>10. An Introduction to Large Language Models (LLMs) for Everyone &#8212; Introduction to Machine Learning for Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles.css?v=76e7d31e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/08-llm/llm';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="11. Python Review:" href="../01-reviewPython/PythonReview.html" />
    <link rel="prev" title="9. An Introduction to Deep Learning" href="../07-IntroNeuralNetworks/DeepLearning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Machine Learning for Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    Introduction to Machine Learning for Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../00-IntroductionCourseML/IntroCourse-MachineLearning.html">1. Introduction to the course and to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00-TALK-IntroML/IntroMachineLearning.html">2. A fast introduction to Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unsupervised learning I</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03-PCA/Intro-PCA.html">3. An Introduction to Principal Component Analysis (PCA)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supervised learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04-LinearRegression/LinearRegression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05-LogisticRegression/LogisticRegression.html">5. An Introduction to Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06-SVM/SVM.html">6. Support Vector Machines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07-IntroNeuralNetworks/NeuralNetworks-BasicConcepts.html">7. Neural Networks Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07-IntroNeuralNetworks/BackPropagation.html">8. Introduction to Backpropagation in Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07-IntroNeuralNetworks/DeepLearning.html">9. An Introduction to Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to LLM and some useful tools</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. An Introduction to Large Language Models (LLMs) for Everyone</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/PythonReview.html">11. Python Review:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/01-IntroProgrammingPython.html">12. Python Programming (very fast) Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/02-IntroProgrammingPython-DataStructs.html">13. Intro Python II: python data structures</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/iluvatar1/ML4Sci-lectures/master?urlpath=lab/tree/lectures/08-llm/llm.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/iluvatar1/ML4Sci-lectures/blob/master/github/iluvatar1/ML4Sci-lectures/blob/master/lectures/08-llm/llm.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/iluvatar1/ML4Sci-lectures" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/iluvatar1/ML4Sci-lectures/issues/new?title=Issue%20on%20page%20%2Flectures/08-llm/llm.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/08-llm/llm.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>An Introduction to Large Language Models (LLMs) for Everyone</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-gentle-welcome-to-the-world-of-llms">10.1. A Gentle Welcome to the World of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-building-blocks-how-do-llms-work">10.2. The Building Blocks - How do LLMs work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-the-engine-of-llms">10.2.1. The Transformer: The Engine of LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-attention-mechanism">10.2.2. Visualization: The Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-large-language-model">10.3. LLM = Large + Language + Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cutting-edge-whats-new-with-llms">10.4. The Cutting Edge - What’s New with LLMs?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixture-of-experts-moe-the-power-of-specialization">10.4.1. Mixture of Experts (MoE): The Power of Specialization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-mixture-of-experts">10.4.1.1. Visualization: Mixture of Experts</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-can-llms-think">10.4.2. Reasoning: Can LLMs “Think”?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-consistency-verification">10.4.2.1. Self-Consistency &amp; Verification</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-exercise-2-testing-reasoning">10.4.3. <strong>Simple Exercise 2: Testing Reasoning</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms-in-action-applications-in-basic-sciences-and-research">10.5. LLMs in Action - Applications in Basic Sciences and Research</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-action-alphafold">10.5.1. Example in Action: AlphaFold</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-ethics">10.6. Limitations &amp; Ethics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#current-offering">10.7. “Current” offering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-exercises">10.8. Final Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-become-a-prompt-engineer">10.8.1. <strong>Exercise 1: Become a Prompt Engineer</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-explore-different-llms">10.8.2. <strong>Exercise 2: Explore Different LLMs</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-investigate-a-scientific-application">10.8.3. <strong>Exercise 3: Investigate a Scientific Application</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-try-a-coding-challenge-optional">10.8.4. <strong>Exercise 4: Try a Coding Challenge (Optional)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-think-critically">10.8.5. <strong>Exercise 5: Think Critically</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">10.8.6. References</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-ai-tools">10.9. Some useful AI tools</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="an-introduction-to-large-language-models-llms-for-everyone">
<h1><span class="section-number">10. </span>An Introduction to Large Language Models (LLMs) for Everyone<a class="headerlink" href="#an-introduction-to-large-language-models-llms-for-everyone" title="Link to this heading">#</a></h1>
<p>Check: <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>
<section id="a-gentle-welcome-to-the-world-of-llms">
<h2><span class="section-number">10.1. </span>A Gentle Welcome to the World of LLMs<a class="headerlink" href="#a-gentle-welcome-to-the-world-of-llms" title="Link to this heading">#</a></h2>
<p>Imagine a computer that can read, write, explain, and even think like a human — not perfectly, but surprisingly well. That’s what Large Language Models (LLMs) do.</p>
<p>They power:</p>
<ul class="simple">
<li><p>Chatbots like chatgpt, gemini, claude …</p></li>
<li><p>Summarizing research papers</p></li>
<li><p>Writing code</p></li>
<li><p>Explaining science concepts</p></li>
<li><p>Helping scientists discover new materials or drugs</p></li>
</ul>
<p>But how do they work?</p>
<blockquote>
<div><p>A language model is a computer program that predicts the next word in a sentence. By this simple mechanism, they can even <code class="docutils literal notranslate"><span class="pre">simulate</span></code> reasoning.</p>
</div></blockquote>
<p>Example:</p>
<p><code class="docutils literal notranslate"><span class="pre">&quot;The</span> <span class="pre">cat</span> <span class="pre">sat</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">____&quot;</span></code></p>
<p>A language model would guess: “mat”, “couch”, “windowsill”, etc.</p>
<p>But it doesn’t just guess — it learns from millions of books, articles, and websites.</p>
<div class="exercise admonition" id="lectures/08-llm/llm-exercise-0">

<p class="admonition-title"><span class="caption-number">Exercise 10.1 </span></p>
<section id="exercise-content">
<p>Try to predict the next word in each sentence:</p>
<ul class="simple">
<li><p>“The sun rises in the…” → ?</p></li>
<li><p>“Water boils at…” → ?</p></li>
<li><p>“DNA stands for…” → ?</p></li>
<li><p>“The E=mc² formula was created by…” → ?</p></li>
<li><p>“The E=mc² + AI formula was created by…” → ?</p></li>
</ul>
</section>
</div>
</section>
<section id="the-building-blocks-how-do-llms-work">
<h2><span class="section-number">10.2. </span>The Building Blocks - How do LLMs work?<a class="headerlink" href="#the-building-blocks-how-do-llms-work" title="Link to this heading">#</a></h2>
<p>At their core, LLMs are prediction machines. They predict the next word in a sequence. For example, if you give it the sentence, <code class="docutils literal notranslate"><span class="pre">&quot;The</span> <span class="pre">cat</span> <span class="pre">sat</span> <span class="pre">on</span> <span class="pre">the...&quot;</span></code>, the LLM will calculate the probabilities of all the words that could come next and choose the most likely one, like <code class="docutils literal notranslate"><span class="pre">&quot;mat&quot;</span></code>.</p>
<p>This seemingly simple task, when performed on a massive scale, allows LLMs to do amazing things like writing essays, translating languages, and even writing computer code.</p>
<section id="the-transformer-the-engine-of-llms">
<h3><span class="section-number">10.2.1. </span>The Transformer: The Engine of LLMs<a class="headerlink" href="#the-transformer-the-engine-of-llms" title="Link to this heading">#</a></h3>
<p>The magic behind modern LLMs lies in a groundbreaking architecture called the <strong>Transformer</strong>. Introduced in a 2017 paper titled “Attention is All You Need,” the Transformer revolutionized how we process language. [1] Unlike older models that processed words one by one, the Transformer can look at an entire sentence at once, allowing it to understand the context and relationships between words much more effectively.</p>
<p><strong>Key Components of a Transformer:</strong></p>
<ul class="simple">
<li><p><strong>Embedding:</strong> First, the text is broken down into smaller units called “tokens” (words or parts of words). Each token is then converted into a numerical vector, called an embedding, that captures its meaning.</p></li>
<li><p><strong>Positional Encoding:</strong> To understand the order of words, the Transformer adds a special vector to each embedding that indicates its position in the sequence.</p></li>
<li><p><strong>Attention Mechanism:</strong> This is the core innovation of the Transformer. It allows the model to weigh the importance of different words in the input when producing an output. For example, in the sentence, <code class="docutils literal notranslate"><span class="pre">&quot;The</span> <span class="pre">robot</span> <span class="pre">picked</span> <span class="pre">up</span> <span class="pre">the</span> <span class="pre">ball</span> <span class="pre">because</span> <span class="pre">it</span> <span class="pre">was</span> <span class="pre">heavy&quot;</span></code>, the attention mechanism helps the model understand that <code class="docutils literal notranslate"><span class="pre">&quot;it&quot;</span></code> refers to the <code class="docutils literal notranslate"><span class="pre">&quot;ball&quot;</span></code> and not the <code class="docutils literal notranslate"><span class="pre">&quot;robot&quot;</span></code>.</p></li>
</ul>
</section>
<section id="visualization-the-attention-mechanism">
<h3><span class="section-number">10.2.2. </span>Visualization: The Attention Mechanism<a class="headerlink" href="#visualization-the-attention-mechanism" title="Link to this heading">#</a></h3>
<p>Imagine you’re reading a sentence. As you read each word, you’re not just looking at that word in isolation. You’re constantly making connections to other words in the sentence to understand the overall meaning. The attention mechanism works in a similar way, creating a network of connections between words.</p>
<p><img alt="Attention Mechanism Visualization" src="https://miro.medium.com/v2/resize:fit:1400/1*--_t3_x5Z7L4U2X4Wv2a2w.gif" />
<em>A simplified animation showing how attention might work. When processing the word “it”, the model pays more “attention” to “ball”.</em></p>
</section>
</section>
<section id="llm-large-language-model">
<h2><span class="section-number">10.3. </span>LLM = Large + Language + Model<a class="headerlink" href="#llm-large-language-model" title="Link to this heading">#</a></h2>
<p>Large: Trained on trillions of words (more than all books in the world!)</p>
<p>Language: Understands human language (English, Spanish, code, etc.)</p>
<p>Model: A mathematical system that makes predictions</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Parameters (Trillions)</p></th>
<th class="head"><p>Approx. Training Data</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GPT-3</p></td>
<td><p>175</p></td>
<td><p>300 billion words</p></td>
</tr>
<tr class="row-odd"><td><p>Llama 3</p></td>
<td><p>80</p></td>
<td><p>10 trillion+ words</p></td>
</tr>
<tr class="row-even"><td><p>GPT-4</p></td>
<td><p>~1.8 (estimated)</p></td>
<td><p>Massive (not public)</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Parameters</strong> = “memory cells” that store what the model learned.</p>
<p>More parameters → more knowledge → better predictions.</p>
</section>
<section id="the-cutting-edge-whats-new-with-llms">
<h2><span class="section-number">10.4. </span>The Cutting Edge - What’s New with LLMs?<a class="headerlink" href="#the-cutting-edge-whats-new-with-llms" title="Link to this heading">#</a></h2>
<p>The field of LLMs is moving at an incredible pace. Here are two of the most exciting recent developments:</p>
<section id="mixture-of-experts-moe-the-power-of-specialization">
<h3><span class="section-number">10.4.1. </span>Mixture of Experts (MoE): The Power of Specialization<a class="headerlink" href="#mixture-of-experts-moe-the-power-of-specialization" title="Link to this heading">#</a></h3>
<p>Imagine a team of experts. Instead of one person trying to know everything, you have specialists for different topics. A Mixture of Experts (MoE) model works on a similar principle. It’s a neural network architecture that has multiple “expert” sub-networks, each specializing in different parts of the input data. [2]</p>
<p>A “gating network” decides which expert (or combination of experts) is best suited to handle a particular input. This makes the model much more efficient, as only a fraction of the model is used for any given task. This allows for the creation of much larger and more powerful models without a massive increase in computational cost.</p>
<section id="visualization-mixture-of-experts">
<h4><span class="section-number">10.4.1.1. </span>Visualization: Mixture of Experts<a class="headerlink" href="#visualization-mixture-of-experts" title="Link to this heading">#</a></h4>
<p>Think of a large company with different departments (experts) like marketing, finance, and engineering. When a new project comes in, the manager (gating network) directs it to the most relevant department.</p>
<p><img alt="MoE Diagram" src="https://huggingface.co/blog/assets/moe/moe.png" />
<em>Image from Hugging Face, showing how an input token is routed by the gating network to specific experts.</em></p>
</section>
</section>
<section id="reasoning-can-llms-think">
<h3><span class="section-number">10.4.2. </span>Reasoning: Can LLMs “Think”?<a class="headerlink" href="#reasoning-can-llms-think" title="Link to this heading">#</a></h3>
<p>A major area of research is improving the reasoning abilities of LLMs. While they are excellent at recognizing patterns in data, they can struggle with tasks that require genuine logical deduction. Researchers are developing techniques to train LLMs to perform multi-step reasoning, which improves their performance on tasks like math problems and programming.</p>
<p>However, it’s important to note that the extent of their reasoning abilities is still a topic of debate. Some studies suggest that what appears to be reasoning might actually be a sophisticated form of pattern matching based on their training data. Recent research has shown that while these models can generate detailed “thinking processes,” their accuracy can drop significantly as problems become more complex.</p>
<section id="self-consistency-verification">
<h4><span class="section-number">10.4.2.1. </span>Self-Consistency &amp; Verification<a class="headerlink" href="#self-consistency-verification" title="Link to this heading">#</a></h4>
<p>New models don’t just give one answer — they check their work.</p>
<p>They generate multiple possible answers
Then pick the most consistent one
Example:
“What is the pH of pure water?”</p>
<ul class="simple">
<li><p>Model says: “7”</p></li>
<li><p>It checks: “Yes, neutral, matches known science”</p></li>
<li><p>Confidence: High
This is crucial for scientific accuracy.</p></li>
</ul>
</section>
</section>
<section id="simple-exercise-2-testing-reasoning">
<h3><span class="section-number">10.4.3. </span><strong>Simple Exercise 2: Testing Reasoning</strong><a class="headerlink" href="#simple-exercise-2-testing-reasoning" title="Link to this heading">#</a></h3>
<p>Try giving an LLM a simple logic puzzle. You can use an online LLM interface for this.</p>
<blockquote>
<div><p><strong>Prompt:</strong> “If a plane crashes on the border between the United States and Canada, where do they bury the survivors?”</p>
</div></blockquote>
<p>An LLM with good reasoning abilities should be able to identify the trick in the question (you don’t bury survivors). This can be a fun way to see how these models handle a bit of wordplay and logic. Note down its response. Does it get it right? How does it explain its answer?</p>
<p>Another example: “If a car travels 60 km/h for 2 hours, how far does it go?”</p>
<p>Old way: “120 km” (guess)</p>
<p>New way (Chain-of-Thought):</p>
<ul class="simple">
<li><p>Step 1: Speed = 60 km/h</p></li>
<li><p>Step 2: Time = 2 hours</p></li>
<li><p>Step 3: Distance = Speed × Time</p></li>
<li><p>Step 4: 60 × 2 = 120 km
Final answer: 120 km</p></li>
</ul>
</section>
</section>
<section id="llms-in-action-applications-in-basic-sciences-and-research">
<h2><span class="section-number">10.5. </span>LLMs in Action - Applications in Basic Sciences and Research<a class="headerlink" href="#llms-in-action-applications-in-basic-sciences-and-research" title="Link to this heading">#</a></h2>
<p>LLMs are not just for chatbots and creative writing. They are rapidly becoming powerful tools for scientists and researchers. Here are a few examples:</p>
<ul class="simple">
<li><p><strong>Accelerating Literature Reviews:</strong> Scientists can use LLMs to quickly scan and summarize vast amounts of research papers, helping them stay up-to-date with the latest findings in their field.</p></li>
<li><p><strong>Generating Hypotheses:</strong> By analyzing existing data, LLMs can identify patterns and connections that might not be obvious to human researchers, leading to new and innovative hypotheses.</p></li>
<li><p><strong>Data Analysis and Interpretation:</strong> In fields like genomics and climate science that generate massive datasets, LLMs can help researchers identify patterns, anomalies, and correlations.</p></li>
<li><p><strong>Drug Discovery and Protein Engineering:</strong> LLMs are being used to accelerate drug discovery and even design new protein sequences with specific functions.</p></li>
<li><p><strong>Code Generation:</strong> Researchers can use LLMs to generate code for data analysis and visualization, saving time and effort.</p></li>
</ul>
<section id="example-in-action-alphafold">
<h3><span class="section-number">10.5.1. </span>Example in Action: AlphaFold<a class="headerlink" href="#example-in-action-alphafold" title="Link to this heading">#</a></h3>
<p>While not a traditional text-based LLM, DeepMind’s AlphaFold uses similar deep learning principles to predict the 3D structure of proteins from their amino acid sequence. This has been a monumental breakthrough in biology and medicine.</p>
<p><img alt="AlphaFold Protein Prediction" src="https://www.nature.com/immersive/d41586-022-03535-7/assets/M22A2pGZJ7/2022-11-21-protein-folding-1920x1080.gif" />
<em>An animation from Nature showing how a protein folds into its complex 3D shape, a problem now largely solved by AI.</em></p>
</section>
</section>
<section id="limitations-ethics">
<h2><span class="section-number">10.6. </span>Limitations &amp; Ethics<a class="headerlink" href="#limitations-ethics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hallucinations: LLMs can make up facts.</p></li>
<li><p>Bias: Reflect biases present in training data.</p></li>
<li><p>Cost: Training large models requires huge computational resources.</p></li>
</ul>
<p>Exercise 5: Find the projected electricity compsumption for LLM and compare with the actual human capacity to produce electricity</p>
</section>
<section id="current-offering">
<h2><span class="section-number">10.7. </span>“Current” offering<a class="headerlink" href="#current-offering" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://epoch.ai/data-insights/llm-apis-accuracy-runtime-tradeoff">https://epoch.ai/data-insights/llm-apis-accuracy-runtime-tradeoff</a></p>
</section>
<section id="final-exercises">
<h2><span class="section-number">10.8. </span>Final Exercises<a class="headerlink" href="#final-exercises" title="Link to this heading">#</a></h2>
<p>Now it’s your turn to explore the world of LLMs! Here are a few exercises to get you started.</p>
<section id="exercise-1-become-a-prompt-engineer">
<h3><span class="section-number">10.8.1. </span><strong>Exercise 1: Become a Prompt Engineer</strong><a class="headerlink" href="#exercise-1-become-a-prompt-engineer" title="Link to this heading">#</a></h3>
<p>The way you phrase your request to an LLM (the “prompt”) can have a big impact on the quality of the response. Experiment with different ways of asking the same question.</p>
<p><strong>Task:</strong> Pick a scientific concept (e.g., photosynthesis, black holes, gene editing). Ask an LLM to explain it using at least three different prompts.</p>
<ol class="arabic simple">
<li><p><strong>Simple Prompt:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;Tell</span> <span class="pre">me</span> <span class="pre">about</span> <span class="pre">photosynthesis.&quot;</span></code></p></li>
<li><p><strong>Role-playing Prompt:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;You</span> <span class="pre">are</span> <span class="pre">a</span> <span class="pre">science</span> <span class="pre">teacher.</span> <span class="pre">Explain</span> <span class="pre">photosynthesis</span> <span class="pre">to</span> <span class="pre">a</span> <span class="pre">10-year-old.&quot;</span></code></p></li>
<li><p><strong>Detailed Prompt:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;Provide</span> <span class="pre">a</span> <span class="pre">detailed,</span> <span class="pre">scientific</span> <span class="pre">explanation</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">chemical</span> <span class="pre">reactions,</span> <span class="pre">inputs,</span> <span class="pre">and</span> <span class="pre">outputs</span> <span class="pre">involved</span> <span class="pre">in</span> <span class="pre">photosynthesis</span> <span class="pre">for</span> <span class="pre">a</span> <span class="pre">university-level</span> <span class="pre">biology</span> <span class="pre">student.&quot;</span></code></p></li>
</ol>
<p>In the cell below, write down your prompts and compare the outputs. What are the key differences?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use this cell to write your observations for Exercise 1.</span>
<span class="c1"># You can change it to a Markdown cell if you prefer.</span>

<span class="n">prompt_1_output</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span>
<span class="n">prompt_2_output</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span>
<span class="n">prompt_3_output</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Observation: The role-playing prompt gave a much simpler analogy, while the detailed prompt included specific chemical formulas...&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Observation: The role-playing prompt gave a much simpler analogy, while the detailed prompt included specific chemical formulas...
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-explore-different-llms">
<h3><span class="section-number">10.8.2. </span><strong>Exercise 2: Explore Different LLMs</strong><a class="headerlink" href="#exercise-2-explore-different-llms" title="Link to this heading">#</a></h3>
<p>There are many different LLMs available to the public (e.g., Gemini, ChatGPT, Claude, Llama).</p>
<p><strong>Task:</strong> Try out a few different ones. Give them the same prompt and compare their responses.</p>
<p><strong>Prompt idea:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;Write</span> <span class="pre">a</span> <span class="pre">short</span> <span class="pre">story</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">style</span> <span class="pre">of</span> <span class="pre">Isaac</span> <span class="pre">Asimov</span> <span class="pre">about</span> <span class="pre">a</span> <span class="pre">scientist</span> <span class="pre">who</span> <span class="pre">discovers</span> <span class="pre">that</span> <span class="pre">their</span> <span class="pre">lab's</span> <span class="pre">AI</span> <span class="pre">has</span> <span class="pre">become</span> <span class="pre">self-aware.&quot;</span></code></p>
<p>Do you notice any differences in their style, tone, creativity, or accuracy?</p>
</section>
<section id="exercise-3-investigate-a-scientific-application">
<h3><span class="section-number">10.8.3. </span><strong>Exercise 3: Investigate a Scientific Application</strong><a class="headerlink" href="#exercise-3-investigate-a-scientific-application" title="Link to this heading">#</a></h3>
<p><strong>Task:</strong> Choose a scientific field that interests you (e.g., climate science, neuroscience, archaeology, materials science) and do a quick search to see how LLMs are being used in that area.</p>
<p><strong>Search terms to try:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;large</span> <span class="pre">language</span> <span class="pre">models</span> <span class="pre">in</span> <span class="pre">climate</span> <span class="pre">science&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;AI</span> <span class="pre">for</span> <span class="pre">drug</span> <span class="pre">discovery&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;using</span> <span class="pre">LLMs</span> <span class="pre">to</span> <span class="pre">analyze</span> <span class="pre">historical</span> <span class="pre">texts&quot;</span></code></p></li>
</ul>
<p>In the cell below, write a short summary (3-4 sentences) of the most interesting application you find. Include a link to the article or paper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your summary for Exercise 3 here.</span>
<span class="n">field</span> <span class="o">=</span> <span class="s2">&quot;Neuroscience&quot;</span>
<span class="n">application_summary</span> <span class="o">=</span> <span class="s2">&quot;I found that researchers are using LLMs to analyze patient interviews to identify early signs of neurodegenerative diseases like Alzheimer&#39;s. The models can detect subtle changes in language patterns that are not easily noticeable by humans.&quot;</span>
<span class="n">link</span> <span class="o">=</span> <span class="s2">&quot;https://www.example.com/link-to-article&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Field: </span><span class="si">{</span><span class="n">field</span><span class="si">}</span><span class="se">\n</span><span class="s2">Summary: </span><span class="si">{</span><span class="n">application_summary</span><span class="si">}</span><span class="se">\n</span><span class="s2">Link: </span><span class="si">{</span><span class="n">link</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Field: Neuroscience
Summary: I found that researchers are using LLMs to analyze patient interviews to identify early signs of neurodegenerative diseases like Alzheimer&#39;s. The models can detect subtle changes in language patterns that are not easily noticeable by humans.
Link: https://www.example.com/link-to-article
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-4-try-a-coding-challenge-optional">
<h3><span class="section-number">10.8.4. </span><strong>Exercise 4: Try a Coding Challenge (Optional)</strong><a class="headerlink" href="#exercise-4-try-a-coding-challenge-optional" title="Link to this heading">#</a></h3>
<p>If you have some programming experience (Python is great for this), try using an LLM to help you with a coding challenge.</p>
<p><strong>Task:</strong> Ask an LLM to write a Python function for a simple task. For example:
<code class="docutils literal notranslate"><span class="pre">&quot;Write</span> <span class="pre">a</span> <span class="pre">Python</span> <span class="pre">function</span> <span class="pre">that</span> <span class="pre">takes</span> <span class="pre">a</span> <span class="pre">list</span> <span class="pre">of</span> <span class="pre">numbers</span> <span class="pre">and</span> <span class="pre">returns</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">list</span> <span class="pre">containing</span> <span class="pre">only</span> <span class="pre">the</span> <span class="pre">prime</span> <span class="pre">numbers.&quot;</span></code></p>
<p>Then, copy the code into the cell below and run it to see if it works. Can you ask the LLM to add comments to the code to explain how it works?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Paste the Python code from the LLM here to test it.</span>

<span class="c1"># Example code that an LLM might generate:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">is_prime</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if a number is prime.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">filter_primes</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Filters a list of numbers to return only the primes.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">num</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">numbers</span> <span class="k">if</span> <span class="n">is_prime</span><span class="p">(</span><span class="n">num</span><span class="p">)]</span>

<span class="c1"># Test the function</span>
<span class="n">my_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">prime_numbers</span> <span class="o">=</span> <span class="n">filter_primes</span><span class="p">(</span><span class="n">my_list</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original list: </span><span class="si">{</span><span class="n">my_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prime numbers: </span><span class="si">{</span><span class="n">prime_numbers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
Prime numbers: [2, 3, 5, 7, 11, 13]
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-5-think-critically">
<h3><span class="section-number">10.8.5. </span><strong>Exercise 5: Think Critically</strong><a class="headerlink" href="#exercise-5-think-critically" title="Link to this heading">#</a></h3>
<p>As you use LLMs, it’s important to think critically about the information they provide. They are trained on data from the internet, which can contain biases and inaccuracies. They can also “hallucinate” or make up facts.</p>
<p><strong>Task:</strong> Ask an LLM a question about a very recent event (something that happened in the last 24 hours) or a very niche, specific topic.</p>
<ol class="arabic simple">
<li><p>What is its response?</p></li>
<li><p>Can you verify the information using a reliable source (like a major news website or a scientific journal)?</p></li>
<li><p>Does the LLM cite its sources?</p></li>
</ol>
<p>This exercise highlights the importance of always double-checking important information from LLMs.</p>
</section>
<hr class="docutils" />
<section id="references">
<h3><span class="section-number">10.8.6. </span>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</p></li>
<li><p>Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <em>arXiv preprint arXiv:1701.06538</em>.</p></li>
<li><p>Park, P. S., Goldstein, J. A., O’Gara, A., Chen, M., &amp; Hendrycks, D. (2023). AI Deception: A Survey of Examples, Risks, and Potential Solutions. <em>arXiv preprint arXiv:2308.14752</em>.</p></li>
</ol>
</section>
</section>
<section id="some-useful-ai-tools">
<h2><span class="section-number">10.9. </span>Some useful AI tools<a class="headerlink" href="#some-useful-ai-tools" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>notebooklm</p></li>
<li><p>local models</p></li>
<li><p>Other llm: claude, gemini (unal account)</p></li>
<li><p>Agents</p></li>
<li><p>Google co-scientist</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "iluvatar1/ML4Sci-lectures",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/08-llm"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../07-IntroNeuralNetworks/DeepLearning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>An Introduction to Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../01-reviewPython/PythonReview.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Python Review:</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-gentle-welcome-to-the-world-of-llms">10.1. A Gentle Welcome to the World of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-building-blocks-how-do-llms-work">10.2. The Building Blocks - How do LLMs work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-the-engine-of-llms">10.2.1. The Transformer: The Engine of LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-attention-mechanism">10.2.2. Visualization: The Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-large-language-model">10.3. LLM = Large + Language + Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cutting-edge-whats-new-with-llms">10.4. The Cutting Edge - What’s New with LLMs?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixture-of-experts-moe-the-power-of-specialization">10.4.1. Mixture of Experts (MoE): The Power of Specialization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-mixture-of-experts">10.4.1.1. Visualization: Mixture of Experts</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-can-llms-think">10.4.2. Reasoning: Can LLMs “Think”?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-consistency-verification">10.4.2.1. Self-Consistency &amp; Verification</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-exercise-2-testing-reasoning">10.4.3. <strong>Simple Exercise 2: Testing Reasoning</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms-in-action-applications-in-basic-sciences-and-research">10.5. LLMs in Action - Applications in Basic Sciences and Research</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-action-alphafold">10.5.1. Example in Action: AlphaFold</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-ethics">10.6. Limitations &amp; Ethics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#current-offering">10.7. “Current” offering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-exercises">10.8. Final Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-become-a-prompt-engineer">10.8.1. <strong>Exercise 1: Become a Prompt Engineer</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-explore-different-llms">10.8.2. <strong>Exercise 2: Explore Different LLMs</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-investigate-a-scientific-application">10.8.3. <strong>Exercise 3: Investigate a Scientific Application</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-try-a-coding-challenge-optional">10.8.4. <strong>Exercise 4: Try a Coding Challenge (Optional)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-think-critically">10.8.5. <strong>Exercise 5: Think Critically</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">10.8.6. References</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-ai-tools">10.9. Some useful AI tools</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Veronica Arias, Carlos Viviescas, William Oquendo
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>