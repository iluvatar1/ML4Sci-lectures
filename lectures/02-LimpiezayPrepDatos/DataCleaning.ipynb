{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qqI0xLLLsPhz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Pandas for data cleaning and analysis\n",
    "\n",
    "```{figure} https://pandas.pydata.org/static/img/pandas_secondary.svg\n",
    "---\n",
    "width: 50%\n",
    "align: center\n",
    "name: \n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Pandas has two main data structures:\n",
    "- **`Series`**: A one-dimensional labeled array, like a single column of data.\n",
    "- **`DataFrame`**: A two-dimensional labeled data structure with columns of potentially different types, similar to a spreadsheet or SQL table.\n",
    "\n",
    "## The Dataframe\n",
    "The dataframe is the most important object inside pandas. It allows to represent, access, process, etc multi-dimensional data. \n",
    "\n",
    "![Pandas dataframe](https://www.w3resource.com/w3r_images/pandas-data-structure.svg)\n",
    "\n",
    "Source: https://www.w3resource.com/python-exercises/pandas/index.php\n",
    "\n",
    "![Pandas dataframe example](https://miro.medium.com/max/1400/1*ZSehcrMtBWN7_qCWq_HiSg.png)\n",
    "\n",
    "Source: https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c\n",
    "\n",
    "You can initialize a dataframe in several ways. For example, you can use a dictionary or a nested list. Or you can read from a file, either local or online. \n",
    "For example, you can do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some needed libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[909976, \"Sweden\"],\n",
    "                   [8615246, \"United Kingdom\"],\n",
    "                   [2872086, \"Italy\"],\n",
    "                   [2273305, \"France\"],\n",
    "                   [344444, np.nan]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno # Check https://github.com/ResidentMario/missingno\n",
    "%matplotlib inline\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna() # returns a new view but does not overwrite the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [\"Stockholm\", \"London\", \"Rome\", \"Paris\"]\n",
    "df.columns = [\"Population\", \"State\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas vs. Polars\n",
    "\n",
    "Polars is a newer, extremely fast DataFrame library built in Rust. It's gaining popularity for its performance, especially on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Pandas | Polars |\n",
    "|---|---|---|\n",
    "| **Backend** | Python/NumPy (partially C) | Rust (built on Apache Arrow)  |\n",
    "| **Performance** | Slower, especially on large data.  | Significantly faster (5-100x) due to parallelism and query optimization.  |\n",
    "| **Execution Model** | Eager (executes line-by-line)  | Supports both Eager and Lazy execution (optimizes the whole query before running)  |\n",
    "| **Memory Usage** | Higher memory footprint.  | More memory efficient.  |\n",
    "| **API** | Very flexible, but can be inconsistent (e.g., `inplace`). | More consistent and expressive, encourages method chaining.  |\n",
    "| **Ecosystem** | Mature and extensive. Integrates with almost every data science library (scikit-learn, Matplotlib, etc.). | Growing, but less integrated with the broader ML ecosystem.  |\n",
    "\n",
    "**When to choose which?**\n",
    "- **Pandas**: Excellent for data exploration, smaller datasets (up to a few GB), and projects that need deep integration with libraries like scikit-learn.\n",
    "- **Polars**: Ideal for large datasets, performance-critical data transformations, and building data pipelines where speed and memory are key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars Syntax Example\n",
    "\n",
    "Notice the similarity, but also the use of expressions (`pl.col()`).\n",
    "\n",
    "**Simple exercise**\n",
    "Complete the demo (find a data source) and create a script with inline dependencies. \n",
    "```python\n",
    "import polars as pl\n",
    "\n",
    "# Same data in a Polars DataFrame\n",
    "df_pl = pl.DataFrame(data)\n",
    "\n",
    "# The same aggregation, but using the Polars expression API\n",
    "polar_stats = df_pl.group_by('Experiment').agg(\n",
    "    pl.col('Measurement').mean().alias('mean'),\n",
    "    pl.col('Measurement').std().alias('std')\n",
    ")\n",
    "\n",
    "print(polar_stats)\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied pandas tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qqI0xLLLsPhz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this tutorial we will explore a real data escenario to look for exoplanets! We will use real data and `pandas` to clean it and get some useful info. The main goal is to be able to answer questions like:\n",
    "- What is the most common method for discovering exoplanets?\n",
    "- What is the average size of a planet discovered in the last decade?\n",
    "- Is there a relationship between a star's temperature and the mass of its planets?\n",
    "\n",
    "But when you first look at the data, you realize it's not that simple. There are gaps, inconsistencies, and errors. This is where we need to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qqI0xLLLsPhz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Getting the data\n",
    "First, let's pull the data directly from the NASA Exoplanet Archive. This is real, live data! <https://exoplanetarchive.ipac.caltech.edu/docs/data.html>\n",
    "\n",
    "The columns we are requesting are:\n",
    "\n",
    "    pl_name: Planet Name\n",
    "\n",
    "    hostname: Host Star Name\n",
    "\n",
    "    discoverymethod: How the planet was found\n",
    "\n",
    "    pl_orbper: Orbital Period (days)\n",
    "\n",
    "    pl_rade: Planet Radius (Earth radii)\n",
    "\n",
    "    pl_bmasse: Planet Mass (Earth masses)\n",
    "\n",
    "    st_teff: Star's Effective Temperature (Kelvin)\n",
    "\n",
    "    st_rad: Star's Radius (Solar radii)\n",
    "\n",
    "    st_mass: Star's Mass (Solar masses)\n",
    "\n",
    "    disc_year: Discovery Year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # We'll need numpy for more advanced NaN handling\n",
    "\n",
    "# The official URL from the NASA Exoplanet Archive\n",
    "# https://exoplanetarchive.ipac.caltech.edu/docs/data.html\n",
    "url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+pl_name,hostname,discoverymethod,pl_orbper,pl_rade,pl_bmasse,st_teff,st_rad,st_mass,disc_year+from+ps&format=csv\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(url) # go and read the manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our Raw Exoplanet Data (First 5 Rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Mission Critical Info ---\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the output:\n",
    "- `Total entries`\n",
    "- `Non-null count` : Significant missing data problem\n",
    "- `Dtype`: getting `float` or `int` is good (where it makes sense). If all were objects, that would signal hidden text or errors.\n",
    "\n",
    "Before doing any chance, let's quantify the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise} Quantifying the void\n",
    "Use a single pandas command to calculate the total number of missing values for each column. What column has the most missing data? does it make sense (measuring mass or radius can be much harder that just measuring the presence)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6b6309c32b7ebe37339e1f50966a9c2",
     "grade": false,
     "grade_id": "cell-109cdd5cd23d0bff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access columns or groups of columns like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pl_orbper', 'pl_rade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "Missing data will affect your analysis. To handle it, you need to think _why_ the data is missing, and _how much_ data is missing.\n",
    "\n",
    "Let's focus on the key planetary characteristics: pl_orbper, pl_rade, and pl_bmasse.\n",
    "\n",
    "Dropping all rows with any missing data (`df.dropna()`) would be a disaster: we'd lose a huge portion of our catalog! A better approach is imputation: filling the gaps with a sensible calculated value.\n",
    "\n",
    "The *median* is often a better choice for imputation than the *mean* for astronomical data, as it's less sensitive to extreme outliers (e.g., a planet with a gigantic radius or a very long orbital period). Check also https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to work on a copy\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Calculate the median for the columns we want to fill\n",
    "# We'll calculate them from the original data before we start changing it\n",
    "median_radius = df_cleaned['pl_rade'].median()\n",
    "median_mass = df_cleaned['pl_bmasse'].median()\n",
    "median_orb_period = df_cleaned['pl_orbper'].median()\n",
    "\n",
    "# Impute the missing values using the calculated medians\n",
    "# df_cleaned['pl_rade'].fillna(median_radius, inplace=True) # Avoid inplace=True in a copy object\n",
    "df_cleaned['pl_rade'] = df_cleaned['pl_rade'].fillna(median_radius)\n",
    "df_cleaned['pl_bmasse'] = df_cleaned['pl_bmasse'].fillna(median_mass)\n",
    "df_cleaned['pl_orbper'] = df_cleaned['pl_orbper'].fillna(median_orb_period)\n",
    "# # or better use\n",
    "# df_cleaned.fillna({\n",
    "#     'pl_rade': df_cleaned['pl_rade'].median(),\n",
    "#     'pl_bmasse': df_cleaned['pl_bmasse'].median(),\n",
    "#     'pl_orbper': df_cleaned['pl_orbper'].median()\n",
    "# }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Missing Values After Imputation ---\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our core planet columns are filled! We still have missing data in the star properties (st_teff, st_rad, st_mass), but we can leave them for now, as our analysis might not always require them.\n",
    "\n",
    ":::{exercise} Missing star data\n",
    "1. Choose one of the star-related columns with missing data (st_teff, st_rad, or st_mass).\n",
    "2. Calculate the median for that column.\n",
    "3. Impute (fill) the missing values in that column using its median.\n",
    "4. Verify your work by checking the isnull().sum() count for that column again. It should be zero!\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d044b6b585201430552f07ddc8526ff",
     "grade": false,
     "grade_id": "cell-306e12e91c7bfdb7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Cosmic Anomalies (Outliers)\n",
    "Outliers are extreme values that can skew our understanding. They could be real, fascinating discoveries (like a planet with a massive orbit) or simple data entry errors. Visualizing the data is the best way to spot them.\n",
    "\n",
    "A **box plot** is a powerful tool for outlier detection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a nice style for our plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Let's visualize the distribution of Planet Radius\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_cleaned['pl_rade'])\n",
    "plt.title('Distribution of Exoplanet Radii (in Earth Radii)')\n",
    "plt.xscale('log') # Log scale is often essential for astronomical data!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing data\n",
    "# Maybe use stripplot, swarmplot, or even a violin plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# First the boxplot\n",
    "sns.boxplot(x=df_cleaned['pl_rade'])\n",
    "\n",
    "# Overlay the data as scatter (stripplot)\n",
    "sns.stripplot(x=df_cleaned['pl_rade'], \n",
    "              color='brown', \n",
    "              alpha=0.2,   # transparency\n",
    "              size=3,      # point size\n",
    "              jitter=True) # adds random jitter to avoid overlap\n",
    "\n",
    "plt.title('Distribution of Exoplanet Radii (in Earth Radii)')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('pl_rade')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the box plot shows many points far to the right. These are planets with very large radii. A log scale helps, but we can see that most planets are clustered in a smaller range. Let's say, for a specific analysis, we want to focus only on planets that are somewhat similar to Earth, and we consider anything over 20 Earth radii to be a \"super-giant\" that we want to handle separately.\n",
    "\n",
    "This is not about deleting data, but about filtering it for a specific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many planets are larger than 20 Earth radii?\n",
    "super_giants = df_cleaned[df_cleaned['pl_rade'] > 20] # you can use any other condition\n",
    "print(f\"There are {len(super_giants)} planets with a radius greater than 20 Earths.\")\n",
    "\n",
    "# For our analysis, let's create a new DataFrame without these giants\n",
    "df_filtered = df_cleaned[df_cleaned['pl_rade'] <= 20].copy()\n",
    "\n",
    "print(f\"\\nOriginal DataFrame size: {len(df_cleaned)}\")\n",
    "print(f\"Filtered DataFrame size: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a nice style for our plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Let's visualize the distribution of Planet Radius\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_filtered['pl_rade'])\n",
    "plt.title('Distribution of Exoplanet Radii (in Earth Radii) - filtered')\n",
    "plt.xscale('log') # Log scale is often essential for astronomical data!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise} Explain this command\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain this syntax\n",
    "tmp = df_cleaned[df_cleaned['pl_rade'] > 20].iloc[100:110] # access specific rows\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise} Planet mass\n",
    "1. Create a box plot for pl_bmasse. Remember to use a log scale (plt.xscale('log')) to get a better view.\n",
    "\n",
    "2. Based on the plot, choose a reasonable upper limit to filter out extreme outliers (for example, you might decide anything over 3000 Earth masses is an outlier worth investigating).\n",
    "\n",
    "3. Filter the df_filtered DataFrame further to remove these mass outliers. Print the number of rows before and after to see how many planets you filtered.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4fd51e1c43fd8b8abaf72f05ec84a32",
     "grade": false,
     "grade_id": "cell-b29778e7d53ef989",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More analysis\n",
    "Now let's try to answer the following question: What is the most common discovery method for each decade?\n",
    "\n",
    "To answer this, we need to make sure our `discoverymethod` and `disc_year` columns are clean and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the different discovery methods\n",
    "print(\"Unique Discovery Methods:\")\n",
    "print(df_filtered['discoverymethod'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks clean. Now, let's use our cleaned data to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a 'decade' column for easier grouping\n",
    "# We can do this by integer division\n",
    "df_filtered['discovery_decade'] = (df_filtered['disc_year'] // 10) * 10\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's count the methods per decade\n",
    "discovery_summary = df_filtered.groupby('discovery_decade')['discoverymethod'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Discovery Method Counts per Decade ---\")\n",
    "print(discovery_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{exercise} Visualizing the discovery data\n",
    "1. The discovery_summary data is a Series. To plot it effectively with seaborn, it's often easier to convert it into a DataFrame. Use the .reset_index() method on discovery_summary.\n",
    "\n",
    "2. Rename the new column (which will likely be named count or 0) to something more descriptive, like planet_count.\n",
    "\n",
    "3. Create a bar plot using seaborn.barplot() to show the planet_count for each discoverymethod, grouped by discovery_decade. Hint: Use x='discovery_decade', y='planet_count', and hue='discoverymethod'.\n",
    "\n",
    "4. Give your plot a good title, like \"Dominant Exoplanet Discovery Methods by Decade\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "081879e1dd45c777e6d1b4ec4e6f4c0a",
     "grade": false,
     "grade_id": "cell-7325ada50ad72f60",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from your plot? Which method dominated the 2010s?\n",
    "\n",
    "### More about pandas\n",
    "`Pandas` offers much more than what was shown here. Please check:\n",
    "- `apply`: to apply a function to a column. Example: make all pl_name lower: `df['pl_name'].apply(lambda x: x.lower()) `\n",
    "- Operation across columns: `df['what is this'] = df['pl_rade'] + df['pl_orbper']`\n",
    "- Concatenation: `concat`\n",
    "- Merge: `merge`\n",
    "- `query`: `df.query('A > B')`, <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>\n",
    "- `at`: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html\n",
    "- `where`: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars\n",
    "```{figure} https://pypi-camo.freetls.fastly.net/fac9f1945cd7594df0567bc53f726274340624f5/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f706f6c612d72732f706f6c6172732d7374617469632f6d61737465722f62616e6e65722f706f6c6172735f6769746875625f62616e6e65722e737667\n",
    "---\n",
    "width: 50%\n",
    "align: center\n",
    "name: \n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "[Polars](https://pola.rs/) is a newer, extremely fast DataFrame library built in Rust. It's gaining popularity for its performance, especially on large datasets. See the [polars documentation](https://docs.pola.rs/) . It seems that the current pandas advantage over polars is [geopandas](https://geopandas.org/en/stable/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Pandas | Polars |\n",
    "|---|---|---|\n",
    "| **Backend** | Python/NumPy (partially C) | Rust (built on Apache Arrow)  |\n",
    "| **Performance** | Slower, especially on large data.  | Significantly faster (5-100x) due to parallelism and query optimization.  |\n",
    "| **Execution Model** | Eager (executes line-by-line)  | Supports both Eager and Lazy execution (optimizes the whole query before running)  |\n",
    "| **Memory Usage** | Higher memory footprint.  | More memory efficient.  |\n",
    "| **API** | Very flexible, but can be inconsistent (e.g., `inplace`). | More consistent and expressive, encourages method chaining.  |\n",
    "| **Ecosystem** | Mature and extensive. Integrates with almost every data science library (scikit-learn, Matplotlib, etc.). | Growing, but less integrated with the broader ML ecosystem.  |\n",
    "\n",
    "\n",
    "```{figure} https://pola.rs/_astro/perf-illustration.jHjw6PiD_hpOso.svg\n",
    "---\n",
    "width: 50%\n",
    "align: center\n",
    "name: \n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**When to choose which?**\n",
    "- **Pandas**: Excellent for data exploration, smaller datasets (up to a few GB), and projects that need deep integration with libraries like scikit-learn.\n",
    "- **Polars**: Ideal for large datasets, performance-critical data transformations, and building data pipelines where speed and memory are key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polars Syntax Example\n",
    "\n",
    "Notice the similarity, but also the use of expressions (`pl.col()`).\n",
    "\n",
    "**Simple exercise**\n",
    "Complete the demo (find a data source) and create a script with inline dependencies. \n",
    "\n",
    "```python\n",
    "import polars as pl\n",
    "\n",
    "# Same data in a Polars DataFrame\n",
    "df_pl = pl.DataFrame(data)\n",
    "\n",
    "# The same aggregation, but using the Polars expression API\n",
    "polar_stats = df_pl.group_by('Experiment').agg(\n",
    "    pl.col('Measurement').mean().alias('mean'),\n",
    "    pl.col('Measurement').std().alias('std')\n",
    ")\n",
    "\n",
    "print(polar_stats)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
