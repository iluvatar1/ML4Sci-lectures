
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5. Linear Regression &#8212; Introduction to Machine Learning for Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles.css?v=76e7d31e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-JMZ8EV2JDJ"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-JMZ8EV2JDJ');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-JMZ8EV2JDJ');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/04-LinearRegression/LinearRegression';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6. An Introduction to Logistic Regression" href="../05-LogisticRegression/LogisticRegression.html" />
    <link rel="prev" title="4. An Introduction to Principal Component Analysis (PCA)" href="../03-PCA/Intro-PCA.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Machine Learning for Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    Introduction to Machine Learning for Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../00-IntroductionCourseML/IntroCourse-MachineLearning.html">1. Introduction to the course and to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00-TALK-IntroML/IntroMachineLearning.html">2. A fast introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02-LimpiezayPrepDatos/DataCleaning.html">3. Pandas for data cleaning and analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unsupervised learning I</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03-PCA/Intro-PCA.html">4. An Introduction to Principal Component Analysis (PCA)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supervised learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05-LogisticRegression/LogisticRegression.html">6. An Introduction to Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06-SVM/SVM.html">7. Support Vector Machines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07-IntroNeuralNetworks/NeuralNetworks-BasicConcepts.html">8. Neural Networks Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07-IntroNeuralNetworks/DeepLearning.html">9. An Introduction to Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/PythonReview.html">10. Python Review:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/01-IntroProgrammingPython.html">11. Python Programming (very fast) Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-reviewPython/02-IntroProgrammingPython-DataStructs.html">12. Intro Python II: python data structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02-LimpiezayPrepDatos/TutorialPandas.html">13. Pandas for data cleaning and analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/iluvatar1/ML4Sci-lectures/master?urlpath=lab/tree/lectures/04-LinearRegression/LinearRegression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/iluvatar1/ML4Sci-lectures/blob/master/lectures/04-LinearRegression/LinearRegression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/iluvatar1/ML4Sci-lectures" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/iluvatar1/ML4Sci-lectures/issues/new?title=Issue%20on%20page%20%2Flectures/04-LinearRegression/LinearRegression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/04-LinearRegression/LinearRegression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-what-is-machine-learning">5.1. Review: What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">5.2. Supervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-regression-predicting-a-continuous-value">5.2.1. A. Regression: Predicting a Continuous Value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-classification-predicting-a-discrete-category">5.2.2. B. Classification: Predicting a Discrete Category</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.3. Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-applications-to-basic-sciences">5.3.1. Some applications to basic sciences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal">5.3.2. The Goal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-find-the-best-line-classical-approach">5.3.3. How Do We Find the “Best” Line? Classical approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-find-the-best-line-learning-the-coefficients">5.3.4. How do we find the “Best” Line? Learning the coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-gradient-descent">5.4. Optimization with Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-happening-at-each-step">5.4.1. What is happening at each step?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-basic-example-hookes-law">5.5. A basic example: Hooke’s Law</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-generation">5.5.1. Data Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-using-sklearn">5.5.2. Linear regression using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-at-each-step-if-we-use-gradient-optimization">5.5.3. What happens at each step if we use gradient optimization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-exercises">5.5.4. Practice Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">5.6. Train-test split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-whats-next">5.7. Conclusion &amp; What’s Next?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">5.8. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">5.8.1. Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">5.9. Multiple linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-multiple-linear-regression">5.9.1. Assumptions of Multiple Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-example-workflow-in-python">5.9.2. Full example Workflow in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pre-processing-and-post-processing-tips">5.10. Data pre-processing and post-processing tips</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">5.10.1. Cross-validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection">5.10.2. Feature selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.10.3. Exercises</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">5. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<section id="review-what-is-machine-learning">
<h2><span class="section-number">5.1. </span>Review: What is Machine Learning?<a class="headerlink" href="#review-what-is-machine-learning" title="Link to this heading">#</a></h2>
<p>At its core, <strong>Machine Learning (ML)</strong> is the science of getting computers to learn and act like humans do, and improve their learning over time in an autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.</p>
<ol class="arabic simple">
<li><p><strong>Supervised Learning:</strong> Learning from data that is <strong>labeled</strong>. You provide the algorithm with examples of inputs and their corresponding correct outputs. The goal is to learn a general rule that maps inputs to outputs. (This is our focus today).</p></li>
<li><p><strong>Unsupervised Learning:</strong> Learning from data that is <strong>unlabeled</strong>. The algorithm tries to find patterns, structures, or clusters in the data on its own.</p></li>
<li><p><strong>Reinforcement Learning:</strong> An agent learns to perform actions in an environment to maximize a cumulative reward. It learns by trial and error.</p></li>
</ol>
</section>
<section id="supervised-learning">
<h2><span class="section-number">5.2. </span>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>Supervised Learning:</strong> Given a dataset of input features <strong>X</strong> and corresponding output labels <strong>y</strong>, we want to learn a function <code class="docutils literal notranslate"><span class="pre">h</span></code> (for hypothesis) such that <code class="docutils literal notranslate"><span class="pre">h(X)</span></code> is a good predictor for <strong>y</strong>.</p>
</div></blockquote>
<p>For all models, you should always keep in mind the bias-variance tradeoff</p>
<div style="text-align: center;">
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9f/Bias_and_variance_contributing_to_total_error.svg" width=50%>
<figcaption> https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/250px-Bias_and_variance_contributing_to_total_error.svg.png </figcaption>
</figure>
</div>
<p>If your model has a low bias, then it might be overfitting the training data and then it will increase the predictions variance (imagine using a very large polynomial to do a fit). If your model has low variance, it might be underfitting, so the bias will be large. Chec <a class="reference external" href="https://mlu-explain.github.io/bias-variance/">https://mlu-explain.github.io/bias-variance/</a>.</p>
<p>There are two primary types of supervised learning problems:</p>
<section id="a-regression-predicting-a-continuous-value">
<h3><span class="section-number">5.2.1. </span>A. Regression: Predicting a Continuous Value<a class="headerlink" href="#a-regression-predicting-a-continuous-value" title="Link to this heading">#</a></h3>
<p>The output <code class="docutils literal notranslate"><span class="pre">y</span></code> is a continuous, numerical value.</p>
<ul class="simple">
<li><p><strong>Question:</strong> Based on a material’s temperature, what will its electrical resistance be?</p></li>
<li><p><strong>Question:</strong> Given the mass of a star, what is its expected luminosity?</p></li>
<li><p><strong>Our main tool today:</strong> <strong>Linear Regression</strong> See <a class="reference external" href="https://www.youtube.com/watch?v=CtsRRUddV2s">Visually Explained: Linear regression</a></p></li>
</ul>
</section>
<section id="b-classification-predicting-a-discrete-category">
<h3><span class="section-number">5.2.2. </span>B. Classification: Predicting a Discrete Category<a class="headerlink" href="#b-classification-predicting-a-discrete-category" title="Link to this heading">#</a></h3>
<p>The output <code class="docutils literal notranslate"><span class="pre">y</span></code> is a discrete category or class label.</p>
<ul class="simple">
<li><p><strong>Question:</strong> Based on a cell’s size and shape, is it cancerous or benign?</p></li>
<li><p><strong>Question:</strong> Given the energy and momentum from a particle collider, did we detect an electron or a muon?</p></li>
<li><p><strong>A common tool:</strong> <strong>Logistic Regression</strong> (despite its name, it’s for classification!)</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature</p></th>
<th class="head text-left"><p>Linear Regression</p></th>
<th class="head text-left"><p>Logistic Regression</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Problem Type</strong></p></td>
<td class="text-left"><p>Regression (predicting continuous values)</p></td>
<td class="text-left"><p>Classification (predicting categorical outcomes)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Output</strong></p></td>
<td class="text-left"><p>Continuous numerical value (e.g., price, temperature)</p></td>
<td class="text-left"><p>Probability (0 to 1), which is then mapped to a class</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Dependent Variable</strong></p></td>
<td class="text-left"><p>Continuous</p></td>
<td class="text-left"><p>Categorical (binary or multi-class)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Underlying Function</strong></p></td>
<td class="text-left"><p>Linear equation: y=β0​+β1​x1​+…+βn​xn​</p></td>
<td class="text-left"><p>Sigmoid (logistic) function applied to a linear equation: p=1+e−(β0​+β1​x1​+…+βn​xn​)1​</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Cost Function</strong></p></td>
<td class="text-left"><p>Mean Squared Error (MSE), Root Mean Squared Error (RMSE)</p></td>
<td class="text-left"><p>Log Loss (Binary Cross-Entropy), Cross-Entropy</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Interpretation of Coefficients</strong></p></td>
<td class="text-left"><p>Change in the dependent variable for a one-unit change in the independent variable</p></td>
<td class="text-left"><p>Change in the log-odds of the dependent variable for a one-unit change in the independent variable</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Assumptions</strong></p></td>
<td class="text-left"><p>Linearity, independence of errors, homoscedasticity, normality of residuals, no multicollinearity</p></td>
<td class="text-left"><p>Linearity of independent variables with log-odds, independence of observations, no multicollinearity</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Common Use Cases</strong></p></td>
<td class="text-left"><p>Predicting house prices, sales forecasting, predicting exam scores, trend analysis</p></td>
<td class="text-left"><p>Spam detection, disease prediction (e.g., presence/absence), customer churn prediction, sentiment analysis</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Evaluation Metrics</strong></p></td>
<td class="text-left"><p>MSE, RMSE, R-squared, MAE</p></td>
<td class="text-left"><p>Accuracy, Precision, Recall, F1-Score, ROC-AUC</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="id1">
<h2><span class="section-number">5.3. </span>Linear Regression<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Linear Regression is one of the simplest and most interpretable machine learning models. It assumes a linear relationship between the input features and the output variable.</p>
<div class="cell tag_hide-input tag_hide-output docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#from aux.linear_regression_example_plot import generate_and_plot_regression_problems</span>

<span class="c1"># linear_regression_plots.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span> <span class="c1"># Required for 3D projection</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_and_plot_regression_problems</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates data and plots 2D and 3D linear regression problems.</span>
<span class="sd">    Returns the matplotlib figure object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create a figure with two subplots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span> <span class="c1"># 1 row, 2 columns</span>

    <span class="c1"># --- Left Subplot: 2D Linear Regression Problem ---</span>
    <span class="c1"># Generate data for a straight line with noise</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">x_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">true_slope</span> <span class="o">=</span> <span class="mf">2.5</span>
    <span class="n">true_intercept</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">y_true_2d</span> <span class="o">=</span> <span class="n">true_slope</span> <span class="o">*</span> <span class="n">x_2d</span> <span class="o">+</span> <span class="n">true_intercept</span>
    <span class="n">noise_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x_2d</span><span class="p">))</span>
    <span class="n">y_noisy_2d</span> <span class="o">=</span> <span class="n">y_true_2d</span> <span class="o">+</span> <span class="n">noise_2d</span>

    <span class="c1"># Plot the true line</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_2d</span><span class="p">,</span> <span class="n">y_true_2d</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Line&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the noisy points</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_2d</span><span class="p">,</span> <span class="n">y_noisy_2d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Noisy Data&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;2D Linear Regression Problem&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># --- Right Subplot: 3D Plane Regression Problem ---</span>
    <span class="c1"># Generate data for a plane with noise</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span> <span class="c1"># Specify 3D projection</span>
    <span class="n">x_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">y_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">X_3d</span><span class="p">,</span> <span class="n">Y_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_3d</span><span class="p">,</span> <span class="n">y_3d</span><span class="p">)</span>
    <span class="n">true_coeff_x</span> <span class="o">=</span> <span class="mf">1.2</span>
    <span class="n">true_coeff_y</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.8</span>
    <span class="n">true_intercept_3d</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">Z_true_3d</span> <span class="o">=</span> <span class="n">true_coeff_x</span> <span class="o">*</span> <span class="n">X_3d</span> <span class="o">+</span> <span class="n">true_coeff_y</span> <span class="o">*</span> <span class="n">Y_3d</span> <span class="o">+</span> <span class="n">true_intercept_3d</span>
    <span class="n">noise_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Z_true_3d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">Z_noisy_3d</span> <span class="o">=</span> <span class="n">Z_true_3d</span> <span class="o">+</span> <span class="n">noise_3d</span>

    <span class="c1"># Plot the true plane surface</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X_3d</span><span class="p">,</span> <span class="n">Y_3d</span><span class="p">,</span> <span class="n">Z_true_3d</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Plane&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the noisy points</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_3d</span><span class="p">,</span> <span class="n">Y_3d</span><span class="p">,</span> <span class="n">Z_noisy_3d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Noisy Data&#39;</span><span class="p">)</span> <span class="c1"># s is marker size</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;3D Linear Regression Problem (Plane)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Z&#39;</span><span class="p">)</span>

    <span class="c1"># Adjust layout to prevent overlap</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="c1"># No plt.show() here, as we return the figure to be shown in Jupyter</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># if __name__ == &#39;__main__&#39;:</span>
<span class="c1">#     # This block only runs if the script is executed directly, not when imported</span>
<span class="c1">#     fig = generate_and_plot_regression_problems()</span>
<span class="c1">#     plt.show()</span>


<span class="n">fig_regression_problems</span> <span class="o">=</span> <span class="n">generate_and_plot_regression_problems</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<details class="admonition hide below-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell output</p>
<p class="expanded admonition-title">Hide code cell output</p>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/722220ad01f8cf117fb09a352d2d3008d21c552ca5d6f92832eee8584381df61.png" src="../../_images/722220ad01f8cf117fb09a352d2d3008d21c552ca5d6f92832eee8584381df61.png" />
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">plotly.graph_objects</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">go</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">plotly.subplots</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_subplots</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_and_plot_regression_problems_plotly</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates data and plots interactive 2D and 3D linear regression problems using Plotly.</span>
<span class="sd">    Returns the Plotly figure object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create a figure with two subplots: one 2D, one 3D</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">make_subplots</span><span class="p">(</span>
        <span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">specs</span><span class="o">=</span><span class="p">[[{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;xy&#39;</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;surface&#39;</span><span class="p">}]],</span>
        <span class="n">subplot_titles</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;2D Linear Regression Problem&#39;</span><span class="p">,</span> <span class="s1">&#39;3D Linear Regression Problem (Plane)&#39;</span><span class="p">),</span>
        <span class="n">column_widths</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> 
    <span class="p">)</span>

    <span class="c1"># --- Left Subplot: 2D Linear Regression Problem ---</span>
    <span class="c1"># Generate data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">x_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">true_slope</span> <span class="o">=</span> <span class="mf">2.5</span>
    <span class="n">true_intercept</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">y_true_2d</span> <span class="o">=</span> <span class="n">true_slope</span> <span class="o">*</span> <span class="n">x_2d</span> <span class="o">+</span> <span class="n">true_intercept</span>
    <span class="n">noise_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x_2d</span><span class="p">))</span>
    <span class="n">y_noisy_2d</span> <span class="o">=</span> <span class="n">y_true_2d</span> <span class="o">+</span> <span class="n">noise_2d</span>

    <span class="c1"># Add the true line trace</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span>
        <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_2d</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_true_2d</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;True Line&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)),</span>
        <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="c1"># Add the noisy data points trace</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span>
        <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_2d</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_noisy_2d</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Noisy Data&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)),</span>
        <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>

    <span class="c1"># --- Right Subplot: 3D Plane Regression Problem ---</span>
    <span class="c1"># Generate data</span>
    <span class="n">x_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">y_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">X_3d</span><span class="p">,</span> <span class="n">Y_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_3d</span><span class="p">,</span> <span class="n">y_3d</span><span class="p">)</span>
    <span class="n">true_coeff_x</span> <span class="o">=</span> <span class="mf">1.2</span>
    <span class="n">true_coeff_y</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.8</span>
    <span class="n">true_intercept_3d</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">Z_true_3d</span> <span class="o">=</span> <span class="n">true_coeff_x</span> <span class="o">*</span> <span class="n">X_3d</span> <span class="o">+</span> <span class="n">true_coeff_y</span> <span class="o">*</span> <span class="n">Y_3d</span> <span class="o">+</span> <span class="n">true_intercept_3d</span>
    <span class="n">noise_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Z_true_3d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">Z_noisy_3d</span> <span class="o">=</span> <span class="n">Z_true_3d</span> <span class="o">+</span> <span class="n">noise_3d</span>

    <span class="c1"># Add the true plane surface trace</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span>
        <span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_3d</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_3d</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">Z_true_3d</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;True Plane&#39;</span><span class="p">,</span> <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
    <span class="c1"># Add the noisy 3D data points trace</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span>
        <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">X_3d</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_3d</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">z</span><span class="o">=</span><span class="n">Z_noisy_3d</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Noisy Data&#39;</span>
        <span class="p">),</span>
        <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="c1"># --- Update layout and axis titles ---</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">title_text</span><span class="o">=</span><span class="s1">&#39;Linear Regression Examples&#39;</span><span class="p">,</span>
        <span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span>
        <span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
            <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
            <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_xaxes</span><span class="p">(</span><span class="n">title_text</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">title_text</span><span class="o">=</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Generate and show the plots in the Jupyter Notebook</span>
<span class="n">fig_regression_problems_plotly</span> <span class="o">=</span> <span class="n">generate_and_plot_regression_problems_plotly</span><span class="p">()</span>
<span class="n">fig_regression_problems_plotly</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
</div>
</div>
<section id="some-applications-to-basic-sciences">
<h3><span class="section-number">5.3.1. </span>Some applications to basic sciences<a class="headerlink" href="#some-applications-to-basic-sciences" title="Link to this heading">#</a></h3>
<p>Physics and Statistical Physics (Linear Regression/Ridge Regression)</p>
<ul class="simple">
<li><p>Alexander Mozeika, Mansoor Sheikh, Fabian Aguirre-Lopez, Fabrizio Antenucci, and Anthony C. C. Coolen (2021). Exact results on high-dimensional linear regression via statistical physics. PHYSICAL REVIEW E, 103, 042142 (2021). DOI/Link &lt;<a class="reference external" href="http://doi.org/10.1103/PhysRevE.103.042142">doi.org/10.1103/PhysRevE.103.042142</a>&gt;</p></li>
<li><p>J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová (2019). The statistical physics of ridge regression and the bias-variance trade-off. Proceedings of the National Academy of Sciences U.S.A., 116, 5451 (2019)</p></li>
<li><p>M. Advani and S. Ganguli (2016). Statistical physics of high-dimensional inference: The linear regression model. Physical Review X, 6, 031034 (2016)</p></li>
</ul>
<p>Materials Science and Engineering (Linear Regression/Knowledge Discovery)</p>
<ul class="simple">
<li><p>Doreswamy, Hemanth K S, and Manohar M G (2011). Linear Regression Model for Knowledge Discovery in Engineering Materials. AIAA 2011, CS &amp; IT 03, pp. 147–156 (2011) . DOI/Link: 10.5121/csit.2011.1313</p></li>
<li><p>A. M. Deml, R. O’Hayre, C. Wolverton, and V. Stevanović (2016). Predicting density functional theory total energies and enthalpies of formation of metal-nonmetal compounds by linear regression. Phys. Rev. B: Condens. Matter Mater. Phys., 93, 085142 (2016)</p></li>
</ul>
<p>Biology and Genomics (Linear Models/RNA-Seq and Microarray)</p>
<ul class="simple">
<li><p>M. E. Ritchie, B. Phipson, D. Wu, Y. Hu, C. W. Law, W. Shi, and G. K. Smyth (2015). limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Research, 43(7), e47 (2015)</p></li>
<li><p>C. W. Law, Y. Chen, W. Shi, and G. K. Smyth (2014). Voom: precision weights unlock linear model analysis tools for RNA-seq read counts. Genome Biology, 15, R29 (2014)</p></li>
</ul>
<p>Chemistry, Analytical Chemistry, and Drug Design (Multiple Linear Regression - MLR)</p>
<ul class="simple">
<li><p>J. I. García, H. García-Marín, J. A. Mayoral, and P. Pérez (2013). Quantitative structure-property relationships prediction of some physic-chemical properties of glycerol based solvents. Green Chemistry, 15, 2283–2293 (2013)</p></li>
<li><p>A. G. A. Jameel, N. Naser, A-H. Emwas, S. Dooley, and S. M. Sarathy (2016). Predicting fuel ignition quality using 1H NMR spectroscopy and multiple linear regression.  Energy &amp; Fuels, 30, 9819–9835 (2016)</p></li>
</ul>
<p>Environmental Science and Planning (Linear and Spatial Regression Models)</p>
<ul class="simple">
<li><p>Sanqing He, Yanan Sun, Ningyi Zeng, Lei Wang, Zihan Cao, and Zhen He (2025). Regional divergence in the urban form-carbon emission nexus: a comparative analysis of linear and non-linear spatial modeling approaches for 286 Chinese cities. Frontiers in Environmental Science, 13:1658538 (2025).  10.3389/fenvs.2025.1658538</p></li>
</ul>
</section>
<section id="the-goal">
<h3><span class="section-number">5.3.2. </span>The Goal<a class="headerlink" href="#the-goal" title="Link to this heading">#</a></h3>
<p>To find the “best-fit” line that describes the data. For a single input feature <code class="docutils literal notranslate"><span class="pre">x</span></code>, the equation of the line is:</p>
<div class="math notranslate nohighlight">
\[ \hat{y} = \theta_0 + \theta_1 x \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> (y-hat) is the <strong>predicted value</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the <strong>input feature</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_0\)</span> (theta-zero) is the <strong>y-intercept</strong> (also called the bias). It’s the value of <span class="math notranslate nohighlight">\(\hat{y}\)</span> when <span class="math notranslate nohighlight">\(x=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_1\)</span> (theta-one) is the <strong>slope</strong> or <strong>coefficient</strong>. It represents the change in <span class="math notranslate nohighlight">\(\hat{y}\)</span> for a one-unit change in <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>Our goal is to find the optimal values for <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> that make our line fit the data as closely as possible. We are going to follow to paths to do this: the classical exact solution, and the ML (gradient descent) way.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Classical (Analytical)</p></th>
<th class="head"><p>Machine Learning (Gradient Descent)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Mathematical Foundation</strong></p></td>
<td><p>Calculus: <span class="math notranslate nohighlight">\(\partial/\partial\theta = 0\)</span></p></td>
<td><p>Optimization: <span class="math notranslate nohighlight">\(\theta = \theta -\alpha \nabla\theta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Solution Type</strong></p></td>
<td><p>Exact (closed-form)</p></td>
<td><p>Iterative approximation</p></td>
</tr>
<tr class="row-even"><td><p><strong>Computation Time</strong></p></td>
<td><p>Fast (single calculation)</p></td>
<td><p>Slower (many iterations)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory Requirements</strong></p></td>
<td><p>Can handle large datasets</p></td>
<td><p>May need batch processing</p></td>
</tr>
<tr class="row-even"><td><p><strong>Scalability</strong></p></td>
<td><p>Limited by matrix operations</p></td>
<td><p>Scales to massive datasets</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Interpretability</strong></p></td>
<td><p>Direct mathematical insight</p></td>
<td><p>Requires convergence analysis</p></td>
</tr>
<tr class="row-even"><td><p><strong>Flexibility</strong></p></td>
<td><p>Fixed to linear relationships</p></td>
<td><p>Easily extended (regularization, non-linear)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Noise Handling</strong></p></td>
<td><p>Assumes well-behaved data</p></td>
<td><p>Naturally robust to outliers</p></td>
</tr>
<tr class="row-even"><td><p><strong>Implementation</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.linalg.lstsq()</span></code> or <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></p></td>
<td><p>Custom loops or <code class="docutils literal notranslate"><span class="pre">SGDRegressor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>When to Use</strong></p></td>
<td><p>Small-medium datasets, interpretability needed</p></td>
<td><p>Large datasets, part of ML pipeline</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The Key Insight</strong></p>
<p>Both methods minimize the <strong>same cost function</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \frac{1}{2n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2\]</div>
<ul class="simple">
<li><p><strong>Classical</strong>: Solves <span class="math notranslate nohighlight">\(\nabla\)</span>MSE = 0 analytically</p></li>
<li><p><strong>ML</strong>: Follows <span class="math notranslate nohighlight">\(\nabla\)</span>MSE downhill iteratively</p></li>
</ul>
<p>They’re different paths to the same mathematical destination!</p>
</section>
<section id="how-do-we-find-the-best-line-classical-approach">
<h3><span class="section-number">5.3.3. </span>How Do We Find the “Best” Line? Classical approach<a class="headerlink" href="#how-do-we-find-the-best-line-classical-approach" title="Link to this heading">#</a></h3>
<p>For this simple example, computing both parameters is “easily” done by defining a metric to minimize, computing its partial derivatives, making them null and then computing the parameters. The distance from a giving predicted pint from its corresponding data is <span class="math notranslate nohighlight">\((\hat y_i - y_i)^2\)</span>, and it is squared to take into account values over or under estimating the data. We can define the <strong>MEAN SQUARED ERROR</strong> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-59fe96d8-2d4d-4196-ba4c-01878eb42851">
<span class="eqno">(5.1)<a class="headerlink" href="#equation-59fe96d8-2d4d-4196-ba4c-01878eb42851" title="Permalink to this equation">#</a></span>\[\begin{equation}
\Delta (\theta_0, \theta_1) = \frac{1}{2n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 = \frac{1}{2n} \sum_{i=1}^{n} ((\theta_0 + \theta_1 x_i) - y_i)^2,
\end{equation}\]</div>
<p>and the goal is to minimize it.</p>
<p>In this simple case, we can compute <span class="math notranslate nohighlight">\(\partial \Delta/\partial \theta_0 = 0\)</span> and <span class="math notranslate nohighlight">\(\partial \Delta/\partial \theta_1 = 0\)</span>, and from it arrive to</p>
<div class="amsmath math notranslate nohighlight" id="equation-283f2e4f-78ce-47a1-818e-ed6d8ec83b35">
<span class="eqno">(5.2)<a class="headerlink" href="#equation-283f2e4f-78ce-47a1-818e-ed6d8ec83b35" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta_1 &amp;= \frac{n\sum x_i y_i - \sum x_i \sum y_i}{n\sum x_i^2 - (\sum x_i)^2},\\\\
\theta_0 &amp;= \frac{\sum y_i - \theta_1\sum x_i}{n}.
\end{align}\]</div>
<p>This is the Ordinary Least Squares(OLS) formulation, and is good for simple problems. You can use python <code class="docutils literal notranslate"><span class="pre">statmodels</span></code> to get even more info about your solution and parameters. But generalizing it to more dimensions, for instance, might be more difficult and cumbersome, so maybe we can re-formulate the problem in another way: “learning” the parameters so the model can be tested on the data and its predictive power be used for other data. It is important to not blindly apply OLS, since it can fail when some assumptions are not fulfilled (more at the end)</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Simulate nonlinear relation + heteroscedastic noise</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># Fit simple linear regression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;OLS Fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Relation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;When linear fit poorly captures true relation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Residuals</span>
<span class="n">resid</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Residuals vs Fitted: Nonlinearity and Heteroscedasticity visible&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/ea6bb43787434c196710b6961864e405f3277fe3841929f379b681444e4644cf.png" src="../../_images/ea6bb43787434c196710b6961864e405f3277fe3841929f379b681444e4644cf.png" />
<img alt="../../_images/a509e637136efc8f80909514a9bade977adda55b8ecb8d935987fdaa41331a13.png" src="../../_images/a509e637136efc8f80909514a9bade977adda55b8ecb8d935987fdaa41331a13.png" />
</div>
</div>
</section>
<section id="how-do-we-find-the-best-line-learning-the-coefficients">
<h3><span class="section-number">5.3.4. </span>How do we find the “Best” Line? Learning the coefficients<a class="headerlink" href="#how-do-we-find-the-best-line-learning-the-coefficients" title="Link to this heading">#</a></h3>
<p>We need a way to quantify how “wrong” our line is. To do so, we define the same  <strong>Cost Function</strong> (or Loss Function) as before:</p>
<div class="amsmath math notranslate nohighlight" id="equation-64430547-c32b-443b-bf92-a55bdb13be40">
<span class="eqno">(5.3)<a class="headerlink" href="#equation-64430547-c32b-443b-bf92-a55bdb13be40" title="Permalink to this equation">#</a></span>\[\begin{equation}
\Delta (\theta_0, \theta_1) = \frac{1}{2n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 = \frac{1}{2n} \sum_{i=1}^{n} ((\theta_0 + \theta_1 x_i) - y_i)^2.
\end{equation}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are several definitions for the loss, like with absolute value, or with the square root of the quantity shown. Each one has different advantages and disadvantages, like slower/faster convergence, sensitivity to outliers and so on.</p>
</div>
<p>To optimize this, we will follow and approach that is ubiquitous in machine learning: start with some random values, compute the loss function and its gradient, adjust the coefficient values according to the loss magnitude and iterate. This implies that e are using back propagation!</p>
</section>
</section>
<section id="optimization-with-gradient-descent">
<h2><span class="section-number">5.4. </span>Optimization with Gradient Descent<a class="headerlink" href="#optimization-with-gradient-descent" title="Link to this heading">#</a></h2>
<p><strong>Analogy:</strong> Imagine you are a hiker in a foggy valley and you want to get to the lowest point. You can’t see the whole valley, but you can feel the slope of the ground right under your feet. What do you do? You take a step in the steepest downward direction.</p>
<p>This is exactly what Gradient Descent does:</p>
<ol class="arabic simple">
<li><p>Start with some random values for <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span>.</p></li>
<li><p>Calculate the gradient (the “slope”) of the cost function at that point.</p></li>
<li><p>Take a small step in the opposite direction of the gradient (downhill).</p></li>
<li><p>Repeat until you reach the bottom (the minimum), where the slope is zero.</p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For a nice visualization of gradient descent, check: <a class="reference external" href="https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html">https://aero-learn.imperial.ac.uk/vis/Machine Learning/gradient_descent_3d.html</a></p>
</div>
<p>The size of the “step” you take is called the <strong>learning rate</strong> (alpha, <span class="math notranslate nohighlight">\(\alpha\)</span>). A small learning rate will converge slowly, while a large one might overshoot the minimum. See <a class="reference external" href="https://www.youtube.com/watch?v=gsfbWn4Gy5Q">https://www.youtube.com/watch?v=gsfbWn4Gy5Q</a></p>
<section id="what-is-happening-at-each-step">
<h3><span class="section-number">5.4.1. </span>What is happening at each step?<a class="headerlink" href="#what-is-happening-at-each-step" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Give some initial value to params</p></li>
<li><p>For <span class="math notranslate nohighlight">\(N\)</span> steps:</p>
<ul>
<li><p>Predict : <span class="math notranslate nohighlight">\(\hat y = \theta_0 + \theta_1 x\)</span></p></li>
<li><p>compute error and loss: <span class="math notranslate nohighlight">\(\Delta = \dfrac{1}{2m} \sum (\hat y - y)^2\)</span></p></li>
<li><p>compute gradients: <span class="math notranslate nohighlight">\(\dfrac{\partial \Delta}{\partial \theta_0}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial \Delta}{\partial \theta_1}\)</span> (autodiff)</p></li>
<li><p>improve params: <span class="math notranslate nohighlight">\(\theta = \theta -\alpha \nabla_\theta\)</span> (back-propagation, <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate)</p></li>
</ul>
</li>
</ul>
<p>To do so, we will implement a simple function to do the iterations, and then test it</p>
</section>
</section>
<section id="a-basic-example-hookes-law">
<h2><span class="section-number">5.5. </span>A basic example: Hooke’s Law<a class="headerlink" href="#a-basic-example-hookes-law" title="Link to this heading">#</a></h2>
<p>Hooke’s Law is a fundamental principle in physics that states the force (<code class="docutils literal notranslate"><span class="pre">F</span></code>) needed to extend or compress a spring by some distance (<code class="docutils literal notranslate"><span class="pre">x</span></code>) is directly proportional to that distance.</p>
<div class="math notranslate nohighlight">
\[ F = kx \]</div>
<p>This is a perfect linear relationship! We can use linear regression to find the spring constant <code class="docutils literal notranslate"><span class="pre">k</span></code> from experimental data. Let’s assume we conducted an experiment and got some noisy measurements.</p>
<section id="data-generation">
<h3><span class="section-number">5.5.1. </span>Data Generation<a class="headerlink" href="#data-generation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: Generate some experimental data</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Let&#39;s assume the true spring constant k is 4.5 N/m</span>
<span class="n">k_true</span> <span class="o">=</span> <span class="mf">4.5</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># for reproducibility</span>

<span class="c1"># Displacement (x) in meters. This is our feature X.</span>
<span class="c1"># The .reshape(-1, 1) is needed because scikit-learn expects 2D arrays for features.</span>
<span class="n">xdata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">x_displacement</span> <span class="o">=</span> <span class="n">xdata</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Force (F) in Newtons. This is our target y.</span>
<span class="c1"># We&#39;ll calculate the true force and add some random &quot;measurement noise&quot;</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">x_displacement</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y_force</span> <span class="o">=</span> <span class="n">k_true</span> <span class="o">*</span> <span class="n">x_displacement</span> <span class="o">+</span> <span class="n">noise</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">bokeh.plotting</span><span class="w"> </span><span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">bokeh.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">output_notebook</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Sample data (replace with your actual data)</span>
<span class="n">y_plot</span> <span class="o">=</span> <span class="n">k_true</span> <span class="o">*</span> <span class="n">x_displacement</span> <span class="o">+</span> <span class="n">noise</span>
<span class="c1"># Setup for inline plotting in a Jupyter Notebook</span>
<span class="n">output_notebook</span><span class="p">()</span>

<span class="c1"># Create a figure with all properties set at once</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Hooke&#39;s Law: Force vs. Displacement&quot;</span><span class="p">,</span>
    <span class="n">x_axis_label</span><span class="o">=</span><span class="s2">&quot;Displacement (x) [m]&quot;</span><span class="p">,</span>
    <span class="n">y_axis_label</span><span class="o">=</span><span class="s2">&quot;Force (F) [N]&quot;</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span>  <span class="c1"># Define size</span>
<span class="p">)</span>

<span class="c1"># Add the scatter glyph with styling</span>
<span class="n">p</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_displacement</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_force</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">legend_label</span><span class="o">=</span><span class="s1">&#39;Experimental Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">line_color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">show</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html">    <style>
        .bk-notebook-logo {
            display: block;
            width: 20px;
            height: 20px;
            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);
        }
    </style>
    <div>
        <a href="https://bokeh.org" target="_blank" class="bk-notebook-logo"></a>
        <span id="caa5e98e-30f8-48a7-b901-f061a4d9361d">Loading BokehJS ...</span>
    </div>
</div><script type="application/javascript">'use strict';
(function(root) {
  function now() {
    return new Date();
  }

  const force = true;

  if (typeof root._bokeh_onload_callbacks === "undefined" || force === true) {
    root._bokeh_onload_callbacks = [];
    root._bokeh_is_loading = undefined;
  }

const JS_MIME_TYPE = 'application/javascript';
  const HTML_MIME_TYPE = 'text/html';
  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';
  const CLASS_NAME = 'output_bokeh rendered_html';

  /**
   * Render data to the DOM node
   */
  function render(props, node) {
    const script = document.createElement("script");
    node.appendChild(script);
  }

  /**
   * Handle when an output is cleared or removed
   */
  function handleClearOutput(event, handle) {
    function drop(id) {
      const view = Bokeh.index.get_by_id(id)
      if (view != null) {
        view.model.document.clear()
        Bokeh.index.delete(view)
      }
    }

    const cell = handle.cell;

    const id = cell.output_area._bokeh_element_id;
    const server_id = cell.output_area._bokeh_server_id;

    // Clean up Bokeh references
    if (id != null) {
      drop(id)
    }

    if (server_id !== undefined) {
      // Clean up Bokeh references
      const cmd_clean = "from bokeh.io.state import curstate; print(curstate().uuid_to_server['" + server_id + "'].get_sessions()[0].document.roots[0]._id)";
      cell.notebook.kernel.execute(cmd_clean, {
        iopub: {
          output: function(msg) {
            const id = msg.content.text.trim()
            drop(id)
          }
        }
      });
      // Destroy server and session
      const cmd_destroy = "import bokeh.io.notebook as ion; ion.destroy_server('" + server_id + "')";
      cell.notebook.kernel.execute(cmd_destroy);
    }
  }

  /**
   * Handle when a new output is added
   */
  function handleAddOutput(event, handle) {
    const output_area = handle.output_area;
    const output = handle.output;

    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only
    if ((output.output_type != "display_data") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {
      return
    }

    const toinsert = output_area.element.find("." + CLASS_NAME.split(' ')[0]);

    if (output.metadata[EXEC_MIME_TYPE]["id"] !== undefined) {
      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];
      // store reference to embed id on output_area
      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE]["id"];
    }
    if (output.metadata[EXEC_MIME_TYPE]["server_id"] !== undefined) {
      const bk_div = document.createElement("div");
      bk_div.innerHTML = output.data[HTML_MIME_TYPE];
      const script_attrs = bk_div.children[0].attributes;
      for (let i = 0; i < script_attrs.length; i++) {
        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);
        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent
      }
      // store reference to server id on output_area
      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE]["server_id"];
    }
  }

  function register_renderer(events, OutputArea) {

    function append_mime(data, metadata, element) {
      // create a DOM node to render to
      const toinsert = this.create_output_subarea(
        metadata,
        CLASS_NAME,
        EXEC_MIME_TYPE
      );
      this.keyboard_manager.register_events(toinsert);
      // Render to node
      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};
      render(props, toinsert[toinsert.length - 1]);
      element.append(toinsert);
      return toinsert
    }

    /* Handle when an output is cleared or removed */
    events.on('clear_output.CodeCell', handleClearOutput);
    events.on('delete.Cell', handleClearOutput);

    /* Handle when a new output is added */
    events.on('output_added.OutputArea', handleAddOutput);

    /**
     * Register the mime type and append_mime function with output_area
     */
    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {
      /* Is output safe? */
      safe: true,
      /* Index of renderer in `output_area.display_order` */
      index: 0
    });
  }

  // register the mime type if in Jupyter Notebook environment and previously unregistered
  if (root.Jupyter !== undefined) {
    const events = require('base/js/events');
    const OutputArea = require('notebook/js/outputarea').OutputArea;

    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {
      register_renderer(events, OutputArea);
    }
  }
  if (typeof (root._bokeh_timeout) === "undefined" || force === true) {
    root._bokeh_timeout = Date.now() + 5000;
    root._bokeh_failed_load = false;
  }

  const NB_LOAD_WARNING = {'data': {'text/html':
     "<div style='background-color: #fdd'>\n"+
     "<p>\n"+
     "BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \n"+
     "may be due to a slow or bad network connection. Possible fixes:\n"+
     "</p>\n"+
     "<ul>\n"+
     "<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\n"+
     "<li>use INLINE resources instead, as so:</li>\n"+
     "</ul>\n"+
     "<code>\n"+
     "from bokeh.resources import INLINE\n"+
     "output_notebook(resources=INLINE)\n"+
     "</code>\n"+
     "</div>"}};

  function display_loaded(error = null) {
    const el = document.getElementById("caa5e98e-30f8-48a7-b901-f061a4d9361d");
    if (el != null) {
      const html = (() => {
        if (typeof root.Bokeh === "undefined") {
          if (error == null) {
            return "BokehJS is loading ...";
          } else {
            return "BokehJS failed to load.";
          }
        } else {
          const prefix = `BokehJS ${root.Bokeh.version}`;
          if (error == null) {
            return `${prefix} successfully loaded.`;
          } else {
            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;
          }
        }
      })();
      el.innerHTML = html;

      if (error != null) {
        const wrapper = document.createElement("div");
        wrapper.style.overflow = "auto";
        wrapper.style.height = "5em";
        wrapper.style.resize = "vertical";
        const content = document.createElement("div");
        content.style.fontFamily = "monospace";
        content.style.whiteSpace = "pre-wrap";
        content.style.backgroundColor = "rgb(255, 221, 221)";
        content.textContent = error.stack ?? error.toString();
        wrapper.append(content);
        el.append(wrapper);
      }
    } else if (Date.now() < root._bokeh_timeout) {
      setTimeout(() => display_loaded(error), 100);
    }
  }

  function run_callbacks() {
    try {
      root._bokeh_onload_callbacks.forEach(function(callback) {
        if (callback != null)
          callback();
      });
    } finally {
      delete root._bokeh_onload_callbacks
    }
    console.debug("Bokeh: all callbacks have finished");
  }

  function load_libs(css_urls, js_urls, callback) {
    if (css_urls == null) css_urls = [];
    if (js_urls == null) js_urls = [];

    root._bokeh_onload_callbacks.push(callback);
    if (root._bokeh_is_loading > 0) {
      console.debug("Bokeh: BokehJS is being loaded, scheduling callback at", now());
      return null;
    }
    if (js_urls == null || js_urls.length === 0) {
      run_callbacks();
      return null;
    }
    console.debug("Bokeh: BokehJS not loaded, scheduling load and callback at", now());
    root._bokeh_is_loading = css_urls.length + js_urls.length;

    function on_load() {
      root._bokeh_is_loading--;
      if (root._bokeh_is_loading === 0) {
        console.debug("Bokeh: all BokehJS libraries/stylesheets loaded");
        run_callbacks()
      }
    }

    function on_error(url) {
      console.error("failed to load " + url);
    }

    for (let i = 0; i < css_urls.length; i++) {
      const url = css_urls[i];
      const element = document.createElement("link");
      element.onload = on_load;
      element.onerror = on_error.bind(null, url);
      element.rel = "stylesheet";
      element.type = "text/css";
      element.href = url;
      console.debug("Bokeh: injecting link tag for BokehJS stylesheet: ", url);
      document.body.appendChild(element);
    }

    for (let i = 0; i < js_urls.length; i++) {
      const url = js_urls[i];
      const element = document.createElement('script');
      element.onload = on_load;
      element.onerror = on_error.bind(null, url);
      element.async = false;
      element.src = url;
      console.debug("Bokeh: injecting script tag for BokehJS library: ", url);
      document.head.appendChild(element);
    }
  };

  function inject_raw_css(css) {
    const element = document.createElement("style");
    element.appendChild(document.createTextNode(css));
    document.body.appendChild(element);
  }

  const js_urls = ["https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js"];
  const css_urls = [];

  const inline_js = [    function(Bokeh) {
      Bokeh.set_log_level("info");
    },
function(Bokeh) {
    }
  ];

  function run_inline_js() {
    if (root.Bokeh !== undefined || force === true) {
      try {
            for (let i = 0; i < inline_js.length; i++) {
      inline_js[i].call(root, root.Bokeh);
    }

      } catch (error) {display_loaded(error);throw error;
      }if (force === true) {
        display_loaded();
      }} else if (Date.now() < root._bokeh_timeout) {
      setTimeout(run_inline_js, 100);
    } else if (!root._bokeh_failed_load) {
      console.log("Bokeh: BokehJS failed to load within specified timeout.");
      root._bokeh_failed_load = true;
    } else if (force !== true) {
      const cell = $(document.getElementById("caa5e98e-30f8-48a7-b901-f061a4d9361d")).parents('.cell').data().cell;
      cell.output_area.append_execute_result(NB_LOAD_WARNING)
    }
  }

  if (root._bokeh_is_loading === 0) {
    console.debug("Bokeh: BokehJS loaded, going straight to plotting");
    run_inline_js();
  } else {
    load_libs(css_urls, js_urls, function() {
      console.debug("Bokeh: BokehJS plotting callback run at", now());
      run_inline_js();
    });
  }
}(window));</script><div class="output text_html">
  <div id="ff7aa2f7-8068-4930-8d54-ab0a6906eadd" data-root-id="p1008" style="display: contents;"></div>
</div><script type="application/javascript">(function(root) {
  function embed_document(root) {
  const docs_json = {"b718f6e9-0a33-4ab7-be4a-f37659ab13ff":{"version":"3.8.0","title":"Bokeh Application","config":{"type":"object","name":"DocumentConfig","id":"p1054","attributes":{"notifications":{"type":"object","name":"Notifications","id":"p1055"}}},"roots":[{"type":"object","name":"Figure","id":"p1008","attributes":{"width":800,"height":400,"x_range":{"type":"object","name":"DataRange1d","id":"p1009"},"y_range":{"type":"object","name":"DataRange1d","id":"p1010"},"x_scale":{"type":"object","name":"LinearScale","id":"p1018"},"y_scale":{"type":"object","name":"LinearScale","id":"p1019"},"title":{"type":"object","name":"Title","id":"p1011","attributes":{"text":"Hooke's Law: Force vs. Displacement"}},"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1049","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1043","attributes":{"selected":{"type":"object","name":"Selection","id":"p1044","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1045"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/2NggACN9Qv3tH3aZQ+hT9nHOVfyPjW9AuXfsgfxplx/ABV/Yg+mpjyHyr+yB0sHvIOq+2CfA5GAqv9iHwBmfIXq+25vBlQd5/wTqv+XvdSpbCDzD9Scf/ZQZzkAAJ6NW+CgAAAA"},"shape":[20],"dtype":"float64","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/wGgAF//fDCpKCrKzz/CY75SLuTZPyAKJB7jVvQ/cKAU++V1AUCpakDGS3H8P04obiPFAgJAzIg42b4NDUBpQ40Kl5gNQCRLfq8ZcAxAr/N0sUMjEkBMpiunQQUSQPtVjBQg6RNAsOe+KYQ4F0BiA46wFc4UQOlLkTOUExdA93JI9OVLG0DeekHHRUocQMf7HAplayBAt/hR/wQlIEDEvr5Dc5YgQJgtAhGgAAAA"},"shape":[20],"dtype":"float64","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1050","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1051"}}},"glyph":{"type":"object","name":"Scatter","id":"p1046","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"fill_color":{"type":"value","value":"blue"},"hatch_color":{"type":"value","value":"blue"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1047","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"blue"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"blue"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1048","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"blue"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"blue"},"hatch_alpha":{"type":"value","value":0.2}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1017","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p1030"},{"type":"object","name":"WheelZoomTool","id":"p1031","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p1032","attributes":{"dimensions":"both","overlay":{"type":"object","name":"BoxAnnotation","id":"p1033","attributes":{"syncable":false,"line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","handles":{"type":"object","name":"BoxInteractionHandles","id":"p1039","attributes":{"all":{"type":"object","name":"AreaVisuals","id":"p1038","attributes":{"fill_color":"white","hover_fill_color":"lightgray"}}}}}}}},{"type":"object","name":"SaveTool","id":"p1040"},{"type":"object","name":"ResetTool","id":"p1041"},{"type":"object","name":"HelpTool","id":"p1042"}]}},"left":[{"type":"object","name":"LinearAxis","id":"p1025","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1026","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1027"},"axis_label":"Force (F) [N]","major_label_policy":{"type":"object","name":"AllLabels","id":"p1028"}}}],"below":[{"type":"object","name":"LinearAxis","id":"p1020","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1021","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1022"},"axis_label":"Displacement (x) [m]","major_label_policy":{"type":"object","name":"AllLabels","id":"p1023"}}}],"center":[{"type":"object","name":"Grid","id":"p1024","attributes":{"axis":{"id":"p1020"}}},{"type":"object","name":"Grid","id":"p1029","attributes":{"dimension":1,"axis":{"id":"p1025"}}},{"type":"object","name":"Legend","id":"p1052","attributes":{"items":[{"type":"object","name":"LegendItem","id":"p1053","attributes":{"label":{"type":"value","value":"Experimental Data"},"renderers":[{"id":"p1049"}]}}]}}]}}]}};
  const render_items = [{"docid":"b718f6e9-0a33-4ab7-be4a-f37659ab13ff","roots":{"p1008":"ff7aa2f7-8068-4930-8d54-ab0a6906eadd"},"root_ids":["p1008"]}];
  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);
  }
  if (root.Bokeh !== undefined) {
    embed_document(root);
  } else {
    let attempts = 0;
    const timer = setInterval(function(root) {
      if (root.Bokeh !== undefined) {
        clearInterval(timer);
        embed_document(root);
      } else {
        attempts++;
        if (attempts > 100) {
          clearInterval(timer);
          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
        }
      }
    }, 10, root)
  }
})(window);</script></div>
</div>
<p>This looks like a good candidate for linear regression! The data points roughly follow a straight line.</p>
</section>
<section id="linear-regression-using-sklearn">
<h3><span class="section-number">5.5.2. </span>Linear regression using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#linear-regression-using-sklearn" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build and train the model using Scikit-Learn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="c1"># Create a linear regression model object</span>
<span class="c1"># Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train the model using our data</span>
<span class="c1"># The .fit() method is where the &#39;learning&#39; (Gradient Descent) happens!</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_displacement</span><span class="p">,</span> <span class="n">y_force</span><span class="p">)</span> <span class="c1"># USE ALL data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><body><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LinearRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                <table class="parameters-table">
                  <tbody>
                    
        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('fit_intercept',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">fit_intercept&nbsp;</td>
            <td class="value">True</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('copy_X',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">copy_X&nbsp;</td>
            <td class="value">True</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('tol',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">tol&nbsp;</td>
            <td class="value">1e-06</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('n_jobs',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">n_jobs&nbsp;</td>
            <td class="value">None</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('positive',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">positive&nbsp;</td>
            <td class="value">False</td>
        </tr>
    
                  </tbody>
                </table>
            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script></body></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Analyze the results</span>

<span class="c1"># Get the learned parameters (theta_0 and theta_1)</span>
<span class="c1"># .intercept_ is an array, so we take the first element</span>
<span class="n">theta_0</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># .coef_ is a 2D array, so we access it with [0][0]</span>
<span class="n">theta_1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The model has learned the following equation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Force = </span><span class="si">{</span><span class="n">theta_0</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> * Displacement</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The estimated spring constant (k) is: </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> N/m&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The true spring constant was: </span><span class="si">{</span><span class="n">k_true</span><span class="si">}</span><span class="s2"> N/m&quot;</span><span class="p">)</span>

<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">+</span> <span class="n">theta_1</span><span class="o">*</span><span class="n">x_displacement</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The model has learned the following equation:
Force = 0.387 + 4.027 * Displacement

The estimated spring constant (k) is: 4.027 N/m
The true spring constant was: 4.5 N/m
</pre></div>
</div>
</div>
</div>
<p>That’s pretty close! Our model successfully estimated the spring constant from the noisy data. The small non-zero intercept <code class="docutils literal notranslate"><span class="pre">theta_0</span></code> is a result of the random noise we added; in a perfect world, it would be zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">bokeh.plotting</span><span class="w"> </span><span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">bokeh.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">output_notebook</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="c1"># --- Setup and Plotting ---</span>
<span class="n">output_notebook</span><span class="p">()</span>

<span class="c1"># Create a figure with all properties set at once</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Hooke&#39;s Law with Model Fit&quot;</span><span class="p">,</span>
    <span class="n">x_axis_label</span><span class="o">=</span><span class="s2">&quot;Displacement (x) [m]&quot;</span><span class="p">,</span>
    <span class="n">y_axis_label</span><span class="o">=</span><span class="s2">&quot;Force (F) [N]&quot;</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span>
<span class="p">)</span>

<span class="c1"># Plot the original data</span>
<span class="n">p</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_displacement</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_force</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">legend_label</span><span class="o">=</span><span class="s1">&#39;Experimental Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">line_color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Plot the regression line</span>
<span class="n">p</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">x_displacement</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_predicted</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">legend_label</span><span class="o">=</span><span class="s1">&#39;Linear Regression Fit&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">line_width</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Display the plot</span>
<span class="n">show</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">    <style>
        .bk-notebook-logo {
            display: block;
            width: 20px;
            height: 20px;
            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);
        }
    </style>
    <div>
        <a href="https://bokeh.org" target="_blank" class="bk-notebook-logo"></a>
        <span id="f09f1c88-36e8-49d7-a361-1ff847212376">Loading BokehJS ...</span>
    </div>
</div><script type="application/javascript">'use strict';
(function(root) {
  function now() {
    return new Date();
  }

  const force = true;

  if (typeof root._bokeh_onload_callbacks === "undefined" || force === true) {
    root._bokeh_onload_callbacks = [];
    root._bokeh_is_loading = undefined;
  }

const JS_MIME_TYPE = 'application/javascript';
  const HTML_MIME_TYPE = 'text/html';
  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';
  const CLASS_NAME = 'output_bokeh rendered_html';

  /**
   * Render data to the DOM node
   */
  function render(props, node) {
    const script = document.createElement("script");
    node.appendChild(script);
  }

  /**
   * Handle when an output is cleared or removed
   */
  function handleClearOutput(event, handle) {
    function drop(id) {
      const view = Bokeh.index.get_by_id(id)
      if (view != null) {
        view.model.document.clear()
        Bokeh.index.delete(view)
      }
    }

    const cell = handle.cell;

    const id = cell.output_area._bokeh_element_id;
    const server_id = cell.output_area._bokeh_server_id;

    // Clean up Bokeh references
    if (id != null) {
      drop(id)
    }

    if (server_id !== undefined) {
      // Clean up Bokeh references
      const cmd_clean = "from bokeh.io.state import curstate; print(curstate().uuid_to_server['" + server_id + "'].get_sessions()[0].document.roots[0]._id)";
      cell.notebook.kernel.execute(cmd_clean, {
        iopub: {
          output: function(msg) {
            const id = msg.content.text.trim()
            drop(id)
          }
        }
      });
      // Destroy server and session
      const cmd_destroy = "import bokeh.io.notebook as ion; ion.destroy_server('" + server_id + "')";
      cell.notebook.kernel.execute(cmd_destroy);
    }
  }

  /**
   * Handle when a new output is added
   */
  function handleAddOutput(event, handle) {
    const output_area = handle.output_area;
    const output = handle.output;

    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only
    if ((output.output_type != "display_data") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {
      return
    }

    const toinsert = output_area.element.find("." + CLASS_NAME.split(' ')[0]);

    if (output.metadata[EXEC_MIME_TYPE]["id"] !== undefined) {
      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];
      // store reference to embed id on output_area
      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE]["id"];
    }
    if (output.metadata[EXEC_MIME_TYPE]["server_id"] !== undefined) {
      const bk_div = document.createElement("div");
      bk_div.innerHTML = output.data[HTML_MIME_TYPE];
      const script_attrs = bk_div.children[0].attributes;
      for (let i = 0; i < script_attrs.length; i++) {
        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);
        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent
      }
      // store reference to server id on output_area
      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE]["server_id"];
    }
  }

  function register_renderer(events, OutputArea) {

    function append_mime(data, metadata, element) {
      // create a DOM node to render to
      const toinsert = this.create_output_subarea(
        metadata,
        CLASS_NAME,
        EXEC_MIME_TYPE
      );
      this.keyboard_manager.register_events(toinsert);
      // Render to node
      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};
      render(props, toinsert[toinsert.length - 1]);
      element.append(toinsert);
      return toinsert
    }

    /* Handle when an output is cleared or removed */
    events.on('clear_output.CodeCell', handleClearOutput);
    events.on('delete.Cell', handleClearOutput);

    /* Handle when a new output is added */
    events.on('output_added.OutputArea', handleAddOutput);

    /**
     * Register the mime type and append_mime function with output_area
     */
    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {
      /* Is output safe? */
      safe: true,
      /* Index of renderer in `output_area.display_order` */
      index: 0
    });
  }

  // register the mime type if in Jupyter Notebook environment and previously unregistered
  if (root.Jupyter !== undefined) {
    const events = require('base/js/events');
    const OutputArea = require('notebook/js/outputarea').OutputArea;

    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {
      register_renderer(events, OutputArea);
    }
  }
  if (typeof (root._bokeh_timeout) === "undefined" || force === true) {
    root._bokeh_timeout = Date.now() + 5000;
    root._bokeh_failed_load = false;
  }

  const NB_LOAD_WARNING = {'data': {'text/html':
     "<div style='background-color: #fdd'>\n"+
     "<p>\n"+
     "BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \n"+
     "may be due to a slow or bad network connection. Possible fixes:\n"+
     "</p>\n"+
     "<ul>\n"+
     "<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\n"+
     "<li>use INLINE resources instead, as so:</li>\n"+
     "</ul>\n"+
     "<code>\n"+
     "from bokeh.resources import INLINE\n"+
     "output_notebook(resources=INLINE)\n"+
     "</code>\n"+
     "</div>"}};

  function display_loaded(error = null) {
    const el = document.getElementById("f09f1c88-36e8-49d7-a361-1ff847212376");
    if (el != null) {
      const html = (() => {
        if (typeof root.Bokeh === "undefined") {
          if (error == null) {
            return "BokehJS is loading ...";
          } else {
            return "BokehJS failed to load.";
          }
        } else {
          const prefix = `BokehJS ${root.Bokeh.version}`;
          if (error == null) {
            return `${prefix} successfully loaded.`;
          } else {
            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;
          }
        }
      })();
      el.innerHTML = html;

      if (error != null) {
        const wrapper = document.createElement("div");
        wrapper.style.overflow = "auto";
        wrapper.style.height = "5em";
        wrapper.style.resize = "vertical";
        const content = document.createElement("div");
        content.style.fontFamily = "monospace";
        content.style.whiteSpace = "pre-wrap";
        content.style.backgroundColor = "rgb(255, 221, 221)";
        content.textContent = error.stack ?? error.toString();
        wrapper.append(content);
        el.append(wrapper);
      }
    } else if (Date.now() < root._bokeh_timeout) {
      setTimeout(() => display_loaded(error), 100);
    }
  }

  function run_callbacks() {
    try {
      root._bokeh_onload_callbacks.forEach(function(callback) {
        if (callback != null)
          callback();
      });
    } finally {
      delete root._bokeh_onload_callbacks
    }
    console.debug("Bokeh: all callbacks have finished");
  }

  function load_libs(css_urls, js_urls, callback) {
    if (css_urls == null) css_urls = [];
    if (js_urls == null) js_urls = [];

    root._bokeh_onload_callbacks.push(callback);
    if (root._bokeh_is_loading > 0) {
      console.debug("Bokeh: BokehJS is being loaded, scheduling callback at", now());
      return null;
    }
    if (js_urls == null || js_urls.length === 0) {
      run_callbacks();
      return null;
    }
    console.debug("Bokeh: BokehJS not loaded, scheduling load and callback at", now());
    root._bokeh_is_loading = css_urls.length + js_urls.length;

    function on_load() {
      root._bokeh_is_loading--;
      if (root._bokeh_is_loading === 0) {
        console.debug("Bokeh: all BokehJS libraries/stylesheets loaded");
        run_callbacks()
      }
    }

    function on_error(url) {
      console.error("failed to load " + url);
    }

    for (let i = 0; i < css_urls.length; i++) {
      const url = css_urls[i];
      const element = document.createElement("link");
      element.onload = on_load;
      element.onerror = on_error.bind(null, url);
      element.rel = "stylesheet";
      element.type = "text/css";
      element.href = url;
      console.debug("Bokeh: injecting link tag for BokehJS stylesheet: ", url);
      document.body.appendChild(element);
    }

    for (let i = 0; i < js_urls.length; i++) {
      const url = js_urls[i];
      const element = document.createElement('script');
      element.onload = on_load;
      element.onerror = on_error.bind(null, url);
      element.async = false;
      element.src = url;
      console.debug("Bokeh: injecting script tag for BokehJS library: ", url);
      document.head.appendChild(element);
    }
  };

  function inject_raw_css(css) {
    const element = document.createElement("style");
    element.appendChild(document.createTextNode(css));
    document.body.appendChild(element);
  }

  const js_urls = ["https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js", "https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js"];
  const css_urls = [];

  const inline_js = [    function(Bokeh) {
      Bokeh.set_log_level("info");
    },
function(Bokeh) {
    }
  ];

  function run_inline_js() {
    if (root.Bokeh !== undefined || force === true) {
      try {
            for (let i = 0; i < inline_js.length; i++) {
      inline_js[i].call(root, root.Bokeh);
    }

      } catch (error) {display_loaded(error);throw error;
      }if (force === true) {
        display_loaded();
      }} else if (Date.now() < root._bokeh_timeout) {
      setTimeout(run_inline_js, 100);
    } else if (!root._bokeh_failed_load) {
      console.log("Bokeh: BokehJS failed to load within specified timeout.");
      root._bokeh_failed_load = true;
    } else if (force !== true) {
      const cell = $(document.getElementById("f09f1c88-36e8-49d7-a361-1ff847212376")).parents('.cell').data().cell;
      cell.output_area.append_execute_result(NB_LOAD_WARNING)
    }
  }

  if (root._bokeh_is_loading === 0) {
    console.debug("Bokeh: BokehJS loaded, going straight to plotting");
    run_inline_js();
  } else {
    load_libs(css_urls, js_urls, function() {
      console.debug("Bokeh: BokehJS plotting callback run at", now());
      run_inline_js();
    });
  }
}(window));</script><div class="output text_html">
  <div id="ac232b54-cd2a-4339-b493-61703d293f42" data-root-id="p1058" style="display: contents;"></div>
</div><script type="application/javascript">(function(root) {
  function embed_document(root) {
  const docs_json = {"90d5f127-aee3-4f35-91f1-ecf4f1dfa185":{"version":"3.8.0","title":"Bokeh Application","config":{"type":"object","name":"DocumentConfig","id":"p1114","attributes":{"notifications":{"type":"object","name":"Notifications","id":"p1115"}}},"roots":[{"type":"object","name":"Figure","id":"p1058","attributes":{"width":800,"height":400,"x_range":{"type":"object","name":"DataRange1d","id":"p1059"},"y_range":{"type":"object","name":"DataRange1d","id":"p1060"},"x_scale":{"type":"object","name":"LinearScale","id":"p1068"},"y_scale":{"type":"object","name":"LinearScale","id":"p1069"},"title":{"type":"object","name":"Title","id":"p1061","attributes":{"text":"Hooke's Law with Model Fit"}},"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1099","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1093","attributes":{"selected":{"type":"object","name":"Selection","id":"p1094","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1095"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/2NggACN9Qv3tH3aZQ+hT9nHOVfyPjW9AuXfsgfxplx/ABV/Yg+mpjyHyr+yB0sHvIOq+2CfA5GAqv9iHwBmfIXq+25vBlQd5/wTqv+XvdSpbCDzD9Scf/ZQZzkAAJ6NW+CgAAAA"},"shape":[20],"dtype":"float64","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/wGgAF//fDCpKCrKzz/CY75SLuTZPyAKJB7jVvQ/cKAU++V1AUCpakDGS3H8P04obiPFAgJAzIg42b4NDUBpQ40Kl5gNQCRLfq8ZcAxAr/N0sUMjEkBMpiunQQUSQPtVjBQg6RNAsOe+KYQ4F0BiA46wFc4UQOlLkTOUExdA93JI9OVLG0DeekHHRUocQMf7HAplayBAt/hR/wQlIEDEvr5Dc5YgQJgtAhGgAAAA"},"shape":[20],"dtype":"float64","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1100","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1101"}}},"glyph":{"type":"object","name":"Scatter","id":"p1096","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"fill_color":{"type":"value","value":"blue"},"hatch_color":{"type":"value","value":"blue"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1097","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"blue"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"blue"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1098","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":10},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"blue"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"blue"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p1110","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1104","attributes":{"selected":{"type":"object","name":"Selection","id":"p1105","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1106"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/2NggACN9Qv3tH3aZQ+hT9nHOVfyPjW9AuXfsgfxplx/ABV/Yg+mpjyHyr+yB0sHvIOq+2CfA5GAqv9iHwBmfIXq+25vBlQd5/wTqv+XvdSpbCDzD9Scf/ZQZzkAAJ6NW+CgAAAA"},"shape":[20],"dtype":"float64","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"H4sIAAEAAAAC/wGgAF//AGj79j/J2D/lsIyOOvXpP+XWzZDqwvM/V1VV2jeL+j/lae6RwqkAQB4psjbpDQRAV+h12w9yB0CQpzmANtYKQMpm/SRdOg5AApPg5EHPEECeckI3VYESQDtSpIloMxRA1zEG3HvlFUBzEWguj5cXQBDxyYCiSRlArtAr07X7GkBKsI0lya0cQOaP73fcXx5Awrco5fcIIECQp1mOAeIgQLpmw0qgAAAA"},"shape":[20],"dtype":"float64","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1111","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1112"}}},"glyph":{"type":"object","name":"Line","id":"p1107","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"red","line_width":3}},"nonselection_glyph":{"type":"object","name":"Line","id":"p1108","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"red","line_alpha":0.1,"line_width":3}},"muted_glyph":{"type":"object","name":"Line","id":"p1109","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"red","line_alpha":0.2,"line_width":3}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1067","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p1080"},{"type":"object","name":"WheelZoomTool","id":"p1081","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p1082","attributes":{"dimensions":"both","overlay":{"type":"object","name":"BoxAnnotation","id":"p1083","attributes":{"syncable":false,"line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","handles":{"type":"object","name":"BoxInteractionHandles","id":"p1089","attributes":{"all":{"type":"object","name":"AreaVisuals","id":"p1088","attributes":{"fill_color":"white","hover_fill_color":"lightgray"}}}}}}}},{"type":"object","name":"SaveTool","id":"p1090"},{"type":"object","name":"ResetTool","id":"p1091"},{"type":"object","name":"HelpTool","id":"p1092"}]}},"left":[{"type":"object","name":"LinearAxis","id":"p1075","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1076","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1077"},"axis_label":"Force (F) [N]","major_label_policy":{"type":"object","name":"AllLabels","id":"p1078"}}}],"below":[{"type":"object","name":"LinearAxis","id":"p1070","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1071","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1072"},"axis_label":"Displacement (x) [m]","major_label_policy":{"type":"object","name":"AllLabels","id":"p1073"}}}],"center":[{"type":"object","name":"Grid","id":"p1074","attributes":{"axis":{"id":"p1070"}}},{"type":"object","name":"Grid","id":"p1079","attributes":{"dimension":1,"axis":{"id":"p1075"}}},{"type":"object","name":"Legend","id":"p1102","attributes":{"items":[{"type":"object","name":"LegendItem","id":"p1103","attributes":{"label":{"type":"value","value":"Experimental Data"},"renderers":[{"id":"p1099"}]}},{"type":"object","name":"LegendItem","id":"p1113","attributes":{"label":{"type":"value","value":"Linear Regression Fit"},"renderers":[{"id":"p1110"}]}}]}}]}}]}};
  const render_items = [{"docid":"90d5f127-aee3-4f35-91f1-ecf4f1dfa185","roots":{"p1058":"ac232b54-cd2a-4339-b493-61703d293f42"},"root_ids":["p1058"]}];
  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);
  }
  if (root.Bokeh !== undefined) {
    embed_document(root);
  } else {
    let attempts = 0;
    const timer = setInterval(function(root) {
      if (root.Bokeh !== undefined) {
        clearInterval(timer);
        embed_document(root);
      } else {
        attempts++;
        if (attempts > 100) {
          clearInterval(timer);
          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
        }
      }
    }, 10, root)
  }
})(window);</script></div>
</div>
</section>
<section id="what-happens-at-each-step-if-we-use-gradient-optimization">
<h3><span class="section-number">5.5.3. </span>What happens at each step if we use gradient optimization?<a class="headerlink" href="#what-happens-at-each-step-if-we-use-gradient-optimization" title="Link to this heading">#</a></h3>
<p>Let’s implement a simple function to do the iterations, and then test it</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="n">theta_0</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
    <span class="c1"># util function to print</span>
    <span class="n">mylog</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">ytheo</span> <span class="o">=</span> <span class="n">k_true</span> <span class="o">*</span> <span class="n">x_displacement</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">mylog</span><span class="p">(</span><span class="s2">&quot;Prediction with full data&quot;</span><span class="p">)</span>
        <span class="n">ypred</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">+</span> <span class="n">theta_1</span><span class="o">*</span><span class="n">x_displacement</span> 
        
        <span class="n">mylog</span><span class="p">(</span><span class="s2">&quot;Computing loss&quot;</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">ypred</span><span class="o">-</span><span class="n">ytheo</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">mylog</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">loss</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">mylog</span><span class="p">(</span><span class="s2">&quot;Computing the gradients&quot;</span><span class="p">)</span>
        <span class="n">grad_0</span> <span class="o">=</span> <span class="n">error</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">grad_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span><span class="o">*</span><span class="n">x_displacement</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">mylog</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">grad_0</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">grad_1</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">mylog</span><span class="p">(</span><span class="s2">&quot;Improving paramether estimation&quot;</span><span class="p">)</span>
        <span class="c1"># NOTE: learning rate hyper paramemeter alpha</span>
        <span class="n">theta_0</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">grad_0</span>
        <span class="n">theta_1</span> <span class="o">=</span> <span class="n">theta_1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">grad_1</span>
        <span class="n">mylog</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">theta_0</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">theta_1</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">mylog</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span>

    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta_0</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">theta_1</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">step</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction with full data
Computing loss
loss=np.float64(107.63157894736842)
Computing the gradients
grad_0=np.float64(-2.4999999999999996), grad_1=np.float64(-3.789473684210526)
Improving paramether estimation
theta_0=np.float64(1.25), theta_1=np.float64(1.3789473684210527)

(np.float64(1.25), np.float64(1.3789473684210527))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now with more steps</span>
<span class="nb">print</span><span class="p">(</span><span class="n">step</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(np.float64(1.0787927631817147e-07), np.float64(4.49999991017813))
</pre></div>
</div>
</div>
</div>
<div class="{exercise}Plotting docutils">
<p>Let’s plot the parameters as functions of the iterations and the learning rate</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Params</span>
<span class="n">NMAX</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> <span class="c1"># CHANGE THIS</span>

<span class="c1"># YOUR CODE HERE</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="practice-exercises">
<h3><span class="section-number">5.5.4. </span>Practice Exercises<a class="headerlink" href="#practice-exercises" title="Link to this heading">#</a></h3>
<p>Now it’s your turn! Apply what you’ve learned to new scientific datasets.</p>
<div class="exercise admonition" id="tensorflow">

<p class="admonition-title"><span class="caption-number">Exercise 5.1 </span> (Tensorflow/pytorch)</p>
<section id="exercise-content">
<p>Implement the same example but using <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> and <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>. Compare easy of use.</p>
</section>
</div>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-1">

<p class="admonition-title"><span class="caption-number">Exercise 5.2 </span> (Biology - Brain vs. Body Weight)</p>
<section id="exercise-content">
<p>Allometry is the study of the relationship of body size to shape, anatomy, and physiology. It is a well-known fact that the brain weight of mammals generally increases with body weight. Let’s model this relationship.</p>
<p><strong>Task:</strong></p>
<ol class="arabic simple">
<li><p>Load the provided data for various mammal species.</p></li>
<li><p>The relationship is often modeled on a log-log scale. First, amke a plot to justify that. Then, transform both <code class="docutils literal notranslate"><span class="pre">body_wt</span></code> and <code class="docutils literal notranslate"><span class="pre">brain_wt</span></code> by taking their natural logarithm (<code class="docutils literal notranslate"><span class="pre">np.log()</span></code>).</p></li>
<li><p>Fit a linear regression model to the log-transformed data.</p></li>
<li><p>Print the equation of your model.</p></li>
<li><p>Plot the log-transformed data as a scatter plot and overlay your regression line.</p></li>
</ol>
</section>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data for Exercise 1</span>
<span class="n">body_wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.385</span><span class="p">,</span> <span class="mf">0.48</span><span class="p">,</span> <span class="mf">1.35</span><span class="p">,</span> <span class="mf">465.0</span><span class="p">,</span> <span class="mf">36.33</span><span class="p">,</span> <span class="mf">27.66</span><span class="p">,</span> <span class="mf">1.04</span><span class="p">,</span> <span class="mf">4.235</span><span class="p">,</span> <span class="mf">10.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">600.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">6.8</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">,</span> <span class="mf">3.92</span><span class="p">,</span> <span class="mf">572.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.92</span><span class="p">,</span> <span class="mf">119.5</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">14.83</span><span class="p">,</span> <span class="mf">192.0</span><span class="p">])</span>
<span class="n">brain_wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">44.5</span><span class="p">,</span> <span class="mf">15.5</span><span class="p">,</span> <span class="mf">8.1</span><span class="p">,</span> <span class="mf">423.0</span><span class="p">,</span> <span class="mf">119.5</span><span class="p">,</span> <span class="mf">115.0</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mf">25.6</span><span class="p">,</span> <span class="mf">73.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">6.6</span><span class="p">,</span> <span class="mf">812.0</span><span class="p">,</span> <span class="mf">10.8</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">57.0</span><span class="p">,</span> <span class="mf">17.5</span><span class="p">,</span> <span class="mf">655.0</span><span class="p">,</span> <span class="mf">157.0</span><span class="p">,</span> <span class="mf">12.1</span><span class="p">,</span> <span class="mf">11.4</span><span class="p">,</span> <span class="mf">75.0</span><span class="p">,</span> <span class="mf">62.0</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">48.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">])</span>

<span class="c1"># 1. (Data is already loaded)</span>

<span class="c1"># YOUR CODE HERE</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="train-test-split">
<h2><span class="section-number">5.6. </span>Train-test split<a class="headerlink" href="#train-test-split" title="Link to this heading">#</a></h2>
<p>In order to test a model, it is customary to split the data set into train-test sets. The goal is to use the train data to train the model, and then use the test data, which corresponds to data not seen before, and check for the model performance. An overfitted model (small train error) will have a large variance (large test error), so its predictions will vary greatly when tested on new data. In general, a small train error does not guarantee a small test error.</p>
<p>To split the data into the train and test datasets, we can use the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function (check the manual). Then we train on the train data, and then we compare the predictions on the test data, suing different metrics. This is an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>

<span class="c1"># --- Step 1: Prepare Your Data ---</span>
<span class="c1"># Let&#39;s create some sample data for this example.</span>
<span class="c1"># Replace these with your actual &#39;x&#39; and &#39;y&#39; arrays.</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Create a single feature &#39;x&#39;. It needs to be a 2D array for sklearn.</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Create &#39;y&#39; with a linear relationship to &#39;x&#39; plus some noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># --- Step 2: Split Data into Training and Testing Sets ---</span>
<span class="c1"># We&#39;ll use 80% of the data for training and 20% for testing.</span>
<span class="c1"># &#39;test_size=0.2&#39; specifies the proportion of the data for the test set.</span>
<span class="c1"># &#39;random_state&#39; ensures that the split is the same every time you run the code,</span>
<span class="c1"># making the results reproducible.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of training data (X_train): </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of testing data (X_test): </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># --- Step 3: Create and Train the Linear Regression Model ---</span>
<span class="c1"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Fit the model to the training data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># You can inspect the learned parameters (optional)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Model Intercept (theta_0): </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model Coefficient (theta_1): </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># --- Step 4: Make Predictions on the Test Set ---</span>
<span class="c1"># Use the trained model to predict the y-values for the test data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>


<span class="c1"># --- Step 5: Compute Metrics to Evaluate the Model ---</span>
<span class="c1"># Calculate the R-squared (R²) score.</span>
<span class="c1"># This metric measures how well the model&#39;s predictions approximate the real values.</span>
<span class="c1"># An R² of 1 indicates a perfect fit.</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># You can also calculate other metrics like Mean Squared Error (MSE)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Model Evaluation ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R-squared (R²): </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of training data (X_train): (80, 1)
Shape of testing data (X_test): (20, 1)

Model Intercept (theta_0): 4.142913319458566
Model Coefficient (theta_1): 2.7993236574802762

--- Model Evaluation ---
R-squared (R²): 0.8072
Mean Squared Error (MSE): 0.6537
</pre></div>
</div>
</div>
</div>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-2">

<p class="admonition-title"><span class="caption-number">Exercise 5.3 </span> (Splitting data into train and test subsets)</p>
<section id="exercise-content">
<p>Use one of the previous examples to actually split the data into train and test sets, using sklearn functions, and then compute metrics like <span class="math notranslate nohighlight">\(R^2\)</span>.
BUT, do not use sklearn LinearRegression, since it uses the exact least square formulation. Use <code class="docutils literal notranslate"><span class="pre">SGDRegressor</span></code>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html</a>, and partial_fit, to get the loss function after each iteration. Finally, plot the loss function as a function of the iteration. Do not forget to standarize the data. gradient descent is sensible to that.</p>
</section>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># YOUR CODE HERE</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="conclusion-whats-next">
<h2><span class="section-number">5.7. </span>Conclusion &amp; What’s Next?<a class="headerlink" href="#conclusion-whats-next" title="Link to this heading">#</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ul class="simple">
<li><p>Supervised learning uses <strong>labeled data</strong> (X, y) to learn a predictive function.</p></li>
<li><p><strong>Regression</strong> predicts continuous values, while <strong>Classification</strong> predicts discrete categories.</p></li>
<li><p><strong>Linear Regression</strong> finds the best-fit line by minimizing a <strong>cost function</strong> (like MSE).</p></li>
<li><p><strong>Gradient Descent</strong> is the optimization algorithm used to find the model parameters that minimize the cost.</p></li>
<li><p>Libraries like <strong>Scikit-Learn</strong> make it incredibly easy to implement these powerful models.</p></li>
</ul>
<p><strong>What’s Next?</strong></p>
<ul class="simple">
<li><p>What if our data isn’t linear? We can use <strong>Polynomial Regression</strong>.</p></li>
<li><p>How do we handle classification problems? We’ll use models like <strong>Logistic Regression</strong> and <strong>Support Vector Machines</strong>.</p></li>
<li><p>What happens when we have many features? We need to be careful about <strong>overfitting</strong> and use techniques like <strong>regularization</strong>.</p></li>
</ul>
</section>
<section id="regularization">
<h2><span class="section-number">5.8. </span>Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h2>
<p>The bias-variance trade-off shows that is important to not go to the extremes when fitting a model. For example, when the model performs well on the training data but does not work well on the test data, we might need to add a <code class="docutils literal notranslate"><span class="pre">regularization</span></code> so penalize the cost function to improve the trade-off. One example is the so-called ridge regularization, where</p>
<div class="amsmath math notranslate nohighlight" id="equation-6803b478-4864-4a12-af01-f97b76cae58d">
<span class="eqno">(5.4)<a class="headerlink" href="#equation-6803b478-4864-4a12-af01-f97b76cae58d" title="Permalink to this equation">#</a></span>\[\begin{equation}
\Delta(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat y(x_i) - y_i \right)^2 + \lambda \sum_{j=1}^{n} \theta_j^2,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the hyper-regularization parameter. This controls the magnitude of the coefficients preventing overfitting. When <span class="math notranslate nohighlight">\(\lambda\)</span> is large, the parameters <span class="math notranslate nohighlight">\(\theta\)</span> decrease, which shrinks the impact of variables no so correlated with the output. When <span class="math notranslate nohighlight">\(\lambda\)</span> decreases, we basically converge to the usual linear regression.</p>
<p>There are several other regularization techniques, such as lasso regression (L1),</p>
<div class="amsmath math notranslate nohighlight" id="equation-ff439b8b-3509-4dec-931c-951ba016e1e8">
<span class="eqno">(5.5)<a class="headerlink" href="#equation-ff439b8b-3509-4dec-931c-951ba016e1e8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\Delta(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x_i) - y_i \right)^2 + \lambda \sum_{j=1}^{n} |\theta_j|, 
\end{equation}\]</div>
<p>Elastic Net regression, where both previous regressions are combines, and so on.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">ElasticNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Compare regularization methods</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Ridge&#39;</span><span class="p">:</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
    <span class="s1">&#39;Lasso&#39;</span><span class="p">:</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
    <span class="s1">&#39;ElasticNet&#39;</span><span class="p">:</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> CV R²: </span><span class="si">{</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (±</span><span class="si">{</span><span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="exercises">
<h3><span class="section-number">5.8.1. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h3>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-3">

<p class="admonition-title"><span class="caption-number">Exercise 5.4 </span> (Ridge, lasso and elastic net)</p>
<section id="exercise-content">
<p>Implement ridge regularization into our step by step approach. Check the role of several values. Now do the same for lasso, and then for Elastic Net.</p>
</section>
</div>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-4">

<p class="admonition-title"><span class="caption-number">Exercise 5.5 </span> (Ransac regression with outliers)</p>
<section id="exercise-content">
<p>Look for ransac regression in sklearn and implement an example showing how ransac can ignore outliers in data.</p>
</section>
</div>
</section>
</section>
<section id="multiple-linear-regression">
<h2><span class="section-number">5.9. </span>Multiple linear regression<a class="headerlink" href="#multiple-linear-regression" title="Link to this heading">#</a></h2>
<p>REF: <a class="reference external" href="https://www.digitalocean.com/community/tutorials/multiple-linear-regression-python">https://www.digitalocean.com/community/tutorials/multiple-linear-regression-python</a></p>
<p>Until now, we explore a single variable linear regression. But, in general, we have multiple variables problems. In this case, we want to study a model like</p>
<div class="amsmath math notranslate nohighlight" id="equation-be9bc908-3da0-40b2-a3b8-79df01c3b128">
<span class="eqno">(5.6)<a class="headerlink" href="#equation-be9bc908-3da0-40b2-a3b8-79df01c3b128" title="Permalink to this equation">#</a></span>\[\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon = \vec \beta \cdot \vec V + \epsilon,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec \beta\)</span> are the coefficients we want to optimize, and we have <span class="math notranslate nohighlight">\(n\)</span> “independent” variables.</p>
<p>There are several assumptions here to be able to apply linear regression, and there are some tests that you can use to check for them.</p>
<p>In this section we will still use the traditional Linear Regression model from scikit learn. The goal is to learn some metrics and techniques that could help understandad better the data.</p>
<section id="assumptions-of-multiple-linear-regression">
<h3><span class="section-number">5.9.1. </span>Assumptions of Multiple Linear Regression<a class="headerlink" href="#assumptions-of-multiple-linear-regression" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Assumption</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Test(s) to Check</p></th>
<th class="head text-left"><p>Python Implementation (<code class="docutils literal notranslate"><span class="pre">pandas</span></code> + <code class="docutils literal notranslate"><span class="pre">seaborn</span></code>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Linearity</strong></p></td>
<td class="text-left"><p>The relationship between predictors (X) and the outcome (y) is linear.</p></td>
<td class="text-left"><p><strong>Scatter Plots</strong> or <strong>Fitted vs. Residuals Plot</strong>. Look for a random scatter. Remember that you can tranform the data</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">seaborn</span> <span class="pre">as</span> <span class="pre">sns</span></code><br><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">model</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">fitted</span> <span class="pre">OLS</span> <span class="pre">model</span></code><br><code class="docutils literal notranslate"><span class="pre">residuals</span> <span class="pre">=</span> <span class="pre">model.resid</span></code><br><code class="docutils literal notranslate"><span class="pre">fitted</span> <span class="pre">=</span> <span class="pre">model.fittedvalues</span></code><br><code class="docutils literal notranslate"><span class="pre">sns.residplot(x=fitted,</span> <span class="pre">y=residuals,</span> <span class="pre">lowess=True)</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Independence of Residuals</strong></p></td>
<td class="text-left"><p>The residuals (errors) are independent of each other (no autocorrelation).</p></td>
<td class="text-left"><p><strong>Durbin-Watson Test</strong>. Look for a value around 2.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">statsmodels.stats.stattools</span> <span class="pre">import</span> <span class="pre">durbin_watson</span></code><br><code class="docutils literal notranslate"><span class="pre">dw_stat</span> <span class="pre">=</span> <span class="pre">durbin_watson(model.resid)</span></code><br><code class="docutils literal notranslate"><span class="pre">print(f&quot;Durbin-Watson:</span> <span class="pre">{dw_stat:.2f}&quot;)</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Homoscedasticity</strong></p></td>
<td class="text-left"><p>Residuals have constant variance across all levels of X.</p></td>
<td class="text-left"><p><strong>Breusch-Pagan Test</strong> or <strong>White Test</strong>. Look for a p-value &gt; 0.05. Visual check with <strong>Residuals vs. Fitted Plot</strong>.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">statsmodels.stats.api</span> <span class="pre">as</span> <span class="pre">sms</span></code><br><code class="docutils literal notranslate"><span class="pre">bp_test</span> <span class="pre">=</span> <span class="pre">sms.het_breuschpagan(model.resid,</span> <span class="pre">model.model.exog)</span></code><br><code class="docutils literal notranslate"><span class="pre">print(f&quot;Breusch-Pagan</span> <span class="pre">p-value:</span> <span class="pre">{bp_test[1]:.4f}&quot;)</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Normality of Residuals</strong></p></td>
<td class="text-left"><p>The residuals are approximately normally distributed.</p></td>
<td class="text-left"><p><strong>Jarque-Bera Test</strong> or <strong>Q-Q Plot</strong>. Points on the Q-Q plot should follow the line.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">statsmodels.api</span> <span class="pre">as</span> <span class="pre">sm</span></code><br><code class="docutils literal notranslate"><span class="pre">sm.qqplot(model.resid,</span> <span class="pre">line='s')</span></code><br><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">Jarque-Bera</span> <span class="pre">result</span> <span class="pre">is</span> <span class="pre">in</span> <span class="pre">model.summary()</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. No Perfect Multicollinearity</strong></p></td>
<td class="text-left"><p>Independent variables are not highly correlated with each other.</p></td>
<td class="text-left"><p><strong>A) Correlation Matrix (Preliminary)</strong><br><br><strong>B) Variance Inflation Factor (VIF) (Definitive)</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">##</span> <span class="pre">A)</span> <span class="pre">Correlation</span> <span class="pre">Matrix</span></code><br><code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">seaborn</span> <span class="pre">as</span> <span class="pre">sns</span></code><br><code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">matplotlib.pyplot</span> <span class="pre">as</span> <span class="pre">plt</span></code><br><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">df</span> <span class="pre">is</span> <span class="pre">your</span> <span class="pre">full</span> <span class="pre">DataFrame</span> <span class="pre">(X's</span> <span class="pre">and</span> <span class="pre">y)</span></code><br><code class="docutils literal notranslate"><span class="pre">corr_matrix</span> <span class="pre">=</span> <span class="pre">df.corr()</span></code><br><code class="docutils literal notranslate"><span class="pre">sns.heatmap(corr_matrix,</span> <span class="pre">annot=True,</span> <span class="pre">cmap='coolwarm')</span></code><br><code class="docutils literal notranslate"><span class="pre">plt.show()</span></code><br><br><code class="docutils literal notranslate"><span class="pre">##</span> <span class="pre">B)</span> <span class="pre">VIF</span></code><br><code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">statsmodels.stats.outliers_influence</span> <span class="pre">import</span> <span class="pre">variance_inflation_factor</span></code><br><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">X</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">DataFrame</span> <span class="pre">of</span> <span class="pre">predictors</span> <span class="pre">only</span></code><br><code class="docutils literal notranslate"><span class="pre">vif</span> <span class="pre">=</span> <span class="pre">[variance_inflation_factor(X.values,</span> <span class="pre">i)</span> <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(X.shape[1])]</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>It is also useful to check for the correlation between the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> and the independent variables <span class="math notranslate nohighlight">\(\vec X\)</span>. Large positive or negative correlations allow to select the most important variables for applying linear regression.</p>
<p>The following code shows an example of this:</p>
<hr class="docutils" />
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Create sample data representing a &quot;good&quot; scenario</span>
<span class="c1"># X1 and X2 are good predictors of y</span>
<span class="c1"># X3 is a weak predictor</span>
<span class="c1"># X1 and X2 have low correlation with each other</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>
<span class="n">X3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="c1"># Weak predictor</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X3</span> <span class="o">+</span> <span class="n">noise</span> <span class="c1"># X3 has a small effect</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The most important line of code in any analysis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7150aeac8182fa279e3520660a67cecfa23f70baa3cfa9871e04a3895ef0fa67.png" src="../../_images/7150aeac8182fa279e3520660a67cecfa23f70baa3cfa9871e04a3895ef0fa67.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X1&#39;</span><span class="p">:</span> <span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">:</span> <span class="n">X2</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">:</span> <span class="n">X3</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>

<span class="c1"># --- Generate the Correlation Matrix ---</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># --- Visualize it with a Heatmap ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Full Correlation Matrix (for y and X)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># --- Explicitly Analyze the Correlations ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- 1. Correlation with Dependent Variable (y) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;We want these values to be high (far from zero).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- 2. Correlation Among Independent Variables (X) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;We want the off-diagonal values here to be low (close to zero).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">corr</span><span class="p">())</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2b7a43863296bbd4189c2cd8edb2115e4cf4c8cf2edc817d96a65bf07e7aa43a.png" src="../../_images/2b7a43863296bbd4189c2cd8edb2115e4cf4c8cf2edc817d96a65bf07e7aa43a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- 1. Correlation with Dependent Variable (y) ---
We want these values to be high (far from zero).
y     1.000000
X1    0.825275
X3    0.092789
X2   -0.530057
Name: y, dtype: float64

--- 2. Correlation Among Independent Variables (X) ---
We want the off-diagonal values here to be low (close to zero).
          X1        X2        X3
X1  1.000000 -0.034033 -0.037654
X2 -0.034033  1.000000 -0.146354
X3 -0.037654 -0.146354  1.000000
</pre></div>
</div>
</div>
</div>
</section>
<section id="full-example-workflow-in-python">
<h3><span class="section-number">5.9.2. </span>Full example Workflow in Python<a class="headerlink" href="#full-example-workflow-in-python" title="Link to this heading">#</a></h3>
<p>Here is a quick summary of how you would typically check these assumptions after fitting a model.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.outliers_influence</span><span class="w"> </span><span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.stattools</span><span class="w"> </span><span class="kn">import</span> <span class="n">durbin_watson</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.stats.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sms</span>

<span class="c1"># 1. Prepare your data (assuming you have a pandas DataFrame `df`)</span>
<span class="c1"># Let&#39;s create some sample data for demonstration</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">X2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="c1"># X2 is somewhat correlated with X1</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X1&#39;</span><span class="p">:</span> <span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">:</span> <span class="n">X2</span><span class="p">})</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># Add a constant for the intercept</span>

<span class="c1"># 2. Fit the OLS model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.898
Model:                            OLS   Adj. R-squared:                  0.896
Method:                 Least Squares   F-statistic:                     428.0
Date:                Wed, 22 Oct 2025   Prob (F-statistic):           7.45e-49
Time:                        20:41:53   Log-Likelihood:                -311.60
No. Observations:                 100   AIC:                             629.2
Df Residuals:                      97   BIC:                             637.0
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          2.1903      1.049      2.089      0.039       0.109       4.272
X1             3.0667      0.337      9.101      0.000       2.398       3.735
X2             4.8641      0.617      7.883      0.000       3.639       6.089
==============================================================================
Omnibus:                        5.239   Durbin-Watson:                   2.103
Prob(Omnibus):                  0.073   Jarque-Bera (JB):                5.526
Skew:                           0.306   Prob(JB):                       0.0631
Kurtosis:                       3.975   Cond. No.                         11.9
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

================================================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">significant_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
<span class="nb">print</span><span class="p">(</span><span class="n">significant_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;const&#39;, &#39;X1&#39;, &#39;X2&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 3. Check Assumptions</span>

<span class="c1"># --- Linearity &amp; Homoscedasticity (Visual Check) ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for Linearity and Homoscedasticity...&quot;</span><span class="p">)</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fittedvalues</span>
<span class="n">sns</span><span class="o">.</span><span class="n">residplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">fitted</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">residuals</span><span class="p">,</span> <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Residuals vs. Fitted Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># --- Homoscedasticity (Statistical Test) ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for Homoscedasticity (Breusch-Pagan Test)...&quot;</span><span class="p">)</span>
<span class="n">bp_test</span> <span class="o">=</span> <span class="n">sms</span><span class="o">.</span><span class="n">het_breuschpagan</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Breusch-Pagan Test p-value: </span><span class="si">{</span><span class="n">bp_test</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">bp_test</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: No evidence of heteroscedasticity (Good).&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: Evidence of heteroscedasticity found (Bad).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># --- Independence of Residuals ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for Independence of Residuals (Durbin-Watson Test)...&quot;</span><span class="p">)</span>
<span class="n">dw_stat</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Durbin-Watson statistic: </span><span class="si">{</span><span class="n">dw_stat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="mf">1.5</span> <span class="o">&lt;</span> <span class="n">dw_stat</span> <span class="o">&lt;</span> <span class="mf">2.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: No significant autocorrelation (Good).&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: Potential autocorrelation detected (Bad).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># --- Normality of Residuals ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for Normality of Residuals (Q-Q Plot and Jarque-Bera)...&quot;</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Q-Q Plot of Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># The Jarque-Bera test result is in the model summary `Prob(JB)`</span>
<span class="n">jb_prob</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jarque-Bera test probability: </span><span class="si">{</span><span class="n">jb_prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">jb_prob</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>
     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: Residuals appear to be normally distributed (Good).&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result: Residuals may not be normally distributed (Bad).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># --- Multicollinearity ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for Multicollinearity (VIF)...&quot;</span><span class="p">)</span>
<span class="c1"># Note: We check VIF on the design matrix X without the constant</span>
<span class="n">X_no_const</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vif_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;feature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_no_const</span><span class="o">.</span><span class="n">columns</span>
<span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;VIF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_no_const</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_no_const</span><span class="o">.</span><span class="n">columns</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vif_data</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;VIF&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result: No significant multicollinearity detected (Good).&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result: Potential multicollinearity detected (Bad).&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Checking for Linearity and Homoscedasticity...
</pre></div>
</div>
<img alt="../../_images/3b2b785802d186898ede5d90cb26058129c3e8c05327cd971f03c83b80fec8ca.png" src="../../_images/3b2b785802d186898ede5d90cb26058129c3e8c05327cd971f03c83b80fec8ca.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Checking for Homoscedasticity (Breusch-Pagan Test)...
Breusch-Pagan Test p-value: 0.4893
Result: No evidence of heteroscedasticity (Good).

================================================================================

Checking for Independence of Residuals (Durbin-Watson Test)...
Durbin-Watson statistic: 2.10
Result: No significant autocorrelation (Good).

================================================================================

Checking for Normality of Residuals (Q-Q Plot and Jarque-Bera)...
</pre></div>
</div>
<img alt="../../_images/63009280a8a2a9468d15ee49ba4e6d60d9555e1cd02ac7fb6a51a58a05284b8d.png" src="../../_images/63009280a8a2a9468d15ee49ba4e6d60d9555e1cd02ac7fb6a51a58a05284b8d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jarque-Bera test probability: 2.103
Result: Residuals appear to be normally distributed (Good).

================================================================================

Checking for Multicollinearity (VIF)...
  feature       VIF
0      X1  9.923332
1      X2  9.923332

Result: Potential multicollinearity detected (Bad).
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="data-pre-processing-and-post-processing-tips">
<h2><span class="section-number">5.10. </span>Data pre-processing and post-processing tips<a class="headerlink" href="#data-pre-processing-and-post-processing-tips" title="Link to this heading">#</a></h2>
<p>What happens if there is missing data? or the independent variables are of very different magnitudes? this could affect the actual analysis, so it is better to perform a data cleaning or pre-processing stage.</p>
<p>For example, for missing data detection, you can use</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
<p>If you find any missing data, you have to carefully analyze what to do.</p>
<p>You should also use the correlation matrix to select the more relevant variables to use in the model.</p>
<p>Additionally, it is useful to standardize data so their mean become 0 and its variance 1. This ensures that no variable dominates the model. To do so, you can use a scaller like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Initialize the StandardScaler object</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Fit the scaler to the data and transform it</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Print the scaled data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>After this, you can apply some statistical test to check for the multiple linear assumptions.</p>
<p>Finally, you can also split your data into training and testing subsets, to be able to check for model prediction, by using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># The &#39;LinearRegression&#39; model is initialized and fitted to the training data.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># The model is used to predict the target variable for the test set.</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Squared Error:&quot;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R-squared:&quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<section id="cross-validation">
<h3><span class="section-number">5.10.1. </span>Cross-validation<a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h3>
<p>After the initial results and tests, you can use cross-validation to check for your model performance with unseen data, by splitting your data in k groups ad computing R2 as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cross-Validation Scores:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean CV R^2:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Line Plot for Cross-Validation Scores</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">scores</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;R-squared&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cross-Validation R-squared Scores&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="feature-selection">
<h3><span class="section-number">5.10.2. </span>Feature selection<a class="headerlink" href="#feature-selection" title="Link to this heading">#</a></h3>
<p>It is possible to recursively try to eliminate features until some k-features are selected. This is called recursive features elimination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RFE</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Features:&quot;</span><span class="p">,</span> <span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">)</span>

<span class="c1"># Bar Plot of Feature Rankings</span>
<span class="n">feature_ranking</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
   <span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">selected_features</span><span class="p">,</span>
   <span class="s1">&#39;Ranking&#39;</span><span class="p">:</span> <span class="n">rfe</span><span class="o">.</span><span class="n">ranking_</span>
<span class="p">})</span>
<span class="n">feature_ranking</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Ranking&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Ranking&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Feature Ranking (Lower is Better)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Ranking&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3><span class="section-number">5.10.3. </span>Exercises<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>For the following exercises, perform a full analysis with explicit statistical tests computation and interpretation. Explain why you use some of the features, or why you need to use all of them. Plot correlation matrices and so on. Also perform a previous pre-processing stage.</p>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-5">

<p class="admonition-title"><span class="caption-number">Exercise 5.6 </span> (A large number of features model)</p>
<section id="exercise-content">
<p>Use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression">sklearn.datasets.make_regression</a> to generate a 100 features model but 10 relevant features.</p>
</section>
</div>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-6">

<p class="admonition-title"><span class="caption-number">Exercise 5.7 </span> (Star dataset)</p>
<section id="exercise-content">
<p>Use the star dataset to try to predict luminosity: <a class="reference external" href="https://www.kaggle.com/datasets/waqi786/stars-dataset">https://www.kaggle.com/datasets/waqi786/stars-dataset</a>.</p>
</section>
</div>
<div class="exercise admonition" id="lectures/04-LinearRegression/LinearRegression-exercise-7">

<p class="admonition-title"><span class="caption-number">Exercise 5.8 </span> (Polynomial regression)</p>
<section id="exercise-content">
<p>Generate a random set following the model</p>
<div class="amsmath math notranslate nohighlight" id="equation-885823b1-ba44-40a1-b1d2-8148febc7267">
<span class="eqno">(5.7)<a class="headerlink" href="#equation-885823b1-ba44-40a1-b1d2-8148febc7267" title="Permalink to this equation">#</a></span>\[\begin{equation}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon
\end{equation}\]</div>
<p>and apply (polynomial) linear regression to get the coefficients.  Check <a class="reference external" href="https://www.geeksforgeeks.org/machine-learning/python-implementation-of-polynomial-regression/">https://www.geeksforgeeks.org/machine-learning/python-implementation-of-polynomial-regression/</a></p>
</section>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "iluvatar1/ML4Sci-lectures",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/04-LinearRegression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../03-PCA/Intro-PCA.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4. </span>An Introduction to Principal Component Analysis (PCA)</p>
      </div>
    </a>
    <a class="right-next"
       href="../05-LogisticRegression/LogisticRegression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>An Introduction to Logistic Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-what-is-machine-learning">5.1. Review: What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">5.2. Supervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-regression-predicting-a-continuous-value">5.2.1. A. Regression: Predicting a Continuous Value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-classification-predicting-a-discrete-category">5.2.2. B. Classification: Predicting a Discrete Category</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.3. Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-applications-to-basic-sciences">5.3.1. Some applications to basic sciences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal">5.3.2. The Goal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-find-the-best-line-classical-approach">5.3.3. How Do We Find the “Best” Line? Classical approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-find-the-best-line-learning-the-coefficients">5.3.4. How do we find the “Best” Line? Learning the coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-gradient-descent">5.4. Optimization with Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-happening-at-each-step">5.4.1. What is happening at each step?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-basic-example-hookes-law">5.5. A basic example: Hooke’s Law</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-generation">5.5.1. Data Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-using-sklearn">5.5.2. Linear regression using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-at-each-step-if-we-use-gradient-optimization">5.5.3. What happens at each step if we use gradient optimization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-exercises">5.5.4. Practice Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">5.6. Train-test split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-whats-next">5.7. Conclusion &amp; What’s Next?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">5.8. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">5.8.1. Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">5.9. Multiple linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-multiple-linear-regression">5.9.1. Assumptions of Multiple Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-example-workflow-in-python">5.9.2. Full example Workflow in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pre-processing-and-post-processing-tips">5.10. Data pre-processing and post-processing tips</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">5.10.1. Cross-validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection">5.10.2. Feature selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.10.3. Exercises</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Veronica Arias, Carlos Viviescas, William Oquendo
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>