{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0d6e3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "**Goal:** By the end of this session, you will understand the core concepts of supervised learning, know the difference between regression and classification, and be able to build, train, and interpret a simple Linear Regression model using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d5668",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Review: What is Machine Learning?\n",
    "\n",
    "At its core, **Machine Learning (ML)** is the science of getting computers to learn and act like humans do, and improve their learning over time in an autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\n",
    "\n",
    "\n",
    "\n",
    "1.  **Supervised Learning:** Learning from data that is **labeled**. You provide the algorithm with examples of inputs and their corresponding correct outputs. The goal is to learn a general rule that maps inputs to outputs. (This is our focus today).\n",
    "2.  **Unsupervised Learning:** Learning from data that is **unlabeled**. The algorithm tries to find patterns, structures, or clusters in the data on its own.\n",
    "3.  **Reinforcement Learning:** An agent learns to perform actions in an environment to maximize a cumulative reward. It learns by trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984eadc",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "> **Supervised Learning:** Given a dataset of input features **X** and corresponding output labels **y**, we want to learn a function `h` (for hypothesis) such that `h(X)` is a good predictor for **y**.\n",
    "\n",
    "There are two primary types of supervised learning problems:\n",
    "\n",
    "### A. Regression: Predicting a Continuous Value\n",
    "The output `y` is a continuous, numerical value.\n",
    "- **Question:** Based on a material's temperature, what will its electrical resistance be?\n",
    "- **Question:** Given the mass of a star, what is its expected luminosity?\n",
    "- **Our main tool today:** **Linear Regression** See [Visually Explained: Linear regression](https://www.youtube.com/watch?v=CtsRRUddV2s)\n",
    "\n",
    "### B. Classification: Predicting a Discrete Category\n",
    "The output `y` is a discrete category or class label.\n",
    "- **Question:** Based on a cell's size and shape, is it cancerous or benign?\n",
    "- **Question:** Given the energy and momentum from a particle collider, did we detect an electron or a muon?\n",
    "- **A common tool:** **Logistic Regression** (despite its name, it's for classification!)\n",
    "\n",
    "| Feature | Linear Regression | Logistic Regression |\n",
    "| :---- | :---- | :---- |\n",
    "| **Problem Type** | Regression (predicting continuous values) | Classification (predicting categorical outcomes) |\n",
    "| **Output** | Continuous numerical value (e.g., price, temperature) | Probability (0 to 1), which is then mapped to a class |\n",
    "| **Dependent Variable** | Continuous | Categorical (binary or multi-class) |\n",
    "| **Underlying Function** | Linear equation: y=β0​+β1​x1​+...+βn​xn​ | Sigmoid (logistic) function applied to a linear equation: p=1+e−(β0​+β1​x1​+...+βn​xn​)1​ |\n",
    "| **Cost Function** | Mean Squared Error (MSE), Root Mean Squared Error (RMSE) | Log Loss (Binary Cross-Entropy), Cross-Entropy |\n",
    "| **Interpretation of Coefficients** | Change in the dependent variable for a one-unit change in the independent variable | Change in the log-odds of the dependent variable for a one-unit change in the independent variable |\n",
    "| **Assumptions** | Linearity, independence of errors, homoscedasticity, normality of residuals, no multicollinearity | Linearity of independent variables with log-odds, independence of observations, no multicollinearity |\n",
    "| **Common Use Cases** | Predicting house prices, sales forecasting, predicting exam scores, trend analysis | Spam detection, disease prediction (e.g., presence/absence), customer churn prediction, sentiment analysis |\n",
    "| **Evaluation Metrics** | MSE, RMSE, R-squared, MAE | Accuracy, Precision, Recall, F1-Score, ROC-AUC |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941f0b2-a606-464e-b0bc-0a054b8f304f",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear Regression is one of the simplest and most interpretable machine learning models. It assumes a linear relationship between the input features and the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b870c3a-7310-473d-9bce-17a198618560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.linear_regression_example_plot import generate_and_plot_regression_problems\n",
    "fig_regression_problems = generate_and_plot_regression_problems()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeab71a-a99c-4057-8de9-51f9e26fc5a2",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "To find the \"best-fit\" line that describes the data. For a single input feature `x`, the equation of the line is:\n",
    "\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x $$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ (y-hat) is the **predicted value**.\n",
    "- $x$ is the **input feature**.\n",
    "- $\\theta_0$ (theta-zero) is the **y-intercept** (also called the bias). It's the value of $\\hat{y}$ when $x=0$.\n",
    "- $\\theta_1$ (theta-one) is the **slope** or **coefficient**. It represents the change in $\\hat{y}$ for a one-unit change in $x$.\n",
    "\n",
    "Our goal is to find the optimal values for $\\theta_0$ and $\\theta_1$ that make our line fit the data as closely as possible. Notice that it is possible to also add non-linear relationships even if the method is called linear regression.\n",
    "\n",
    "> **Note** In ML, We can apply linear regression to non-linear problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15301f1-1a51-4ce2-b78a-89f25eadc491",
   "metadata": {},
   "source": [
    "## How Do We Find the \"Best\" Line?\n",
    "\n",
    "We need a way to quantify how \"wrong\" our line is. We do this with a **Cost Function** (or Loss Function).\n",
    "\n",
    "1.  For each data point $(x_i, y_i)$, we calculate the difference between the **actual value** ($y_i$) and the **predicted value** ($\\hat{y}_i$). This difference is called the **residual** or **error**.\n",
    "2.  We square these errors (so positive and negative errors don't cancel out) and sum them up.\n",
    "3.  We take the average.\n",
    "\n",
    "This gives us the **Mean Squared Error (MSE)** cost function:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 = \\frac{1}{2m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i)^2 $$\n",
    "\n",
    "- $m$ is the number of data points.\n",
    "- $J(\\theta_0, \\theta_1)$ is the cost for a specific choice of $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "Our goal is to find the values of $\\theta_0$ and $\\theta_1$ that **minimize** this cost function $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1642a-0307-4e37-ab5e-9783dd8910de",
   "metadata": {},
   "source": [
    "#### Optimization with Gradient Descent\n",
    "\n",
    "How do we find the minimum of the cost function? We use an algorithm called **Gradient Descent**.\n",
    "\n",
    "**Analogy:** Imagine you are a hiker in a foggy valley and you want to get to the lowest point. You can't see the whole valley, but you can feel the slope of the ground right under your feet. What do you do? You take a step in the steepest downward direction.\n",
    "\n",
    "This is exactly what Gradient Descent does:\n",
    "1.  Start with some random values for $\\theta_0$ and $\\theta_1$.\n",
    "2.  Calculate the gradient (the \"slope\") of the cost function at that point.\n",
    "3.  Take a small step in the opposite direction of the gradient (downhill).\n",
    "4.  Repeat until you reach the bottom (the minimum), where the slope is zero.\n",
    "\n",
    "```{tip}\n",
    "For a nice visualization of gradient descent, check: <https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html>\n",
    "``` \n",
    "\n",
    "The size of the \"step\" you take is called the **learning rate** (alpha, $\\alpha$). A small learning rate will converge slowly, while a large one might overshoot the minimum. See <https://www.youtube.com/watch?v=gsfbWn4Gy5Q>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206af026",
   "metadata": {},
   "source": [
    "## A Physics Example - Hooke's Law\n",
    "\n",
    "Hooke's Law is a fundamental principle in physics that states the force (`F`) needed to extend or compress a spring by some distance (`x`) is directly proportional to that distance.\n",
    "\n",
    "$$ F = kx $$\n",
    "\n",
    "This is a perfect linear relationship! We can use linear regression to find the spring constant `k` from experimental data. Let's assume we conducted an experiment and got some noisy measurements.\n",
    "\n",
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5079b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate some experimental data\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume the true spring constant k is 4.5 N/m\n",
    "k_true = 4.5\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# Displacement (x) in meters. This is our feature X.\n",
    "# The .reshape(-1, 1) is needed because scikit-learn expects 2D arrays for features.\n",
    "x_displacement = np.linspace(0, 2, 20).reshape(-1, 1)\n",
    "\n",
    "# Force (F) in Newtons. This is our target y.\n",
    "# We'll calculate the true force and add some random \"measurement noise\"\n",
    "noise = np.random.normal(0, 0.5, x_displacement.shape)\n",
    "y_force = k_true * x_displacement + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup visualization \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up plots for a nice look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6)})\n",
    "\n",
    "# Step 3: Visualize the data\n",
    "plt.scatter(x_displacement, y_force, color='blue', edgecolor='k', s=80, label='Experimental Data')\n",
    "plt.xlabel(\"Displacement (x) [m]\")\n",
    "plt.ylabel(\"Force (F) [N]\")\n",
    "plt.title(\"Hooke's Law: Force vs. Displacement\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c01a1",
   "metadata": {},
   "source": [
    "This looks like a good candidate for linear regression! The data points roughly follow a straight line.\n",
    "### Linear regression using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model using Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a linear regression model object\n",
    "# Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using our data\n",
    "# The .fit() method is where the 'learning' (Gradient Descent) happens!\n",
    "model.fit(x_displacement, y_force) # USE ALL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "\n",
    "# Get the learned parameters (theta_0 and theta_1)\n",
    "# .intercept_ is an array, so we take the first element\n",
    "theta_0 = model.intercept_[0]\n",
    "# .coef_ is a 2D array, so we access it with [0][0]\n",
    "theta_1 = model.coef_[0][0]\n",
    "\n",
    "print(f\"The model has learned the following equation:\")\n",
    "print(f\"Force = {theta_0:.3f} + {theta_1:.3f} * Displacement\\n\")\n",
    "\n",
    "print(f\"The estimated spring constant (k) is: {theta_1:.3f} N/m\")\n",
    "print(f\"The true spring constant was: {k_true} N/m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3c6f8",
   "metadata": {},
   "source": [
    "That's pretty close! Our model successfully estimated the spring constant from the noisy data. The small non-zero intercept `theta_0` is a result of the random noise we added; in a perfect world, it would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e25fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model's fit\n",
    "\n",
    "# Generate predictions from our model for the x values\n",
    "y_predicted = model.predict(x_displacement)\n",
    "\n",
    "# Plot the original data\n",
    "plt.scatter(x_displacement, y_force, color='blue', edgecolor='k', s=80, label='Experimental Data')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x_displacement, y_predicted, color='red', linewidth=3, label='Linear Regression Fit')\n",
    "\n",
    "plt.xlabel(\"Displacement (x) [m]\")\n",
    "plt.ylabel(\"Force (F) [N]\")\n",
    "plt.title(\"Hooke's Law with Model Fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3d74c-cb50-42f3-be0f-ed08026ca8df",
   "metadata": {},
   "source": [
    "## Step by step\n",
    "Doing  the linear regression with `sklearn` hides a lot of complexity. But it is useful to stop for a moment and try to understand what is happening at each step. What does it mean to compute the gradient? how is the gradient optimizing the solution parameters? those are the questions to be treated here. We will use the same example as before (same data). The general algorithm would be\n",
    "- Give some initial value to params\n",
    "- For $N$ steps:\n",
    "  + Predict : $\\hat y = \\theta_0 + \\theta_1 x$\n",
    "  + compute error and loss: $\\Delta = \\dfrac{1}{2m} \\sum (\\hat y - y)^2$ \n",
    "  + compute gradients: $\\dfrac{\\partial \\Delta}{\\partial \\theta_0}$, $\\dfrac{\\partial \\Delta}{\\partial \\theta_1}$ (autodiff)\n",
    "  + improve params: $\\theta = \\theta -\\alpha \\nabla_\\theta$ (back-propagation)\n",
    "\n",
    "To do so, we will implement a simple function to do the iterations, and then test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a461bc8-3eab-4f2a-b55c-4be46257c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(theta_0:float, theta_1:float, N:int = 1, alpha: float = 0.1, verbose:bool = False) -> (float, float):\n",
    "    # util function to print\n",
    "    mylog = lambda msg: print(msg) if verbose else None\n",
    "\n",
    "    for ii in np.arange(0, N):\n",
    "        mylog(\"Prediction with full data\")\n",
    "        ypred = theta_0 + theta_1*x_displacement \n",
    "        \n",
    "        mylog(\"Computing loss\")\n",
    "        ytheo = k_true * x_displacement\n",
    "        error = ypred-ytheo\n",
    "        loss = np.power(error, 2).sum()/2\n",
    "        mylog(f\"{loss=}\")\n",
    "        \n",
    "        mylog(\"Computing the gradients\")\n",
    "        grad_0 = error.mean()\n",
    "        grad_1 = (error*x_displacement).mean()\n",
    "        mylog(f\"{grad_0=}, {grad_1=}\")\n",
    "        \n",
    "        mylog(\"Improving paramether estimation\")\n",
    "        # NOTE: learning rate hyper paramemeter alpha\n",
    "        theta_0 = theta_0 - alpha*grad_0\n",
    "        theta_1 = theta_1 - alpha*grad_1\n",
    "        mylog(f\"{theta_0=}, {theta_1=}\")\n",
    "\n",
    "    return theta_0, theta_1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9993c-3329-4ede-823d-9e49e3e4355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = 1.0\n",
    "theta_1 = 1.0\n",
    "print(step(theta_0, theta_1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f71bb1-6ae2-4a59-bde9-39b540730a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with more steps\n",
    "print(step(theta_0, theta_1, N=1000, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38f4d6-035e-45e6-a190-9856cf21eae1",
   "metadata": {},
   "source": [
    "Let's plot the parameters as functions of the iterations and the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be0d5c-49d0-40ee-8c79-cd021ce1ca33",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29186d77bf92ff08a789da4672326079",
     "grade": true,
     "grade_id": "cell-4a548389cb352a24",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 2, sharex = True, sharey = True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c68ea",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Now it's your turn! Apply what you've learned to new scientific datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9ed79-7ec0-4e8b-af2a-174182058298",
   "metadata": {},
   "source": [
    "### Tensorflow/pytorch\n",
    "Implement the same example but using `tensorflow` and `pytorch`. Compare easy of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906b62e",
   "metadata": {},
   "source": [
    "### Biology - Brain vs. Body Weight\n",
    "\n",
    "Allometry is the study of the relationship of body size to shape, anatomy, and physiology. It is a well-known fact that the brain weight of mammals generally increases with body weight. Let's model this relationship.\n",
    "\n",
    "**Task:**\n",
    "1.  Load the provided data for various mammal species.\n",
    "2.  The relationship is often modeled on a log-log scale. Transform both `body_wt` and `brain_wt` by taking their natural logarithm (`np.log()`).\n",
    "3.  Fit a linear regression model to the log-transformed data.\n",
    "4.  Print the equation of your model.\n",
    "5.  Plot the log-transformed data as a scatter plot and overlay your regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd148d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e4892d43a218bf4780a0a6ed70c91a",
     "grade": true,
     "grade_id": "cell-4c234d42b5175162",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data for Exercise 1\n",
    "body_wt = np.array([3.385, 0.48, 1.35, 465.0, 36.33, 27.66, 1.04, 4.235, 10.55, 0.55, 1.0, 600.0, 3.5, 3.5, 6.8, 35.0, 3.92, 572.0, 180.0, 2.5, 1.92, 119.5, 85.0, 0.75, 14.83, 192.0])\n",
    "brain_wt = np.array([44.5, 15.5, 8.1, 423.0, 119.5, 115.0, 5.5, 25.6, 73.5, 2.4, 6.6, 812.0, 10.8, 12.3, 37.0, 57.0, 17.5, 655.0, 157.0, 12.1, 11.4, 75.0, 62.0, 4.7, 48.0, 180.0])\n",
    "\n",
    "# 1. (Data is already loaded)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce5285",
   "metadata": {},
   "source": [
    "## Conclusion & What's Next?\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Supervised learning uses **labeled data** (X, y) to learn a predictive function.\n",
    "- **Regression** predicts continuous values, while **Classification** predicts discrete categories.\n",
    "- **Linear Regression** finds the best-fit line by minimizing a **cost function** (like MSE).\n",
    "- **Gradient Descent** is the optimization algorithm used to find the model parameters that minimize the cost.\n",
    "- Libraries like **Scikit-Learn** make it incredibly easy to implement these powerful models.\n",
    "\n",
    "**What's Next?**\n",
    "- What if our data isn't linear? We can use **Polynomial Regression**.\n",
    "- How do we handle classification problems? We'll use models like **Logistic Regression** and **Support Vector Machines**.\n",
    "- What happens when we have many features? We need to be careful about **overfitting** and use techniques like **regularization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29c5a4-658d-468a-882d-8eebdfabe6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
