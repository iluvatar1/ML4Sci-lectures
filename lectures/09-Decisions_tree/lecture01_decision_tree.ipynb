{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "\n",
    "This popular and powerful mmodel is constructed by learning a set of simple decision rules inferred from the data features in order to predict the value of a target variable. It is used for classification and regression. Decision Trees are an example of non-parametric supervised machine learning method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree - Classification\n",
    "\n",
    "DTs are capeble of multi-class classification on a dataset. \n",
    "\n",
    "As with all supervised classification method, the input is the data set $D = \\{(\\mathbf{x}_i,\\mathbf{y}_i)\\}_{i=1}^{N} $ with features $\\mathbf{x}_i \\in \\mathbb{R}^{n}$ and labels $\\mathbf{y}_i \\in \\mathbb{R}^{l}$.\n",
    "\n",
    "Let us check this example with the **iris** dataset (again!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris() # load the dataset\n",
    "print(iris.DESCR)  # provides a description of the dataset\n",
    "\n",
    "# build the decision tree\n",
    "# complete model\n",
    "X, y = iris.data, iris.target\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "# plot - tree visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "tree.plot_tree(clf, proportion=False)\n",
    "plt.title(\"iris dataset - Decision Tree classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the restricted model\n",
    "X = iris.data[:,[0,1]]\n",
    "y = iris.target\n",
    "clf_res = tree.DecisionTreeClassifier()\n",
    "clf_res = clf.fit(X, y)\n",
    "\n",
    "# Now plot the decision boundary using a fine mesh as input to a\n",
    "# filled contour plot\n",
    "cmap = plt.cm.RdYlBu\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "plot_step = 0.1\n",
    "xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    "        )\n",
    "Z = clf_res.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "plt.scatter(\n",
    "    X[:,0],\n",
    "    X[:,1],\n",
    "    c=y,\n",
    "    cmap=ListedColormap([\"r\", \"y\", \"b\"]),\n",
    "    edgecolor=\"k\",\n",
    "    s=20,\n",
    ")\n",
    "plt.title(\"iris dataset - Decision Tree classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree - Regression\n",
    "\n",
    "DTs can also be use for regressions. \n",
    "\n",
    "Here we use the decision tree to fit a curve describing damped oscillations $\\left(e^{-\\frac{1}{2}x^2} \\cos(3x)\\right)$ with added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# The data - the function to fit\n",
    "rng = np.random.RandomState(1234)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)    # x-axis values\n",
    "y = (np.exp(-0.5 * X) * np.cos(3 * X)).ravel()  # flattes the data of the function\n",
    "y[::5] += 1 * (0.5 - rng.rand(16))      # places noise every 5 elements of data\n",
    "\n",
    "# the model - regression\n",
    "regr = DecisionTreeRegressor(max_depth=2)\n",
    "regr.fit(X,y)\n",
    "\n",
    "# predictions\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] # test data\n",
    "y_test = regr.predict(X_test)\n",
    "\n",
    "# plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_test, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Decision Trees always overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "**Task:**\n",
    "Build decision trees for the regression example changing the tree depth or maximum number of leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# The data - the function to fit\n",
    "rng = np.random.RandomState(1234)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)    # x-axis values\n",
    "y = (np.exp(-0.5 * X) * np.cos(3 * X)).ravel()  # flattes the data of the function\n",
    "y[::5] += 1 * (0.5 - rng.rand(16))      # places noise every 5 elements of data\n",
    "\n",
    "# the model - regression\n",
    "regr = DecisionTreeRegressor(max_depth=2)\n",
    "regr.fit(X,y)\n",
    "# models with different depth\n",
    "regr_d1 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_d1.fit(X,y)\n",
    "# models with different number of leaves\n",
    "regr_nl1 = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "regr_nl1.fit(X,y)\n",
    "\n",
    "# predictions\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] # test data\n",
    "y_test = regr.predict(X_test)\n",
    "y_test_d1 = regr_d1.predict(X_test)\n",
    "y_test_nl1 = regr_nl1.predict(X_test)\n",
    "\n",
    "# plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_test, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_test_d1, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.plot(X_test, y_test_nl1, color=\"red\", label=\"max_leaf_nodes=3\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load the dataser\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# split the data into training and test (default test = 25%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# build a model with a maximum number of leaf nodes.\n",
    "clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# information on the model\n",
    "n_nodes = clf.tree_.node_count # number of nodes\n",
    "children_left = clf.tree_.children_left # ids of left child\n",
    "children_right = clf.tree_.children_right # ids of right child\n",
    "feature = clf.tree_.feature # feature used for the split\n",
    "threshold = clf.tree_.threshold # threshold value for the split\n",
    "values = clf.tree_.value # summary of the data\n",
    "\n",
    "# identifying splitting from leaf nodes.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "while len(stack) > 0:\n",
    "    # `pop` ensures each node is only visited once\n",
    "    node_id, depth = stack.pop()\n",
    "    node_depth[node_id] = depth\n",
    "\n",
    "    # If the left and right child of a node is not the same we have a split\n",
    "    # node\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    # If a split node, append left and right children and depth to `stack`\n",
    "    # so we can loop through them\n",
    "    if is_split_node:\n",
    "        stack.append((children_left[node_id], depth + 1))\n",
    "        stack.append((children_right[node_id], depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "\n",
    "# report on the tree structure\n",
    "print(\n",
    "    \"The binary tree structure has {n} nodes and has \"\n",
    "    \"the following tree structure:\\n\".format(n=n_nodes)\n",
    ")\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\n",
    "            \"{space}node={node} is a leaf node with value={value}.\".format(\n",
    "                space=node_depth[i] * \"\\t\", node=i, value=np.around(values[i], 3)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"{space}node={node} is a split node with value={value}: \"\n",
    "            \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "            \"else to node {right}.\".format(\n",
    "                space=node_depth[i] * \"\\t\",\n",
    "                node=i,\n",
    "                left=children_left[i],\n",
    "                feature=feature[i],\n",
    "                threshold=threshold[i],\n",
    "                right=children_right[i],\n",
    "                value=np.around(values[i], 3),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# tree visulaization\n",
    "tree.plot_tree(clf, proportion=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import load_iris, make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_iris() # load the datase\n",
    "# print(iris.DESCR)  # provides a description of the dataset\n",
    "\n",
    "plot_idx = 1    # initializing the plot labels\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3 #\n",
    "n_estimators = 1 # number of trees in the forest\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 42  # fix the seed on each iteration\n",
    "\n",
    "models = [\n",
    "    DecisionTreeClassifier(max_depth=3),\n",
    "    RandomForestClassifier(n_estimators=n_estimators)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# the restricted model\n",
    "# We use different combinations of the pairs of features\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = iris.data[:, pair]\n",
    "        y = iris.target\n",
    "\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Standardize\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        # Train\n",
    "        model.fit(X, y)\n",
    "\n",
    "        scores = model.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\".\")[-1][:-2][: -len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair, \"has a score of\", scores)\n",
    "\n",
    "        plt.subplot(3, 2, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title, fontsize=9)\n",
    "\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    "        )\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(\n",
    "            X[:, 0],\n",
    "            X[:, 1],\n",
    "            c=y,\n",
    "            cmap=ListedColormap([\"r\", \"y\", \"b\"]),\n",
    "            edgecolor=\"k\",\n",
    "            s=20,\n",
    "        )\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n",
    "plt.axis(\"tight\")\n",
    "plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "ClasificaciÃ³n con el datos de dos medias lunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "n_samples = 200\n",
    "noise = 0.1\n",
    "X_moons, y_moons = make_moons(n_samples=n_samples, noise=noise)\n",
    "# Scale it to a range\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X_moons)\n",
    "y = y_moons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
